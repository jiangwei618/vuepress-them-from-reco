---
title: 网络IO
date: 2019-08-26 10:57:34
description: 网络IO
categories:
  - 语言
  - java
  - io
tags:
  - io
export_on_save:
  markdown: true

---

## 1. 分类

网络 IO 的模型大致包括下面几种

- 同步模型（synchronous IO）
- 阻塞 IO（bloking IO）
- 非阻塞 IO（non-blocking IO）
- 多路复用 IO（multiplexing IO）
- 信号驱动式 IO（signal-driven IO）
- 异步 IO（asynchronous IO）
- 异步 IO

## 2. 详解

&emsp;&emsp;网络 IO 的本质是 socket 的读取，socket 在 linux 系统被抽象为流，IO 可以理解为对流的操作。对于一次 IO 访问，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间，所以一般会经历两个阶段：

1. 等待所有数据都准备好或者一直在等待数据，有数据的时候将数据拷贝到系统内核；
1. 将内核缓存中数据拷贝到用户进程中；

**对于 socket 流而言：**

1. 等待网络上的数据分组到达，然后被复制到内核的某个缓冲区；
1. 把数据从内核缓冲区复制到应用进程缓冲区中；

## 3. 阻塞 IO

### 3.1. 介绍

&emsp;&emsp;这也是最常用的模型，默认情况下所有的套接字都是 阻塞 的；

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-50-25.png)

&emsp;&emsp;当用户进程调用了 recvfrom 这个系统调用，kernel（内核）就开始了 IO 的第一个阶段：准备数据。对于 network io 来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的 UDP 包），这个时候 kernel 就要等待足够的数据到来。

&emsp;&emsp;而在用户进程这边，整个进程会被阻塞。当 kernel 一直等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。
&emsp;&emsp;**所以，blocking IO 的特点就是在 IO 执行的两个阶段（等待数据和拷贝数据两个阶段）都被 block 了。**

&emsp;&emsp;几乎所有的程序员第一次接触到的网络编程都是从 listen()、send()、recv() 等接口开始的，使用这些接口可以很方便的构建服务器/客户机的模型。然而大部分的 socket 接口都是阻塞型的。如下图

&emsp;&emsp;ps：所谓阻塞型接口是指系统调用（一般是 IO 接口）不返回调用结果并让当前线程一直阻塞，只有当该系统调用获得结果或者超时出错时才返回。

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-53-07.png)

&emsp;&emsp;实际上，除非特别指定，几乎所有的 IO 接口 ( 包括 socket 接口 ) 都是阻塞型的。这给网络编程带来了一个很大的问题，如在调用 recv(1024)的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。

&emsp;&emsp;**一个简单的解决方案：**

_在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接_

&emsp;&emsp;**该方案的问题是：**

_开启多进程或都线程的方式，在遇到要同时响应成百上千路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率，而且线程与进程本身也更容易进入假死状态。_

&emsp;&emsp;**改进方案：**

_很多程序员可能会考虑使用“线程池”或“连接池”。“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。“连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。这两种技术都可以很好的降低系统开销，都被广泛应用很多大型系统，如 websphere、tomcat 和各种数据库等。_

&emsp;&emsp;**改进后方案其实也存在着问题：**

_“线程池”和“连接池”技术也只是在一定程度上缓解了频繁调用 IO 接口带来的资源占用。而且，所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。_

_对应上例中的所面临的可能同时出现的上千甚至上万次的客户端请求，“线程池”或“连接池”或许可以缓解部分压力，但是不能解决所有问题。总之，多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求，多线程模型也会遇到瓶颈，可以用非阻塞接口来尝试解决这个问题。_

#### 3.1.1. 优点和缺点

&emsp;&emsp;**优点**：能够及时返回数据，无延迟；方便调试；

&emsp;&emsp;**缺点**：需要付出等待的代价；

## 4. 非阻塞 IO

### 4.1. 介绍

&emsp;&emsp;非阻塞，当所请求的 I/O 操作非得把当前进程设置成睡眠才能完成时，不要把当前进程设置成睡眠，而是返回一个错误信息（数据报没有准备好的情况下），此时当前进程可以做其它的事情，不用阻塞；

&emsp;&emsp;Linux 下，可以通过设置 socket 使其变为 non-blocking。当对一个 non-blocking socket 执行读操作时，流程是这个样子：

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-53-15.png)

&emsp;&emsp;从图中可以看出，当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error。从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是用户就可以在本次到下次再发起 read 询问的时间间隔内做其他事情，或者直接再次发送 read 操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call，那么它马上就将数据拷贝到了用户内存（这一阶段仍然是阻塞的），然后返回。

&emsp;&emsp;也就是说非阻塞的 recvform 系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个 error。进程在返回之后，可以干点别的事情，然后再发起 recvform 系统调用。重复上面的过程，循环往复的进行 recvform 系统调用。这个过程通常被称之为轮询。轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态。

&emsp;&emsp;**所以，在非阻塞式 IO 中，用户进程其实是需要不断的主动询问 kernel 数据准备好了没有。**

&emsp;&emsp;当一个应用进程像这样对一个非阻塞 socket 循环调用 recv/recvfrom 时，则称为轮询；应用进程持续轮询内核，以查看某个操作是否就绪，这么做往往消耗大量的 CPU 时间。

### 4.2. 优点和缺点

&emsp;&emsp;**优点**：相较于阻塞模型，非阻塞不用再等待任务，而是把时间花费到其它任务上，也就是这个当前线程同时处理多个任务；

&emsp;&emsp;**缺点**：导致任务完成的响应延迟增大了，因为每隔一段时间才去执行询问的动作，但是任务可能在两个询问动作的时间间隔内完成，这会导致整体数据吞吐量的降低。应用进程持续轮询内核，以查看某个操作是否就绪，这么做往往消耗大量的 CPU 时间。

## 5. IO 多路复用（IO multiplexing）

### 5.1. 介绍

&emsp;&emsp;IO multiplexing 这个词可能有点陌生，但是如果我说 select/epoll，大概就都能明白了。有些地方也称这种 IO 方式为事件驱动 IO(event driven IO)。我们都知道，select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 IO。它的基本原理就是 select/epoll 这个 function 会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。它的流程如图：

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-53-42.png)

&emsp;&emsp;当用户进程调用了 select，那么整个进程会被 block，而同时，kernel 会“监视”所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。
&emsp;&emsp;这个图和 blockingIO 的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select 和 recvfrom)，而 blocking IO 只调用了一个系统调用(recvfrom)。但是，**用 select 的优势在于它可以同时处理多个 connection。**

&emsp;&emsp;**强调：**

&emsp;&emsp;1. 如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。

&emsp;&emsp;2. 在多路复用模型中，对于每一个 socket，一般都设置成为 non-blocking，但是，如上图所示，整个用户的 process 其实是一直被 block 的。只不过 process 是被 select 这个函数 block，而不是被 socket IO 给 block。

&emsp;&emsp;**结论: select 的优势在于可以处理多个连接，不适用于单个连接**

&emsp;&emsp;**select 监听 fd 变化的过程分析：**

&emsp;&emsp;_用户进程创建 socket 对象，拷贝监听的 fd 到内核空间，每一个 fd 会对应一张系统文件表，内核空间的 fd 响应到数据后，就会发送信号给用户进程数据已到；用户进程再发送系统调用，比如（accept）将内核空间的数据 copy 到用户空间，同时作为接受数据端内核空间的数据清除，这样重新监听时 fd 再有新的数据又可以响应到了（发送端因为基于 TCP 协议所以需要收到应答后才会清除）。_

&emsp;&emsp;**该模型的优点**

_相比其他模型，使用 select() 的事件驱动模型只用单线程（进程）执行，占用资源少，不消耗太多 CPU，同时能够为多客户端提供服务。如果试图建立一个简单的事件驱动的服务器程序，这个模型有一定的参考价值。_

&emsp;&emsp;**该模型的缺点：**

_&emsp;&emsp;首先 select()接口并不是实现“事件驱动”的最好选择。因为当需要探测的句柄值较大时，select()接口本身需要消耗大量时间去轮询各个句柄。
&emsp;&emsp;很多操作系统提供了更为高效的接口，如 linux 提供了 epoll，BSD 提供了 kqueue，Solaris 提供了/dev/poll，…。
&emsp;&emsp;如果需要实现更高效的服务器程序，类似 epoll 这样的接口更被推荐。遗憾的是不同的操作系统特供的 epoll 接口有很大差异，
&emsp;&emsp;所以使用类似于 epoll 的接口实现具有较好跨平台能力的服务器会比较困难。
&emsp;&emsp;其次，该模型将事件探测和事件响应夹杂在一起，一旦事件响应的执行体庞大，则对整个模型是灾难性的。_

## 6. 信号驱动式 I/O 模型

&emsp;&emsp;首先开启套接字的信号驱动式 IO 功能，并且通过 sigaction 系统调用安装一个信号处理函数，该函数调用将立即返回，当前进程没有被阻塞，继续工作；当数据报准备好的时候，内核则为该进程产生 SIGIO 的信号，随后既可以在信号处理函数中调用 recvfrom 读取数据报，并且通知主循环数据已经准备好等待处理，也可以通知主循环让它读取数据报；（其实就是一个待读取的通知和待处理的通知）；

## 7. 异步式 I/O 模型

&emsp;&emsp;Linux 下的 asynchronous IO 其实用得不多，从内核 2.6 版本才开始引入。先看一下它的流程：

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-53-51.png)

&emsp;&emsp;用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal，告诉它 read 操作完成了。

## 8. IO 模型比较分析

&emsp;&emsp;blocking 和 non-blocking 的区别在哪，synchronous IO 和 asynchronous IO 的区别

- _先回答最简单的这个：blocking vs non-blocking。前面的介绍中其实已经很明确的说明了这两者的区别。调用 blocking IO 会一直 block 住对应的进程直到操作完成，而 non-blocking IO 在 kernel 还在准备数据的情况下会立刻返回。_

- _再说明 synchronous IO 和 asynchronous IO 的区别之前，需要先给出两者的定义。Stevens 给出的定义（其实是 POSIX 的定义）是这样子的：_

&emsp;&emsp;&emsp;&emsp; _A synchronous I/O operation causes the requesting process to be blocked until that I/O operationcompletes;_

&emsp;&emsp;&emsp;&emsp;_An asynchronous I/O operation does not cause the requesting process to be blocked; _

&emsp;&emsp;&emsp;&emsp;_两者的区别就在于 synchronous IO 做”IO operation”的时候会将 process 阻塞。按照这个定义，四个 IO 模型可以分为两大类，之前所述的 blocking IO，non-blocking IO，IO multiplexing 都属于 synchronous IO 这一类，而 asynchronous I/O 后一类 。_

&emsp;&emsp;有人可能会说，non-blocking IO 并没有被 block 啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的 IO 操作，就是例子中的 recvfrom 这个 system call。non-blocking IO 在执行 recvfrom 这个 system call 的时候，如果 kernel 的数据没有准备好，这时候不会 block 进程。但是，当 kernel 中数据准备好的时候，recvfrom 会将数据从 kernel 拷贝到用户内存中，这个时候进程是被 block 了，在这段时间内，进程是被 block 的。而 asynchronous IO 则不一样，当进程发起 IO 操作之后，就直接返回再也不理睬了，直到 kernel 发送一个信号，告诉进程说 IO 完成。在这整个过程中，进程完全没有被 block。

&emsp;&emsp;**各个 IO Model 的比较如图所示：**

![](https://raw.githubusercontent.com/jiangwei618/note/master/assets/image/5net_Io.md-2019-08-06-14-54-00.png)

&emsp;&emsp;经过上面的介绍，会发现 non-blocking IO 和 asynchronous IO 的区别还是很明显的。在 non-blocking IO 中，虽然进程大部分时间都不会被 block，但是它仍然要求进程去主动的 check，并且当数据准备完成以后，也需要进程主动的再次调用 recvfrom 来将数据拷贝到用户内存。而 asynchronous IO 则完全不同。它就像是用户进程将整个 IO 操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查 IO 操作的状态，也不需要主动的去拷贝数据。

## 9. selectors 模块

select,poll,epoll

&emsp;&emsp;IO 复用：为了解释这个名词，首先来理解下复用这个概念，复用也就是共用的意思，这样理解还是有些抽象，为此，咱们来理解下复用在通信领域的使用，在通信领域中为了充分利用网络连接的物理介质，往往在同一条网络链路上采用时分复用或频分复用的技术使其在同一链路上传输多路信号，到这里我们就基本上理解了复用的含义，即公用某个“介质”来尽可能多的做同一类(性质)的事，那 IO 复用的“介质”是什么呢？为此我们首先来看看服务器编程的模型，客户端发来的请求服务端会产生一个进程来对其进行服务，每当来一个客户请求就产生一个进程来服务，然而进程不可能无限制的产生，因此为了解决大量客户端访问的问题，引入了 IO 复用技术，即：一个进程可以同时对多个客户请求进行服务。也就是说 IO 复用的“介质”是进程(准确的说复用的是 select 和 poll，因为进程也是靠调用 select 和 poll 来实现的)，复用一个进程(select 和 poll)来对多个 IO 进行服务，虽然客户端发来的 IO 是并发的但是 IO 所需的读写数据多数情况下是没有准备好的，因此就可以利用一个函数(select 和 poll)来监听 IO 所需的这些数据的状态，一旦 IO 有数据可以进行读写了，进程就来对这样的 IO 进行服务。

&emsp;&emsp;理解完 IO 复用后，我们在来看下实现 IO 复用中的三个 API(select、poll 和 epoll)的区别和联系

&emsp;&emsp;select，poll，epoll 都是 IO 多路复用的机制，I/O 多路复用就是通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知应用程序进行相应的读写操作。但 select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步 I/O 则无需自己负责进行读写，异步 I/O 的实现会负责把数据从内核拷贝到用户空间。三者的原型如下所示：

```
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);

int poll(struct pollfd *fds, nfds_t nfds, int timeout);

int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

1.select 的第一个参数 nfds 为 fdset 集合中最大描述符值加 1，fdset 是一个位数组，其大小限制为\_\_FD_SETSIZE（1024），位数组的每一位代表其对应的描述符是否需要被检查。第二三四参数表示需要关注读、写、错误事件的文件描述符位数组，这些参数既是输入参数也是输出参数，可能会被内核修改用于标示哪些描述符上发生了关注的事件，所以每次调用 select 前都需要重新初始化 fdset。timeout 参数为超时时间，该结构会被内核修改，其值为超时剩余的时间。

select 的调用步骤如下：

（1）使用 copy_from_user 从用户空间拷贝 fdset 到内核空间

（2）注册回调函数\_\_pollwait

（3）遍历所有 fd，调用其对应的 poll 方法（对于 socket，这个 poll 方法是 sock_poll，sock_poll 根据情况会调用到 tcp_poll,udp_poll 或者 datagram_poll）

（4）以 tcp_poll 为例，其核心实现就是\_\_pollwait，也就是上面注册的回调函数。

（5）\_\_pollwait 的主要工作就是把 current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于 tcp_poll 来说，其等待队列是 sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数 据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时 current 便被唤醒了。

（6）poll 方法返回时会返回一个描述读写操作是否就绪的 mask 掩码，根据这个 mask 掩码给 fd_set 赋值。

（7）如果遍历完所有的 fd，还没有返回一个可读写的 mask 掩码，则会调用 schedule_timeout 是调用 select 的进程（也就是 current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout 指定），还是没人唤醒，则调用 select 的进程会重新被唤醒获得 CPU，进而重新遍历 fd，判断有没有就绪的 fd。

（8）把 fd_set 从内核空间拷贝到用户空间。

总结下 select 的几大缺点：

（1）每次调用 select，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大

（2）同时每次调用 select 都需要在内核遍历传递进来的所有 fd，这个开销在 fd 很多时也很大

（3）select 支持的文件描述符数量太小了，默认是 1024

2． poll 与 select 不同，通过一个 pollfd 数组向内核传递需要关注的事件，故没有描述符个数的限制，pollfd 中的 events 字段和 revents 分别用于标示关注的事件和发生的事件，故 pollfd 数组只需要被初始化一次。

poll 的实现机制与 select 类似，其对应内核中的 sys_poll，只不过 poll 向内核传递 pollfd 数组，然后对 pollfd 中的每个描述符进行 poll，相比处理 fdset 来说，poll 效率更高。poll 返回后，需要对 pollfd 中的每个元素检查其 revents 值，来得指事件是否发生。

3．直到 Linux2.6 才出现了由内核直接支持的实现方法，那就是 epoll，被公认为 Linux2.6 下性能最好的多路 I/O 就绪通知方法。epoll 可以同时支持水平触发和边缘触发（Edge Triggered，只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发），理论上边缘触发的性能要更高一些，但是代码实现相当复杂。epoll 同样只告知那些就绪的文件描述符，而且当我们调用 epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去 epoll 指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。另一个本质的改进在于 epoll 采用基于事件的就绪通知方式。在 select/poll 中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而 epoll 事先通过 epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait()时便得到通知。

epoll 既然是对 select 和 poll 的改进，就应该能避免上述的三个缺点。那 epoll 都是怎么解决的呢？在此之前，我们先看一下 epoll 和 select 和 poll 的调用接口上的不同，select 和 poll 都只提供了一个函数——select 或者 poll 函数。而 epoll 提供了三个函 数，epoll_create,epoll_ctl 和 epoll_wait，epoll_create 是创建一个 epoll 句柄；epoll_ctl 是注 册要监听的事件类型；epoll_wait 则是等待事件的产生。

对于第一个缺点，epoll 的解决方案在 epoll_ctl 函数中。每次注册新的事件到 epoll 句柄中时（在 epoll_ctl 中指定 EPOLL_CTL_ADD），会把所有的 fd 拷贝进内核，而不是在 epoll_wait 的时候重复拷贝。epoll 保证了每个 fd 在整个过程中只会拷贝 一次。

对于第二个缺点，epoll 的解决方案不像 select 或 poll 一样每次都把 current 轮流加入 fd 对应的设备等待队列中，而只在 epoll_ctl 时把 current 挂一遍（这一遍必不可少）并为每个 fd 指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调 函数，而这个回调函数会把就绪的 fd 加入一个就绪链表）。epoll_wait 的工作实际上就是在这个就绪链表中查看有没有就绪的 fd（利用 schedule_timeout()实现睡一会，判断一会的效果，和 select 实现中的第 7 步是类似的）。

对于第三个缺点，epoll 没有这个限制，它所支持的 FD 上限是最大可以打开文件的数目，这个数字一般远大于 2048,举个例子, 在 1GB 内存的机器上大约是 10 万左右，具体数目可以 cat /proc/sys/fs/file-max 察看,一般来说这个数目和系统内存关系很大。

总结：

（1）select，poll 实现需要自己不断轮询所有 fd 集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而 epoll 其实也需要调用 epoll_wait 不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪 fd 放入就绪链表中，并唤醒在 epoll_wait 中进入睡眠的进程。虽然都要睡眠和交替，但是 select 和 poll 在“醒着”的时候要遍历整个 fd 集合，而 epoll 在“醒着”的 时候只要判断一下就绪链表是否为空就行了，这节省了大量的 CPU 时间，这就是回调机制带来的性能提升。

（2）select，poll 每次调用都要把 fd 集合从用户态往内核态拷贝一次，并且要把 current 往设备等待队列中挂一次，而 epoll 只要 一次拷贝，而且把 current 往等待队列上挂也只挂一次（在 epoll_wait 的开始，注意这里的等待队列并不是设备等待队列，只是一个 epoll 内 部定义的等待队列），这也能节省不少的开销。
