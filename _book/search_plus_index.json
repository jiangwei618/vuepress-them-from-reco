{"Linux-Tutorial/SUMMARY.html":{"url":"Linux-Tutorial/SUMMARY.html","title":"Linux","keywords":"","body":"Java 程序员眼中的 Linux Linux 介绍 Ubuntu 介绍 Ubuntu 安装 Ubuntu 设置（目录） Kali Linux 介绍和设置（目录） CentOS 介绍 CentOS 6 安装 CentOS 7 安装 CentOS 6 和 CentOS 7 差异 CentOS 设置（目录） Ubuntu 安装 VMware VMware 克隆 CentOS 后网卡信息修改 Vim 安装、配置、快捷键列表 SSH 免密登录 Bash 命令 Bash 其他常用命令 安装的 rm（删除） Sed 命令 Linux 下常用压缩文件的解压、压缩 Yum 下载安装包及对应依赖包 Zsh 入门 终端测速 日常维护 日常监控 nmon 系统性能监控工具 Glances 安装和配置 SSH（Secure Shell）介绍 FTP（File Transfer Protocol）介绍 VPN（Virtual Private Network）介绍 NFS（Network FileSystem）介绍 NTP（Network Time Protocol）介绍 Samba 介绍 Crontab 介绍 Iptables 介绍 花生壳-安装介绍 JDK 安装 Java bin 目录下的工具 SVN 安装和配置 Tomcat 安装和配置、优化 Jenkins 安装和配置 Maven 安装和配置 Nexus 安装和配置 MySQL 安装和配置 MySQL 优化 MySQL 测试 MySQL 教程 Percona XtraDB Cluster（PXC）安装和配置 Redis 安装和配置 MongoDB 安装和配置 Solr 安装和配置 Jira 安装和配置 Jenkins 安装和配置 TeamCity 安装和配置 Nginx 安装和配置 wrk 安装和配置 FastDFS 安装和配置 FastDFS 结合 GraphicsMagick RabbitMQ 安装和配置 Openfire 安装和配置 Rap 安装和配置 Nginx + Keepalived 高可用 黑客入侵检查 Shadowsocks 安装和配置 Mycat 安装和配置 Zookeeper 安装和配置 Daemontools 工具介绍 Tmux 安装和配置 ELK 日志收集系统安装和配置 Dubbo 安装和配置 GitLab 安装和配置 JMeter 安装和配置 Docker 安装和使用 Harbor 安装和配置 LDAP 安装和使用 Alfresco 安装和使用 Apache Thrift 安装和使用 Node.js 安装和使用 CI 整套服务安装和使用 YApi 安装和配置 Kafka 安装和配置 Hadoop 安装和配置 Showdoc 安装和配置 WordPress 安装和配置 GoAccess 安装和配置 Portainer 安装和配置 Grafana 安装和配置 Ansible 安装和配置 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:05 "},"Linux-Tutorial/markdown-file/Linux.html":{"url":"Linux-Tutorial/markdown-file/Linux.html","title":"Linux 介绍","keywords":"","body":"Linux 介绍 Linux 这个名字 Linux 的 Wiki 介绍：http://zh.wikipedia.org/zh/Linux Linux 也称：GNU/Linux，而其中 GNU 的全称又是：Gnu’s Not Unix。 其中 GNU 放前面是有原因的，GNU 介绍：http://zh.wikipedia.org/wiki/GNU 对于 Linux 和 GNU/Linux 的两种叫法是有争议，可以看下面文章：https://zh.wikipedia.org/wiki/GNU/Linux%E5%91%BD%E5%90%8D%E7%88%AD%E8%AD%B0 其实我们可以认为：Linux 本质是指 Linux 内核，而称 GNU/Linux 则代表这是一个系统，所以我认为 Debian 的这个叫法是合理的，但是确实有点不好念和记忆。所以普遍大家直接称作 Linux。 通过上面的全称和资料其实我们也就了解到，Linux 本质来源不是 Unix，但是它借鉴了 Unix 的设计思想，所以在系统业界上把这种和 Unix 是一致设计思想的系统归为：类 Unix 系统。 类 Unix 系统的介绍：https://zh.wikipedia.org/wiki/%E7%B1%BBUnix%E7%B3%BB%E7%BB%9F 类 Unix 系统，除了我们今天要讲的 Linux，还有大家熟知的 Mac OS X、FreeBSD（这两个是直接从 Unix 系发展过来的，所以相对 Linux 是比较地道的类 Unix 系统） FreeBSD 介绍：http://zh.wikipedia.org/zh/FreeBSD Mac OS X 介绍：http://zh.wikipedia.org/wiki/OS_X Linux 的发行版本 Linux 的 Wiki 中有这句话： 通常情况下，Linux 被打包成供个人计算机和服务器使用的 Linux 发行版，一些流行的主流 Linux 发布版，包括 Debian（及其派生版本 Ubuntu、Linux Mint）、Fedora（及其相关版本 Red Hat Enterprise Linux、CentOS）和 openSUSE、ArchLinux（这个是我补充的）等。 通过上面这句话我做了总结，我个人觉得应该这样分： Fedora、RHEL、Centos 是一个系，他们的区别：http://blog.csdn.net/tianlesoftware/article/details/5420569 Debian、Ubuntu 是一个系的，他们的区别直接看 Ubuntu 的 Wiki 就可以得知：http://zh.wikipedia.org/zh/Ubuntu ArchLinux 自己一个系：http://zh.wikipedia.org/wiki/Arch_Linux openSUSE 自己一个系：http://zh.wikipedia.org/wiki/OpenSUSE 根据用途可以再总结： Fedora 系业界一般用于做服务器 Debian 系业界一般用于桌面，移动端，TV这一类 ArchLinux 系，很轻量的Linux，适合有一定Linux基础，并且爱折腾的人使用，用它做桌面或是服务器都行。 OpenSuse 系，嘛，嗯…人气相对比较差，一般是服务器。 其实 Linux 的发行版本有太多了，我也只是简单说下常见的而已，具体可以看：http://zh.wikipedia.org/wiki/Linux%E5%8F%91%E8%A1%8C%E7%89%88%E5%88%97%E8%A1%A8 Linux 作用 为什么要用 Linux 系统？大家常看到的说法是这样的： Linux 是一个开源的，有潜力，安全，免费的操作系统 我觉得这几个点都比较虚， 特别是免费这东西，在景德镇应该算是最不值钱的东西。作为系统的上层使用者来讲，我们之所以喜欢某个操作系统就是因为它可以加快的你生产效率，提高产能。我推荐 Linux 也只是因为它适合常见的编程语言做开发环境，仅此一点。 所有，对此我的总结就是： 如果你是某种语言的开发者，你从事这个行业，不管你怎么学习下去，Linux 永远绕不开。从简单的各种语言开发，到后期的服务器部署，分布式，集群环境，数据库相关等，Linux 都在等着你。如果你是新手程序员可能还不太懂我这句话，但是我这里可以这样提示：你可以认真去看下各个语言的官网、对应的开发组件官网，看下他们的下载和新手上路相关页面，都会有 Linux 系统对应的介绍，但是不一定有会 Windows。（P.S：微软系、美工等设计系是唯一这个总结之外的人） 在认识 Linux 作用上我以下面这边文章为结尾。Linux 和 Mac OS X 都是类 Unix 系统，所以这篇文章中基本上的理由都可以用到 Linux 上的。 为什么国外程序员爱用 Mac？http://www.vpsee.com/2009/06/why-programmers-love-mac/ 推荐的发行版本 Ubuntu：适用于开发机 推荐版本：Ubuntu kylin 15.10 Ubuntu kylin 官网：http://cn.Ubuntu.com/desktop Ubuntu 英文官网：http://www.ubuntu.com Ubuntu 中文官网：http://www.ubuntu.org.cn 网易镜像：http://mirrors.163.com/ubuntu-releases/ 阿里云镜像：http://mirrors.aliyun.com/ubuntu-releases/ Ubuntu kylin 15.10 64 位镜像地址：http://cdimage.ubuntu.com/ubuntukylin/releases/15.10/release/ubuntukylin-15.10-desktop-amd64.iso 推荐理由： 我们是要在上面做开发的，不是要把他变成生活用机的，所以你认为自己尝试安装各种中文输入法很爽吗？自己尝试让国际 Ubuntu 版变成又一个符合国情的 kylin 很爽吗？真心别折腾这些没用的东西。就像我以前说的，大学老师让 Java 新手使用记事本写代码就是一种非常 shit 行为，不断地在 Windows 上用 cmd > javac 是毫无意义的。 CentOS：适用于服务器机 推荐版本：6.7 CentOS 官网：http://www.centos.org/download/ 网易镜像：http://mirrors.163.com/centos/ 阿里云镜像：http://mirrors.aliyun.com/centos/ CentOS 6.7 64 位镜像地址：http://mirrors.163.com/centos/6.7/isos/x86_64/CentOS-6.7-x86_64-bin-DVD1.iso 推荐理由： Fedora（CentOS、RHEL） 系，是在国内外，作为企业服务器的系统最多，没有之一。我在 Quora 和知乎上也搜索了下，基本上大家都是赞同这个观点的。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"work/note/linux/base.html":{"url":"work/note/linux/base.html","title":"基本介绍","keywords":"","body":"UbuntuLinux系统环境变量配置文件  Ubuntu Linux系统环境变量配置文件分为两种：系统级文件和用户级文件，下面详细介绍环境变量的配置文件。 系统级文件：  /etc/profile:在登录时,操作系统定制用户环境时使用的第一个文件，此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行。并从/etc/profile.d目录的配置文件中搜集shell的设置。这个文件一般就是调用/etc/bash.bashrc文件。  /etc/bash.bashrc：系统级的bashrc文件，为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.  /etc/environment:在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量。 用户级文件：  ~/.profile:在登录时用到的第三个文件 是.profile文件,每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件。  ~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取。不推荐放到这儿，因为每开一个shell，这个文件会读取一次，效率 上讲不好。  ~/.bash_profile：每个用户都可使用该文件输入专用于自己 使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件。~/.bash_profile 是交互式、login 方式进入 bash 运行的~/ .bashrc是交互式 non-login 方式进入 bash 运行的通常二者设置大致相同，所以通常前者会调用后者。  ~./bash_login:不推荐使用这个，这些不会影响图形界面。而且.bash_profile优先级比bash_login高。当它们存在时，登录shell启动时会读取它们。  ~/.bash_logout:当每次退出系统(退出bash shell)时,执行该文件.  ~/.pam_environment：用户级的环境变量设置文件。 另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承 /etc/profile中的变量,他们是\"父子\"关系。 /etc/profile与/etc/enviroment的比较  /etc/profile 是所有用户的环境变量  /etc/enviroment是系统的环境变量  如果同一个变量在用户环境(/etc/profile)和系统环境(/etc/environment) 有不同的值那应该是以用户环境为准了。 设置环境变量的方法 由以上分析可知： /etc/profile全局的，随系统启动设置【设置这个文件是一劳永逸的办法】 /root/.profile和/home/myname/.profile只对当前窗口有效。 /root/.bashrc和 /home/yourname/.bashrc随系统启动，设置用户的环境变量【平时设置这个文件就可以了】那么要配置Ubuntu的环境变量，就是在这几个配置文件中找一个合适的文件进行操作了；如想将一个路径加入到$PATH中，可以由下面这样几种添加方法：1.控制台中：$PATH=\"$PATH:/my_new_path\" （关闭shell，会还原PATH）2.修改profile文件： $sudo gedit /etc/profile 在里面加入: exportPATH=\"$PATH:/my_new_path\" 3.修改.bashrc文件： $ sudo gedit /root/.bashrc 在里面加入： export PATH=\"$PATH:/my_new_path\" 后两种方法一般需要重新注销系统才能生效，最后可以通过echo命令测试一下： 四、小结  综上所述，在Ubuntu 系统中/etc/profile文件是全局的环境变量配置文件，它适用于所有的shell。在我们登陆Linux系统时，首先启动/etc/profile文件，然后再启动用户目录下的~/.bash_profile、~/.bash_login或~/.profile文件中的其中一个，执行的顺序和上面的排序一样。如果~/.bash_profile文件存在的话，一般还会执行~/.bashrc文件。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/linux/command.html":{"url":"work/note/linux/command.html","title":"常用命令","keywords":"","body":"常用命令介绍 系统信息 arch 显示机器的处理器架构(1)uname -m 显示机器的处理器架构(2)uname -r 显示正在使用的内核版本dmidecode -q 显示硬件系统部件 - (SMBIOS / DMI)hdparm -i /dev/hda 罗列一个磁盘的架构特性hdparm -tT /dev/sda 在磁盘上执行测试性读取操作cat /proc/cpuinfo 显示CPU info的信息cat /proc/interrupts 显示中断cat /proc/meminfo 校验内存使用cat /proc/swaps 显示哪些swap被使用cat /proc/version 显示内核的版本cat /proc/net/dev 显示网络适配器及统计cat /proc/mounts 显示已加载的文件系统lspci -tv 罗列 PCI 设备lsusb -tv 显示 USB 设备date 显示系统日期cal 2007 显示2007年的日历表date 041217002007.00 设置日期和时间 - 月日时分年.秒clock -w 将时间修改保存到 BIOS 关机 (系统的关机、重启以及登出 ) shutdown -h now 关闭系统(1)init 0 关闭系统(2)telinit 0 关闭系统(3)shutdown -h hours:minutes & 按预定时间关闭系统shutdown -c 取消按预定时间关闭系统shutdown -r now 重启(1)reboot 重启(2)logout 注销 文件（夹） 切换目录 cd /home 进入 '/ home' 目录'cd .. 返回上一级目录cd ../.. 返回上两级目录cd 进入个人的主目录cd ~user1 进入个人的主目录cd - 返回上次所在的目录pwd 显示工作路径 查看文件 ls 查看目录中的文件ls -F 查看目录中的文件ls -l 显示文件和目录的详细资料ls -a 显示隐藏文件ls [0-9] 显示包含数字的文件名和目录名tree 显示文件和目录由根目录开始的树形结构(1)lstree 显示文件和目录由根目录开始的树形结构(2) 创建文件（夹） mkdir dir1 创建一个叫做 'dir1' 的目录'mkdir dir1 dir2 同时创建两个目录mkdir -p /tmp/dir1/dir2 创建一个目录树touce file 创建文件 删除文件 rm -f file1 删除一个叫做 'file1' 的文件'rmdir dir1 删除一个叫做 'dir1' 的目录'rm -rf dir1 删除一个叫做 'dir1' 的目录并同时删除其内容rm -rf dir1 dir2 同时删除两个目录及它们的内容 移动文件 mv file new_dir/ 移动文件到目录mv file new_dir/file2 移动并重命名mv dir dir2/ 移动文件夹mv dir dir2/dir2 移动并重命名 拷贝文件 cp file1 file2 复制一个文件cp dir/* . 复制一个目录下的所有文件到当前工作目录cp -a /tmp/dir1 . 复制一个目录到当前工作目录cp -a dir1 dir2 复制一个目录 创建软连接 ln -s file1 lnk1 创建一个指向文件或目录的软链接ln file1 lnk1 创建一个指向文件或目录的物理链接touch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm) 文件搜索 find / -name file1 从 '/' 开始进入根文件系统搜索文件和目录find / -user user1 搜索属于用户 'user1' 的文件和目录find /home/user1 -name *.bin 在目录 '/ home/user1' 中搜索带有'.bin' 结尾的文件find /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件find /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件find / -name *.rpm -exec chmod 755 '{}' \\; 搜索以 '.rpm' 结尾的文件并定义其权限find / -xdev -name *.rpm 搜索以 '.rpm' 结尾的文件，忽略光驱、捷盘等可移动设备locate *.ps 寻找以 '.ps' 结尾的文件 - 先运行 'updatedb' 命令whereis halt 显示一个二进制文件、源码或man的位置which halt 显示一个二进制文件或可执行文件的完整路径 查看文件内容 cat file1 从第一个字节开始正向查看文件的内容tac file1 从最后一行开始反向查看一个文件的内容more file1 查看一个长文件的内容less file1 类似于 'more' 命令，但是它允许在文件中和正向操作一样的反向操作head -2 file1 查看一个文件的前两行tail -2 file1 查看一个文件的最后两行tail -f /var/log/messages 实时查看被添加到一个文件中的内容 文本处理 cat file1 file2 ... | command <> file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUTcat file1 | command( sed, grep, awk, grep, etc...) > result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中cat file1 | command( sed, grep, awk, grep, etc...) >> result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中grep Aug /var/log/messages 在文件 '/var/log/messages'中查找关键词\"Aug\"grep ^Aug /var/log/messages 在文件 '/var/log/messages'中查找以\"Aug\"开始的词汇grep [0-9] /var/log/messages 选择 '/var/log/messages' 文件中所有包含数字的行grep Aug -R /var/log/ 在目录 '/var/log' 及随后的目录中搜索字符串\"Aug\"sed 's/stringa1/stringa2/g' example.txt 将example.txt文件中的 \"string1\" 替换成 \"string2\"sed '/^$/d' example.txt 从example.txt文件中删除所有空白行sed '/ #/d; /^$/d' example.txt 从example.txt文件中删除所有注释和空白行echo 'esempio' | tr '[:lower:]' '[:upper:]' 合并上下单元格内容sed -e '1d' result.txt 从文件example.txt 中排除第一行sed -n '/stringa1/p' 查看只包含词汇 \"string1\"的行sed -e 's/ $//' example.txt 删除每一行最后的空白字符sed -e 's/stringa1//g' example.txt 从文档中只删除词汇 \"string1\" 并保留剩余全部sed -n '1,5p;5q' example.txt 查看从第一行到第5行内容sed -n '5p;5q' example.txt 查看第5行sed -e 's/00/0/g' example.txt 用单个零替换多个零cat -n file1 标示文件的行数cat example.txt | awk 'NR%2==1' 删除example.txt文件中的所有偶数行echo a b c | awk '{print $1}' 查看一行第一栏echo a b c | awk '{print $1,$3}' 查看一行的第一和第三栏paste file1 file2 合并两个文件或两栏的内容paste -d '+' file1 file2 合并两个文件或两栏的内容，中间用\"+\"区分sort file1 file2 排序两个文件的内容sort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份)sort file1 file2 | uniq -u 删除交集，留下其他的行sort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件)comm -1 file1 file2 比较两个文件的内容只删除 'file1' 所包含的内容comm -2 file1 file2 比较两个文件的内容只删除 'file2' 所包含的内容comm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 字符设置和文件格式转换 dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIXunix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOSrecode ..HTML page.html 将一个文本文件转换成htmlrecode -l | more 显示所有允许的转换格式 权限管理 用户和群组 groupadd group_name 创建一个新用户组groupdel group_name 删除一个用户组groupmod -n new_group_name old_group_name 重命名一个用户组useradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 \"admin\" 用户组的用户useradd user1 创建一个新用户userdel -r user1 删除一个用户 ( '-r' 排除主目录)usermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性passwd 修改口令passwd user1 修改一个用户的口令 (只允许root执行)chage -E 2005-12-31 user1 设置用户口令的失效期限pwck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的用户grpck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的群组newgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组 文件的权限 - 使用 \"+\" 设置权限，使用 \"-\" 用于取消 ls -lh 显示权限ls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示chmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限chmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限chown user1 file1 改变一个文件的所有人属性chown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性chgrp group1 file1 改变文件的群组chown user1:group1 file1 改变一个文件的所有人和群组属性find / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件chmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限chmod u-s /bin/file1 禁用一个二进制文件的 SUID位chmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的chmod g-s /home/public 禁用一个目录的 SGID 位chmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件chmod o-t /home/public 禁用一个目录的 STIKY 位 文件的特殊属性 - 使用 \"+\" 设置权限，使用 \"-\" 用于取消 chattr +a file1 只允许以追加方式读写文件chattr +c file1 允许这个文件能被内核自动压缩/解压chattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件chattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接chattr +s file1 允许一个文件被安全地删除chattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘chattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件lsattr 显示特殊的属性 压缩 打包和压缩文件 bunzip2 file1.bz2 解压一个叫做 'file1.bz2'的文件bzip2 file1 压缩一个叫做 'file1' 的文件gunzip file1.gz 解压一个叫做 'file1.gz'的文件gzip file1 压缩一个叫做 'file1'的文件gzip -9 file1 最大程度压缩rar a file1.rar test_file 创建一个叫做 'file1.rar' 的包rar a file1.rar file1 file2 dir1 同时压缩 'file1', 'file2' 以及目录 'dir1'rar x file1.rar 解压rar包unrar x file1.rar 解压rar包tar -cvf archive.tar file1 创建一个非压缩的 tarballtar -cvf archive.tar file1 file2 dir1 创建一个包含了 'file1', 'file2' 以及 'dir1'的档案文件tar -tf archive.tar 显示一个包中的内容tar -xvf archive.tar 释放一个包tar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下tar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包tar -jxvf archive.tar.bz2 解压一个bzip2格式的压缩包tar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包tar -zxvf archive.tar.gz 解压一个gzip格式的压缩包zip file1.zip file1 创建一个zip格式的压缩包zip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包unzip file1.zip 解压一个zip格式压缩包 存储空间相关 挂载一个文件系统 mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 '/ mnt/hda2' 已经存在umount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 '/ mnt/hda2' 退出fuser -km /mnt/hda2 当设备繁忙时强制卸载umount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用mount /dev/fd0 /mnt/floppy 挂载一个软盘mount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrommount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrommount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrommount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件mount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统mount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享 磁盘空间 df -h 显示已经挂载的分区列表ls -lSr |more 以尺寸大小排列文件和目录du -sh dir1 估算目录 'dir1' 已经使用的磁盘空间'du -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小rpm -q -a --qf '%10{SIZE}t%{NAME}n' | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统)dpkg-query -W -f='${Installed-Size;10}t${Package}n' | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) 软件安装 RPM 包 - （Fedora, Redhat及类似系统） rpm -ivh package.rpm 安装一个rpm包rpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告rpm -U package.rpm 更新一个rpm包但不改变其配置文件rpm -F package.rpm 更新一个确定已经安装的rpm包rpm -e package_name.rpm 删除一个rpm包rpm -qa 显示系统中所有已经安装的rpm包rpm -qa | grep httpd 显示所有名称中包含 \"httpd\" 字样的rpm包rpm -qi package_name 获取一个已安装包的特殊信息rpm -qg \"System Environment/Daemons\" 显示一个组件的rpm包rpm -ql package_name 显示一个已经安装的rpm包提供的文件列表rpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表rpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表rpm -q package_name --whatprovides 显示一个rpm包所占的体积rpm -q package_name --scripts 显示在安装/删除期间所执行的脚本lrpm -q package_name --changelog 显示一个rpm包的修改历史rpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供rpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表rpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书rpm --checksig package.rpm 确认一个rpm包的完整性rpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性rpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间rpm -Va 检查系统中所有已安装的rpm包- 小心使用rpm -Vp package.rpm 确认一个rpm包还未安装rpm2cpio package.rpm | cpio --extract --make-directories bin 从一个rpm包运行可执行文件rpm -ivh /usr/src/redhat/RPMS/arch/package.rpm 从一个rpm源码安装一个构建好的包rpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包 YUM 软件包升级器 - （Fedora, RedHat及类似系统） yum install package_name 下载并安装一个rpm包yum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系yum update package_name.rpm 更新当前系统中所有安装的rpm包yum update package_name 更新一个rpm包yum remove package_name 删除一个rpm包yum list 列出当前系统中安装的所有包yum search package_name 在rpm仓库中搜寻软件包yum clean packages 清理rpm缓存删除下载的包yum clean headers 删除所有头文件yum clean all 删除所有缓存的包和头文件 DEB 包 (Debian, Ubuntu 以及类似系统) dpkg -i package.deb 安装/更新一个 deb 包dpkg -r package_name 从系统删除一个 deb 包dpkg -l 显示系统中所有已经安装的 deb 包dpkg -l | grep httpd 显示所有名称中包含 \"httpd\" 字样的deb包dpkg -s package_name 获得已经安装在系统中一个特殊包的信息dpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表dpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表dpkg -S /bin/ping 确认所给的文件由哪个deb包提供 APT 软件工具 (Debian, Ubuntu 以及类似系统) apt-get install package_name 安装/更新一个 deb 包apt-cdrom install package_name 从光盘安装/更新一个 deb 包apt-get update 升级列表中的软件包apt-get upgrade 升级所有已安装的软件apt-get remove package_name 从系统删除一个deb包apt-get check 确认依赖的软件仓库正确apt-get clean 从下载的软件包中清理缓存apt-cache search searched-package 返回包含所要搜索字符串的软件包名称 文件系统 文件系统分析 badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块fsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性fsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性e2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性e2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性fsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性fsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性fsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性dosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 初始化一个文件系统 mkfs /dev/hda1 在hda1分区创建一个文件系统mke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统mke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统mkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统fdformat -n /dev/fd0 格式化一个软盘mkswap /dev/hda3 创建一个swap文件系统 SWAP文件系统 mkswap /dev/hda3 创建一个swap文件系统swapon /dev/hda3 启用一个新的swap文件系统swapon /dev/hda2 /dev/hdb3 启用两个swap分区 备份 dump -0aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的完整备份dump -1aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的交互式备份restore -if /tmp/home0.bak 还原一个交互式备份rsync -rogpav --delete /home /tmp 同步两边的目录rsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsyncrsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录rsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr 'dd of=hda.gz' 通过ssh在远程主机上执行一次备份本地磁盘的操作dd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件tar -Puf backup.tar /home/user 执行一次对 '/home/user' 目录的交互式备份操作( cd /tmp/local/ && tar c . ) | ssh -C user@ip_addr 'cd /home/share/ && tar x -p' 通过ssh在远程目录中复制一个目录内容( tar c /home ) | ssh -C user@ip_addr 'cd /home/backup-home && tar x -p' 通过ssh在远程目录中复制一个本地目录tar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接find /home/user1 -name '.txt' | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 '.txt' 结尾的文件到另一个目录find /var/log -name '.log' | tar cv --files-from=- | bzip2 > log.tar.bz2 查找所有以 '.log' 结尾的文件并做成一个bzip包dd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作dd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容 光盘 cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容mkisofs /dev/cdrom > cd.iso 在磁盘上创建一个光盘的iso镜像文件mkisofs /dev/cdrom | gzip > cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件mkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件cdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件gzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件mount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件cd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中cd-paranoia -- \"-3\" 从一个CD光盘转录音轨到 wav 文件中（参数-3）cdrecord --scanbus 扫描总线以识别scsi通道dd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD 网络 网络 - （以太网和WIFI无线） ifconfig eth0 显示一个以太网卡的配置ifup eth0 启用一个 'eth0' 网络设备ifdown eth0 禁用一个 'eth0' 网络设备ifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址ifconfig eth0 promisc 设置 'eth0' 成混杂模式以嗅探数据包 (sniffing)dhclient eth0 以dhcp模式启用 'eth0'route -n show routing tableroute add -net 0/0 gw IP_Gateway configura default gatewayroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network '192.168.0.0/16'route del 0/0 gw IP_gateway remove static routeecho \"1\" > /proc/sys/net/ipv4/ip_forward activate ip routinghostname show hostname of systemhost www.example.com lookup hostname to resolve name to ip address and viceversa(1)nslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2)ip link show show link status of all interfacesmii-tool eth0 show link status of 'eth0'ethtool eth0 show statistics of network card 'eth0'netstat -tup show all active network connections and their PIDnetstat -tupl show all network services listening on the system and their PIDtcpdump tcp port 80 show all HTTP trafficiwlist scan show wireless networksiwconfig eth1 show configuration of a wireless network cardhostname show hostnamehost www.example.com lookup hostname to resolve name to ip address and viceversanslookup www.example.com lookup hostname to resolve name to ip address and viceversawhois www.example.com lookup on Whois database 调试工具 JPS工具  jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 命令 $> jps 23991 Jps 23789 BossMain 23651 Resin 比较常用的参数： -q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数 $> jps -q 28680 23789 23651 -m 输出传递给main 方法的参数，在嵌入式jvm上可能是null $> jps -m 28715 Jps -m 23789 BossMain 23651 Resin -socketwait 32768 -stdout /data/aoxj/resin/log/stdout.log -stderr /data/aoxj/resin/log/stderr.log -l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名 $> jps -l 28729 sun.tools.jps.Jps 23789 com.asiainfo.aimc.bossbi.BossMain 23651 com.caucho.server.resin.Resin -v 输出传递给JVM的参数 $> jps -v 23789 BossMain 28802 Jps -Denv.class.path=/data/aoxj/bossbi/twsecurity/java/trustwork140.jar:/data/aoxj/bossbi/twsecurity/java/:/data/aoxj/bossbi/twsecurity/java/twcmcc.jar:/data/aoxj/jdk15/lib/rt.jar:/data/aoxj/jd k15/lib/tools.jar -Dapplication.home=/data/aoxj/jdk15 -Xms8m 23651 Resin -Xss1m -Dresin.home=/data/aoxj/resin -Dserver.root=/data/aoxj/resin -Djava.util.logging.manager=com.caucho.log.LogManagerImpl - Djavax.management.builder.initial=com.caucho.jmx.MBeanServerBuilderImpl sudo jps看到的进程数量最全 jps 192.168.0.77 列出远程服务器192.168.0.77机器所有的jvm实例，采用rmi协议，默认连接端口为1099 （前提是远程服务器提供jstatd服务） 文件权限详解 权限 命令 　　命令：ls -l (或者ll)中显示的内容如下： 　　例子1：　drwxr-xr-x 2 jiang root 4096 2月 29 2016 bin/ 　　例子2：　-rw-r--r-- 1 jiang root 45056 10月 18 21:47 idmlight.db.mv.db 解释 -drwxr-xr-x 含义 - 第一个字符代表文件（-）、目录（d），链接（l） - 其余字符每3个一组（rwx），读（r）、写（w）、执行（x） - 第一组rwx：文件所有者的权限是读、写和执行 - 第二组rw-：与文件所有者同一组的用户的权限是读、写但不能执行 - 第三组r--：不与文件所有者同组的其他用户的权限是读不能写和执行 - 也可用数字表示为：r=4，w=2，x=1 因此rwx=4+2+1=7 1 表示连接的文件数 jiang 表示用户 root表示用户所在的组 4096 表示文件大小（字节） 10月 18 21:47 表示最后修改日期 abc 表示文件名 权限修改命令(chmod) 改变权限的命令 注：也可用数字表示为：r=4，w=2，x=1 因此rwx=4+2+1=7 chmod 改变文件或目录的权限 chmod 755 abc：赋予abc权限rwxr-xr-x chmod u=rwx，g=rx，o=rx abc：同上u=用户权限，g=组权限，o=不同组其他用户权限 chmod u-x，g+w abc：给abc去除用户执行的权限，增加组写的权限 chmod a+r abc：给所有用户添加读的权限 修改所有者或组chown 　　　chown将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户ID；组可以是组名或者组ID；文件是以空格分开的要改变权限的文件列 表，支持通配符。系统管理员经常使用chown命令，在将文件拷贝到另一个用户的名录下之后，让用户拥有使用该文件的权限。 　　1．命令格式： 　　　　chown [选项]... [所有者][:[组]] 文件... 　　2．命令功能： 　　　　通过chown改变文件的拥有者和群组。在更改文件的所有者或所属群组时，可以使用用户名称和用户识别码设置。普通用户不能将自己的文件改变 成其他的拥有者。其操作权限一般为管理员。 　　3．命令参数： 　　必要参数: 　　　　-c 显示更改的部分的信息 　　　　-f 忽略错误信息 　　　　-h 修复符号链接 　　　　-R 处理指定目录以及其子目录下的所有文件 　　　　-v 显示详细的处理信息 　　　　-deference 作用于符号链接的指向，而不是链接文件本身 选择参数: 　　　　--reference= 把指定的目录/文件作为参考，把操作的文件/目录设置成参考文件/目录相同拥有者和群组 　　　　--from= 只有当前用户和群组跟指定的用户和群组相同时才进行改变 　　　　--help 显示帮助信息 　　　　--version 显示版本信息 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"Linux-Tutorial/markdown-file/Ubuntu.html":{"url":"Linux-Tutorial/markdown-file/Ubuntu.html","title":"Ubuntu 介绍","keywords":"","body":"Ubuntu 介绍 Ubuntu 母公司 Canonical：http://www.canonical.com/ Ubuntu 百科：http://baike.baidu.com/item/ubuntu Ubuntu Wiki：http://zh.wikipedia.org/zh/Ubuntu Ubuntu 英文官网：http://www.ubuntu.com Ubuntu 中文官网：http://www.ubuntu.org.cn Ubuntu kylin 官网：http://cn.Ubuntu.com/desktop Ubuntu 标准桌面版下载：http://www.ubuntu.org.cn/download/desktop Ubuntu 官网回答社区：http://askubuntu.com/ Ubuntu 正式衍生版本：https://zh.wikipedia.org/zh/Ubuntu#.E5.88.86.E6.94.AF.E7.89.88.E6.9C.AC Ubuntu 非正式衍生版本：https://zh.wikipedia.org/zh/Ubuntu#.E9.9D.9E.E6.AD.A3.E5.BC.8F.E8.A1.8D.E7.94.9F.E7.89.88.E6.9C.AC Unity 桌面介绍：https://zh.wikipedia.org/wiki/Unity_(使用者介面) GNOME 桌面介绍：https://zh.wikipedia.org/wiki/GNOME KDE 桌面介绍：https://zh.wikipedia.org/wiki/KDE Ubuntu 原型系统：Debian Debian Wiki：http://zh.wikipedia.org/zh/Debian Debian 百科：http://baike.baidu.com/view/40687.htm Debian 官网：http://www.debian.org/index.zh-cn.html Debian 自我介绍：https://www.debian.org/intro/about 关键字： Debian 社群契约 何谓自由 (Free)？ 或者说，何谓自由软件 (Free Software)？ 什么是自由软件？ Debian 历史 Debian 的发行版介绍：https://www.debian.org/releases/ Debian 官网稳定版下载 1：https://www.debian.org/distrib/ Debian 官网稳定版下载 2：https://www.debian.org/CD/http-ftp/#stable Debian 中国镜像 1：http://mirrors.hust.edu.cn/debian-cd/ Debian 中国镜像 2：ftp://mirrors.sohu.com/debian-cd/ Debian 中国镜像 3：ftp://debian.ustc.edu.cn/debian-cd/ Debian 中文安装手册：https://www.debian.org/releases/stable/amd64/ Debian 软件列表：https://packages.debian.org/stable/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Ubuntu-Install.html":{"url":"Linux-Tutorial/markdown-file/Ubuntu-Install.html","title":"Ubuntu 安装","keywords":"","body":"Ubuntu 安装和分区 先下载该系列教程（提取码：8qfg）：https://pan.baidu.com/s/1EHsgLsUZGOBzwNcEuDUKWA 找到如图箭头目录上的两个视频，并看完，你对 Ubuntu 的安装就有了一个大概的了解，视频中 Ubuntu 虽然版本较早 13.04 的， 但是没关系，对于 Ubuntu 来讲新旧版本安装基本都一样的，所以别担心，驱动的问题也别担心，我们不是要在 Ubuntu 打游戏的，所以常见驱动系统是已经帮我们集成的不会影响使用。但是分区这一块的话，我个人建议是手工分区，视频中没有最终执行手动分区，只是演示了一下又返回了。 我个人是要求你手动分区的。 但是再讲分区之前，用什么设备安装是第一前提，我这里推荐用 U 盘，你准备一个 4 G 以上的 U 盘，把 Ubuntu 系统进行格式化到里面，用这个 U 盘作为安装盘进行安装。这个过程不难，具体看如下文章： http://www.Ubuntukylin.com/ask/index.php?qa=jc_1 http://www.wubantu.com/36bc2075036fab76.html http://tieba.baidu.com/p/2795415868 http://www.Ubuntukylin.com/public/pdf/UK1410install.pdf 好了假设你现在已经格式化好 U 盘，现在可以开始讲分区了。这里特别说明的是有多个硬盘的，多块硬盘分区方案就没视频中那么简单，特别是 Linux 的盘符不了解的就更加难了，所以看下图： 以我这边为例：我这边机子的硬盘是：一个 128 G 固态 + 500 G 的机械，我给一个分区方案给你们参考。下面的内容需要你先看过视频才能看懂： Linux 一般可分 3 个分区，分别是 boot 分区、swap 分区 和 根分区（根分区也就是斜杠/） boot 是主分区类型，swap 是是逻辑分区，/ 是逻辑分区，其他如果你还想划分的也都是逻辑分区。 最近年代生产的的主板，可能还需要分出一个 EFI 分区启动。EFI 的选择和 swap 一样，都在那个下拉菜单中。 怎么判断你要不要分出一个 EFI 呢？如果你根据我的要求分了 boot，swap，根之后，点击下一步报错，有提示 EFI 相关的错误信息，那就分一个给它，我这边就是有报错的。 120 G 固态硬盘： /boot == 1G（主分区），这里为boot单独挂载是有必要的。系统崩溃的时候，损坏的是这个分区。我们重装系统之后，其他分区我们保留下来，重新挂载上去就可以用了。 /EFI == 100M（主分区）（我有提示报错需要分这个，我就分了） /swap == 12G（逻辑分区）一般大家的说法这个大小是跟你机子的内存大小相关的，也有说法内存大不需要这个，但是还是建议分，我内存是12G，所以我分12G。 / == 100G（逻辑分区） 500 G 机械硬盘： /home == 500G（逻辑分区） 分区后的安装都是下一步的，而且 Ubuntu kylin 还是中文的说明，所以没啥难度。 到此假设你给自己的电脑安装了 Ubuntu，那下一讲我将讲 Ubuntu 的相关设置。 如果你想用 VMware 虚拟机安装，这个教程推荐给你，讲得很详细。 http://www.jikexueyuan.com/course/1583.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/ubuntu-settings/ubuntu-settings-toc.html":{"url":"Linux-Tutorial/markdown-file/ubuntu-settings/ubuntu-settings-toc.html","title":"Ubuntu 设置（目录）","keywords":"","body":" Ubuntu 网络相关设置问题 Ubuntu 源设置 Ubuntu 给 Dash 添加程序图标 Ubuntu 常用设置 Ubuntu 常见桌面环境方案 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/kali-linux-settings/kali-linux-toc.html":{"url":"Linux-Tutorial/markdown-file/kali-linux-settings/kali-linux-toc.html","title":"Kali Linux 介绍和设置（目录）","keywords":"","body":"Kali Linux 介绍 官网：https://www.kali.org/ 基于 Debian 设计用于数字鉴识和渗透测试，预装了很多渗透测试程序 支持 x86 和 ARM 架构 官网下载：https://www.kali.org/downloads/ 镜像名：Kali Linux 64 Bit，默认是用 GNOME 桌面，比较华丽，相对较卡（好点电脑推荐，习惯 Ubuntu 的基本都会用） 镜像名：Kali Linux Xfce 64 Bit，默认是用 Xfce 桌面，比较简洁，相对不卡（配置较差的推荐，我这里一台笔记本配置较差，用的就是这个） 镜像名：Kali Linux Kde 64 Bit，默认是用 Kde 桌面，比较华丽，相对较卡（不推荐） 镜像名：Kali Linux Mate 64 Bit，默认是用 Mate 桌面，比较华丽，相对较卡（不推荐） 镜像名：Kali Linux Lxde 64 Bit，默认是用 Lxde 桌面，比较简洁，相对不卡（类似 Windows 风格的桌面，不推荐） 学习过程 Kali Linux 系统安装 Kali Linux 基础设置 Kali Linux 渗透测试思路 其他资料 https://zh.wikipedia.org/wiki/Kali_Linux Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/CentOS.html":{"url":"Linux-Tutorial/markdown-file/CentOS.html","title":"CentOS 介绍","keywords":"","body":"CentOS 介绍 CentOS Wiki：https://zh.wikipedia.org/zh/CentOS CentOS 百科：http://baike.baidu.com/item/centos CentOS 官网：https://www.centos.org/ CentOS 的自我介绍：https://wiki.centos.org/zh/About CentOS 官网下载：https://www.centos.org/download/ CentOS 官网下载（含有旧版本）：https://wiki.centos.org/Download CentOS 国内镜像下载：http://isoredirect.centos.org/centos/6/isos/x86_64/ 版本更新记录：https://wiki.centos.org/Manuals/ReleaseNotes CentOS 原型系统：Red Hat Enterprise Linux Red Hat Enterprise Linux Wiki：https://zh.wikipedia.org/wiki/Red_Hat_Enterprise_Linux Red Hat Enterprise Linux 百科：http://baike.baidu.com/view/1139590.htm 衍生版本：https://zh.wikipedia.org/wiki/Red_Hat_Enterprise_Linux衍生版本 Red Hat Enterprise Linux 后花园系统：Fedora Fedora Wiki：https://zh.wikipedia.org/wiki/Fedora Fedora 百科：http://baike.baidu.com/view/182182.htm Fedora 官网：https://fedoraproject.org Fedora 官网桌面版下载：https://getfedora.org/zh_CN/workstation/download/ Fedora 官网文档：https://docs.fedoraproject.org/zh-CN/index.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/CentOS-Install.html":{"url":"Linux-Tutorial/markdown-file/CentOS-Install.html","title":"CentOS 6 安装","keywords":"","body":"CentOS 安装 概括 本教程中主要演示了 VirtualBox 和 VMware Workstation 下安装 CentOS 6.6 的过程。 VirtualBox 是免费开源，我个人在使用经历中遇到过很多次崩溃、安装有问题等，所以它不是我主推的虚拟机 VMware Workstation 是商业软件，很好用，一些辅助功能也很到位，主推该虚拟机。 如果你是要安装到 PC 机中，你需要准备一个 U 盘，以及下载这个软件：USBWriter（提取码：5aa2） USBWriter 的使用很简单，如下图即可制作一个 CentOS 系统盘 VirtualBox 下安装 CentOS 过程 VirtualBox 的介绍和下载 官网：https://www.virtualbox.org/ wiki：https://zh.wikipedia.org/zh/VirtualBox 百度 wiki：http://baike.baidu.com/view/1047853.htm 百度云下载（32 位和 64 位一体）：http://pan.baidu.com/s/1kTR3hOj 官网下载：https://www.virtualbox.org/wiki/Downloads VirtualBox 安装细节开始： 如上图标注 1 所示：点击 新建 一个虚拟机。 如上图标注 2 所示：在填写 名称 的时候，输入 CentOS 相关字眼的时候，下面的版本类型会跟着变化，自动识别为 Red Hat 如上图标注 1 所示：我们装的是 64 位系统，如果你本机内存足够大，建议可以给 2 ~ 4 G 的容量 如上图所示，命名最好规范，最好要带上系统版本号，硬盘大小建议在 8 ~ 20 G 如上图 gif 演示，当我们创建好虚拟机之后，需要选择模拟光驱进行系统镜像安装 如上图箭头所示，我们这里选择 Skip，跳过对镜像的检查，这个一般需要检查很久，而且我们用 iso 文件安装的，一般没这个必要 如上图标注 1 所示，建议 Linux 纯新手使用简体中文环境 如上图标注 1 所示，因为我们是用全新的虚拟机，虚拟出来的硬盘都是空的，所以这里可以忽略所有数据 再次强调下，root 账号也就是图上所说的 根账号，它是最顶级账号，密码最好别忘记了 如上图标注 1 所示，因为我们是用全新的虚拟机，所以这里选择 使用所有空间 ，然后 CentOS 会按它默认的方式进行分区 Desktop 代表：图形界面版，会默认安装很多软件，建议新手选择此模式，我后面其他文章的讲解都是基于此系统下，如果你不是此模式的系统可能在安装某些软件的时候会出现某些依赖包没有安装的情况 basic sever 代表：命令行界面，有良好基础的基本都会喜欢这个 上一步我选择的是 Desktop 所以有很多软件需要安装，这个过程大概需要 5 ~ 10 分钟 安装完成 安装完成后一定要把盘片删除，防止系统启动的时候去读盘，重新进入安装系统模式 VMware 下安装 CentOS 过程 VMware Workstation 的介绍和下载 官网：https://www.vmware.com/products/workstation wiki：https://zh.wikipedia.org/wiki/VMware_Workstation 百度 wiki：http://baike.baidu.com/view/555554.htm 百度云下载（64 位）：http://pan.baidu.com/s/1eRuJAFK 官网下载：http://www.vmware.com/products/workstation/workstation-evaluation VMware 安装细节开始： 默认 VMware 选择的是 典型 我不推荐，我下面的步骤是选择 自定义（高级）。如果你非常了解 Linux 系统倒是可以考虑选择 典型，在它默认安装完的系统上可以很好地按你的需求进行修改 牢记：不要在这一步就选择镜像文件，不然也会直接进入 典型 模式，直接按它的规则进行系统安装 桥接模式：（建议选择此模式）创建一个独立的虚拟主机，在桥接模式下的虚拟主机网络一般要为其配 IP 地址、子网掩码、默认网关（虚拟机ip地址要与主机ip地址处于同网段） NAT 模式：把物理主机作为路由器进行访问互联网，优势:联网简单，劣势:虚拟主机无法与物理主机通讯 主机模式：把虚拟主机网络与真实网络隔开，但是各个虚拟机之间可以互相连接，虚拟机和物理机也可以连接 上面解释来源：http://jingyan.baidu.com/article/3f16e003cd0a0d2591c103b4.html Buslogic 和 LSIlogic 都是虚拟硬盘 SCSI 设备的类型，旧版本的 OS 默认的是 Buslogic，LSIlogic 类型的硬盘改进了性能，对于小文件的读取速度有提高，支持非 SCSI 硬盘比较好。 上面解释来源：http://www.cnblogs.com/R-zqiang/archive/2012/11/23/2785134.html 强烈建议至少要给 20 G，不然装不了多少软件的 如上图 gif 所示，在创建完虚拟机之后，我们要加载系统镜像，然后启动虚拟机进行安装，接下来的安装步骤跟上面使用 VirtualBox 安装细节基本一样，不一样的地方是我这里选择的是自定义分区，不再是选择默认分区方式 如上图箭头所示，这里我们选择自定义分区方式 如上图 gif 所示，我只把最基本的区分出来，如果你有自己的需求可以自己设置 简单分区方案： /boot == 500 M（主分区） /swap == 2 G（逻辑分区）一般大家的说法这个大小是跟你机子的内存大小相关的，也有说法内存大不需要这个，但是还是建议分，虚拟机内存 2 G，所以我分 2 G / == 剩余容量（逻辑分区）剩余的空间都给了根分区 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/CentOS-7-Install.html":{"url":"Linux-Tutorial/markdown-file/CentOS-7-Install.html","title":"CentOS 7 安装","keywords":"","body":"CentOS 7 安装 概括 本教程中主要演示了 VMware Workstation 下安装 CentOS 7.3 的过程。 VMware 的使用细节可以看这篇：CentOS 6 安装 如果你是要安装到 PC 机中，你需要准备一个 U 盘，以及下载这个软件：USBWriter（提取码：5aa2） USBWriter 的使用很简单，如下图即可制作一个 CentOS 系统盘 VMware 下安装 CentOS 过程 VMware Workstation 的介绍和下载 官网：https://www.vmware.com/products/workstation wiki：https://zh.wikipedia.org/wiki/VMware_Workstation 百度 wiki：http://baike.baidu.com/view/555554.htm 百度云下载（64 位）：http://pan.baidu.com/s/1eRuJAFK 官网下载：http://www.vmware.com/products/workstation/workstation-evaluation 安装细节开始： 如上图，默认是最小安装，点击进去，选择桌面安装。 如上图，默认是自动分区，如果懂得分区，点击进去，进行手动分区，CentOS 7 少了主分区，逻辑分区的选择了。 如上图，root 密码必须设置，我习惯测试的时候是：123456 我没有创建用户，喜欢用 root 如上图，许可证必须点击进去勾选同意相关协议。 如上图，网络可以稍后在设置，主机名可以现在先填写 如上图右上角，一般我们都选择跳过 到此完成，其他该做啥就做啥 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/CentOS6-and-CentOS7.html":{"url":"Linux-Tutorial/markdown-file/CentOS6-and-CentOS7.html","title":"CentOS 6 和 CentOS 7 差异","keywords":"","body":"CentOS 6 和 CentOS 7 介绍 总体差异 想装回过去的一些工具 安装：yum install -y tree net-tools bind-utils tree sysstat vim-en* lrzsz NetworkManager-tui ntp ntpdate iftop tcpdump telnet traceroute 查看版本号/主机名 cat /etc/redhat-release cat /etc/hostname 常用配置差异 CentOS 网络配置 CentOS 图形界面的关闭与开启 systemctl 的用法 相当于 CentOS 6 的：service nginx stop systemctl is-enabled iptables.service #查询服务是否开机启动 systemctl enable iptables.service #开机运行服务 systemctl disable iptables.service #取消开机运行 systemctl start iptables.service #启动服务 systemctl stop iptables.service #停止服务 systemctl restart iptables.service #重启服务 systemctl reload iptables.service #重新加载服务配置文件 systemctl status iptables.service #查询服务运行状态 systemctl --failed #显示启动失败的服务 systemctl list-units --type=service #查看所有服务 systemctl is-enabled httpd #查看httpd服务是否开机启动 对于启动脚本的存放位置，也不再是 /etc/init.d/（这个目录也是存在的），而是 /usr/lib/systemd/system/ 开放端口 一般设置软件端口有一个原则： 0 ~ 1024 系统保留，一般不要用到 1024 ~ 65535（2^16） 可以随意用 添加单个端口：firewall-cmd --zone=public --add-port=8883/tcp --permanent 添加范围端口：firewall-cmd --zone=public --add-port=8883-8885/tcp --permanent 删除端口：firewall-cmd --zone=public --remove-port=8883/tcp --permanent 重启防火墙：firewall-cmd --reload 命令解释： --zone #作用域 --add-port=80/tcp #添加端口，格式为：端口/通讯协议 --permanent #永久生效，没有此参数重启后失效 列出所有端口列表：firewall-cmd --list-all 关闭 firewall 使用 iptables 关闭 firewall systemctl stop firewalld.service #停止firewall systemctl disable firewalld.service #禁止firewall开机启动 安装 iptables yum install -y iptables-services 启动 iptables systemctl restart iptables.service #最后重启防火墙使配置生效 systemctl enable iptables.service #设置防火墙开机启动 其他使用照旧 ifconfig 没有了 查看网络配置：ip a 装回 ifconfig：yum install -y net-tools 设置时区 timedatectl set-timezone Asia/Shanghai timedatectl status 资料 http://blog.topspeedsnail.com/archives/3017 http://chenbaocheng.com/2015/07/15/Centos-7-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEiptables/ http://cuidehua.blog.51cto.com/5449828/1858374 http://putty.biz/760 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/centos-settings/centos-settings-toc.html":{"url":"Linux-Tutorial/markdown-file/centos-settings/centos-settings-toc.html","title":"CentOS 设置（目录）","keywords":"","body":" CentOS 网络设置 CentOS 源设置 CentOS 图形界面的关闭与开启 清除系统缓存 修改定时清理 /tmp 目录下的文件 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Ubuntu-Install-VMware.html":{"url":"Linux-Tutorial/markdown-file/Ubuntu-Install-VMware.html","title":"Ubuntu 安装 VMware","keywords":"","body":"Ubuntu 安装 VMware 我个人习惯使用 VMware，在一些个性化和兼容性上，我觉得 VMware 比 box 好很多 安装说明 下载：VMware-Workstation-Full-10.0.4-2249910.x86_64.bundle 下载地址（rzb0）：http://pan.baidu.com/s/1dFuLD2D 安装组件： 命令：sudo apt-get update 命令：sudo apt-get install build-essential linux-headers-`uname -r` 进入软件下载后目录，比如我在 /opt/setups 下 cd /opt/setups chmod +x VMware*.bundle sudo ./VMware*.bundle 接下来会弹出图形界面，则效果跟 Windows 一样，那就没啥好说了。 资料 How To Install VMware Workstation 11 On Ubuntu 14.10 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/CentOS-Virtual-Machine-Copy-Settings.html":{"url":"Linux-Tutorial/markdown-file/CentOS-Virtual-Machine-Copy-Settings.html","title":"VMware 克隆 CentOS 后网卡信息修改","keywords":"","body":"VMware 克隆 CentOS 后网卡信息修改 概述 在我们需要多台 CentOS 虚拟机的时候，对已有虚拟机的系统进行克隆或是复制。但是这样做又有一个问题，克隆出来的虚拟机启动的时候你输入命令：ifconfig，eth0 网卡信息没了，只有一个 eth1。 对于处女座的人来讲这是不允许的。所以我们需要改动下。 复制虚拟机后，首次打开该会提示如下内容，一般选择 copy 这个配置。 CentOS 6 修改方法 设置 hostname：hostnamectl --static set-hostname linux02 命令：sudo vim /etc/udev/rules.d/70-persistent-net.rules 该文件中正常此时应该有两行信息 在文件中把 NAME=\"eth0″ 的这一行注释掉 对于另一行，把 NAME=”eth1″ 的这一行，把 NAME=”eth1″ 改为 NAME=”eth0″，并且把该行：ATTRS{address}==\"00:0c:29:4c:46:01″ 这个属性信息记下来，每台机子都不一样，我这段信息只是例子，你不要直接复制我的。 命令：sudo vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改 IP 地址 把 HWADDR 的值改为上面要求记下来的：00:0c:29:4c:46:01 命令：nmcli con > /opt/info.txt 如果显示两行 UUID 的信息的话，复制不是 System eth0 的那个 UUID 值，下面有用。 编辑：sudo vim /etc/sysconfig/network-scripts/ifcfg-eth0 把文件中的 UUID 值 改为上面要求复制的 UUID 值。 保存配置文件，重启系统，正常应该是可以了。 CentOS 7 修改方法 在 VMware 15 Pro 的情况下，直接 copy 进行后，直接修改网卡配置即可 编辑该文件：vim /etc/sysconfig/network-scripts/ifcfg-ens33 把 ip 地址修改即可 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Vim-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Vim-Install-And-Settings.html","title":"Vim 安装、配置、快捷键列表","keywords":"","body":"Vim 安装和配置、优化 Vim 介绍 Vim 官网：http://www.vim.org/ Vim 安装 CentOS：sudo yum install -y vim Ubuntu：sudo apt-get install -y vim Windows GVim 下载：http://www.xiazaiba.com/html/3347.html Vim 配置（CentOS 环境） 编辑配置文件是：sudo vim /etc/vimrc Vim 基础快捷键 注意 严格区分字母大小写 含有 Ctrl 字眼都表示 Ctrl 键盘按钮 特定符号需要配合 Shift 键，比如字母键盘区上面的数字区：!@#%%^&*() 要按出冒号键 : 也是需要 Shift 的 移动 j，下 k，上 h，左 l，右 v，按 v 之后按方向键可以选中你要选中的文字 gg，跳到第 1 行 G，跳到最后一行 16G 或 :16，跳到第 16 行 $，到本行 行尾 0，到本行 行头 w，到下一个单词的 开头 e，到下一个单词的 结尾 Ctrl + u，向文件 首翻 半屏 Ctrl + d，向文件 尾翻 半屏 Ctrl + f，向文件 尾翻 一屏 Ctrl + b，向文件 首翻 一屏 *，匹配光标当前所在的单词，移动光标到 下一个 匹配单词 #，匹配光标当前所在的单词，移动光标到 上一个 匹配单词 ^，到本行第一个单词头 g_，到本行最后一个单词尾巴 %，匹配括号移动，包括 (、{、[ 插入 I，在当前 行首 插入 A，在当前 行尾 插入 i，在当前字符的 左边 插入 a，在当前字符的 右边 插入 o，在当前行 下面 插入一个新行 O，在当前行 上面 插入一个新行 编辑 删除 x，删除 光标后 的 1 个字符 2x，删除 光标后 的 2 个字符 X，删除 光标前 的 1 个字符 2X，删除 光标前 的 2 个字符 dd，删除当前行 cc，删除当前行后进入 insert 模式 dw，删除当前光标下的单词/空格 d$，删除光标至 行尾 所有字符 dG，删除光标至 文件尾 所有字符 3dd，从当前光标开始，删掉 3 行 echo > aa.txt，从 bash 角度清空文件内容，这个比较高效 复制 y，复制光标所选字符 yw，复制光标后单词 yy，复制当前行 4yy，复制当前行及下面 4 行 y$，复制光标位置至 行尾 的内容 y^，复制光标位置至 行首 的内容 粘贴 p，将粘贴板中内容复制到 光标之后 P，将粘贴板中内容复制到 光标之前 其他 ddp，交换当前光标所在行和下一行的位置 u，撤销 :wq，退出并 保存 :q!，退出并 不保存 Ctrl + v，进入 Vim 列编辑 guu，把当前行的字母全部转换成 小写 gUU，把当前行的字母全部转换成 大写 g~~，把当前行的字母是大写的转换成小写，是小写的转换成大写 :saveas /opt/setups/text.txt，另存到 /opt/setups/text.txt 搜索 /YouMeek，从光标开始处向文件尾搜索 YouMeek 字符，按 n 继续向下找，按 N 继续向上找 ?YouMeek，从光标开始处向文件首搜索 YouMeek 字符，按 n 继续向下找，按 N 继续向上找 替换 :%s/YouMeek/Judasn/g，把文件中所有 YouMeek 替换为：Judasn :%s/YouMeek/Judasn/，把文件中所有行中第一个 YouMeek 替换为：Judasn :s/YouMeek/Judasn/，把光标当前行第一个 YouMeek 替换为 Judasn :s/YouMeek/Judasn/g，把光标当前行所有 YouMeek 替换为 Judasn :s#YouMeek/#Judasn/#，除了使用斜杠作为分隔符之外，还可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符，该命令表示：把光标当前行第一个 YouMeek/ 替换为 Judasn/ :10,31s/YouMeek/Judasn/g，把第 10 行到 31 行之间所有 YouMeek 替换为 Judasn Vim 的特殊复制、黏贴 Vim 提供了 12 个剪贴板，分别是：0,1,2,3,4,5,6,7,8,9,a,\"，默认采用的是 \"，也就是双引号，可能你初读感觉很奇怪。你可以用 Vim 编辑某个文件，然后输入：:reg。你可以看到如下内容： 复制到某个剪切板的命令：\"7y，表示使用 7 号剪切板。 黏贴某个剪切板内容：\"7p，表示使用 7 号剪切板内容进行黏贴 Vim 配置 我个人本地不使用 Vim 的，基本上都是在操作服务器的时候使用，所以这里推荐这个配置文件 vim-for-server 在假设你已经备份好你的 Vim 配置文件后，使用该配置文件：curl https://raw.githubusercontent.com/wklken/vim-for-server/master/vimrc > ~/.vimrc 效果如下： 需要特别注意的是，如果你平时粘贴内容到终端 Vim 出现缩进错乱，一般需要这样做： 进入 vim 后，按 F5，然后 shift + insert 进行粘贴。这种事就不会错乱了。 原因是：vim ~/.vimrc 中有一行这样的设置：set pastetoggle= 其他常用命令 对两个文件进行对比：vimdiff /opt/1.txt /opt/2.txt 资料 vim几个小技巧（批量替换，列编辑） 最佳vim技巧 简明 Vim 练级攻略 vim 批量查找替换 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/SSH-login-without-password.html":{"url":"Linux-Tutorial/markdown-file/SSH-login-without-password.html","title":"SSH 免密登录","keywords":"","body":"SSH 免密登录 环境说明 CentOS 7.3 关键点 免密登录的关键点在于理解谁登录谁。 A 生成的公钥给 B，也给 C、D，则 A 可以直接免密 SSH 登录 B、C、D A 生成密钥 在 A 机器上输入命令：ssh-keygen 根据提示回车，共有三次交互提示，都回车即可。 生成的密钥目录在：/root/.ssh 写入：cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys 测试：ssh localhost 把 A 的公钥发给 B 假设 B 机器的 ip：192.168.1.105 则在 A 机器上输入：ssh-copy-id -i /root/.ssh/id_rsa.pub -p 22 root@192.168.1.105，根据提示输入 B 机器的 root 密码，成功会有相应提示 测试 A 免密登录到 B 在 A 机器上输入命令：ssh -p 22 root@192.168.1.105，则会相应登录成功的提示 如果是用 pem 登录的话，用 ssh-copy-id 是无法使用的 先保存 A 的 pub 到本地：sz /root/.ssh/id_rsa.pub 登录 B 机子：cd /root/.ssh/ 如果 B 机子没有 authorized_keys 文件则创建：touch /root/.ssh/authorized_keys 设置权限：chmod 600 /root/.ssh/authorized_keys 上传 pub 文件到 B 机子，并在 B 机子上执行：cd /root/.ssh/ && cat id_rsa.pub >> authorized_keys Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Bash.html":{"url":"Linux-Tutorial/markdown-file/Bash.html","title":"Bash 命令","keywords":"","body":"Bash 常用命令 基础常用命令 某个命令 --h，对这个命令进行解释 某个命令 --help，解释这个命令(更详细) man某个命令，文档式解释这个命令(更更详细)(执行该命令后,还可以按/+关键字进行查询结果的搜索) Ctrl + c，结束命令 TAB键，自动补全命令（按一次自动补全，连续按两次，提示所有以输入开头字母的所有命令） 键盘上下键，输入临近的历史命令 history，查看所有的历史命令 Ctrl + r，进入历史命令的搜索功能模式 clear，清除屏幕里面的所有命令 pwd，显示当前目录路径（常用） firefox&，最后后面的 & 符号，表示使用后台方式打开 Firefox，然后显示该进程的 PID 值 jobs，查看后台运行的程序列表 ifconfig，查看内网 IP 等信息（常用） curl ifconfig.me，查看外网 IP 信息 curl ip.cn，查看外网 IP 信息 locate 搜索关键字，快速搜索系统文件/文件夹（类似 Windows 上的 everything 索引式搜索）（常用） updatedb，配合上面的 locate，给 locate 的索引更新（locate 默认是一天更新一次索引）（常用） date，查看系统时间（常用） date -s20080103，设置日期（常用） date -s18:24，设置时间，如果要同时更改 BIOS 时间，再执行 hwclock --systohc（常用） cal，在终端中查看日历，肯定没有农历显示的 uptime，查看系统已经运行了多久，当前有几个用户等信息（常用） cat 文件路名，显示文件内容（属于打印语句） cat -n 文件名，显示文件，并每一行内容都编号 more 文件名，用分页的方式查看文件内容（按 space 翻下一页，按 Ctrl + B 返回上页） less文件名，用分页的方式查看文件内容（带上下翻页） 按 j 向下移动，按 k 向上移动 按 / 后，输入要查找的字符串内容，可以对文件进行向下查询，如果存在多个结果可以按 n 调到下一个结果出 按 ？ 后，输入要查找的字符串内容，可以对文件进行向上查询，如果存在多个结果可以按 n 调到下一个结果出 shutdown shutdown -hnow，立即关机 shutdown -h+10，10 分钟后关机 shutdown -h23:30，23:30 关机 shutdown -rnew，立即重启 poweroff，立即关机（常用） reboot，立即重启（常用） zip mytest.zip /opt/test/，把 /opt 目录下的 test/ 目录进行压缩，压缩成一个名叫 mytest 的 zip 文件 unzip mytest.zip，对 mytest.zip 这个文件进行解压，解压到当前所在目录 unzip mytest.zip -d /opt/setups/，对 mytest.zip 这个文件进行解压，解压到 /opt/setups/ 目录下 tar -cvf mytest.tar mytest/，对 mytest/ 目录进行归档处理（归档和压缩不一样） tar -xvf mytest.tar，释放 mytest.tar 这个归档文件，释放到当前目录 tar -xvf mytest.tar -C /opt/setups/，释放 mytest.tar 这个归档文件，释放到 /opt/setups/ 目录下 last，显示最近登录的帐户及时间 lastlog，显示系统所有用户各自在最近登录的记录，如果没有登录过的用户会显示 从未登陆过 ls，列出当前目录下的所有没有隐藏的文件 / 文件夹。 ls -a，列出包括以.号开头的隐藏文件 / 文件夹（也就是所有文件） ls -R，显示出目录下以及其所有子目录的文件 / 文件夹（递归地方式，不显示隐藏的文件） ls -a -R，显示出目录下以及其所有子目录的文件 / 文件夹（递归地方式，显示隐藏的文件） ls -al，列出目录下所有文件（包含隐藏）的权限、所有者、文件大小、修改时间及名称（也就是显示详细信息） ls -ld 目录名，显示该目录的基本信息 ls -t，依照文件最后修改时间的顺序列出文件名。 ls -F，列出当前目录下的文件名及其类型。以 / 结尾表示为目录名，以 * 结尾表示为可执行文件，以 @ 结尾表示为符号连接 ls -lg，同上，并显示出文件的所有者工作组名。 ls -lh，查看文件夹类文件详细信息，文件大小，文件修改时间 ls /opt | head -5，显示 opt 目录下前 5 条记录 ls -l | grep '.jar'，查找当前目录下所有 jar 文件 ls -l /opt |grep \"^-\"|wc -l，统计 opt 目录下文件的个数，不会递归统计 ls -lR /opt |grep \"^-\"|wc -l，统计 opt 目录下文件的个数，会递归统计 ls -l /opt |grep \"^d\"|wc -l，统计 opt 目录下目录的个数，不会递归统计 ls -lR /opt |grep \"^d\"|wc -l，统计 opt 目录下目录的个数，会递归统计 ls -lR /opt |grep \"js\"|wc -l，统计 opt 目录下 js 文件的个数，会递归统计 ls -l，列出目录下所有文件的权限、所有者、文件大小、修改时间及名称（也就是显示详细信息，不显示隐藏文件）。显示出来的效果如下： -rwxr-xr-x. 1 root root 4096 3月 26 10:57，其中最前面的 - 表示这是一个普通文件 lrwxrwxrwx. 1 root root 4096 3月 26 10:57，其中最前面的 l 表示这是一个链接文件，类似 Windows 的快捷方式 drwxr-xr-x. 5 root root 4096 3月 26 10:57，其中最前面的 d 表示这是一个目录 cd，目录切换 cd ..，改变目录位置至当前目录的父目录(上级目录)。 cd ~，改变目录位置至用户登录时的工作目录。 cd 回车，回到家目录 cd -，上一个工作目录 cd dir1/，改变目录位置至 dir1 目录下。 cd ~user，改变目录位置至用户的工作目录。 cd ../user，改变目录位置至相对路径user的目录下。 cd /../..，改变目录位置至绝对路径的目录位置下。 cp 源文件 目标文件，复制文件 cp -r 源文件夹 目标文件夹，复制文件夹 cp -r -v 源文件夹 目标文件夹，复制文件夹(显示详细信息，一般用于文件夹很大，需要查看复制进度的时候) cp /usr/share/easy-rsa/2.0/keys/{ca.crt,server.{crt,key},dh2048.pem,ta.key} /etc/openvpn/keys/，复制同目录下花括号中的文件 tar cpf - . | tar xpf - -C /opt，复制当前所有文件到 /opt 目录下，一般如果文件夹文件多的情况下用这个更好，用 cp 比较容易出问题 mv 文件 目标文件夹，移动文件到目标文件夹 mv 文件，不指定目录重命名后的名字，用来重命名文件 touch 文件名，创建一个空白文件/更新已有文件的时间(后者少用) mkdir 文件夹名，创建文件夹 mkdir -p /opt/setups/nginx/conf/，创建一个名为 conf 文件夹，如果它的上级目录 nginx 没有也会跟着一起生成，如果有则跳过 rmdir 文件夹名，删除文件夹(只能删除文件夹里面是没有东西的文件夹) rm 文件，删除文件 rm -r 文件夹，删除文件夹 rm -r -i 文件夹，在删除文件夹里的文件会提示(要的话,在提示后面输入yes) rm -r -f 文件夹，强制删除 rm -r -f 文件夹1/ 文件夹2/ 文件夹3/删除多个 find，高级查找 find . -name *lin*，其中 . 代表在当前目录找，-name 表示匹配文件名 / 文件夹名，*lin* 用通配符搜索含有lin的文件或是文件夹 find . -iname *lin*，其中 . 代表在当前目录找，-iname 表示匹配文件名 / 文件夹名（忽略大小写差异），*lin* 用通配符搜索含有lin的文件或是文件夹 find / -name *.conf，其中 / 代表根目录查找，*.conf代表搜索后缀会.conf的文件 find /opt -name .oh-my-zsh，其中 /opt 代表目录名，.oh-my-zsh 代表搜索的是隐藏文件 / 文件夹名字为 oh-my-zsh 的 find /opt -type f -iname .oh-my-zsh，其中 /opt 代表目录名，-type f 代表只找文件，.oh-my-zsh 代表搜索的是隐藏文件名字为 oh-my-zsh 的 find /opt -type d -iname .oh-my-zsh，其中 /opt 代表目录名，-type d 代表只找目录，.oh-my-zsh 代表搜索的是隐藏文件夹名字为 oh-my-zsh 的 find . -name \"lin*\" -exec ls -l {} \\;，当前目录搜索lin开头的文件，然后用其搜索后的结果集，再执行ls -l的命令（这个命令可变，其他命令也可以），其中 -exec 和 {} \\; 都是固定格式 find /opt -type f -size +800M -print0 | xargs -0 du -h | sort -nr，找出 /opt 目录下大于 800 M 的文件 find / -name \"*tower*\" -exec rm {} \\;，找到文件并删除 find . -name \"*\" |xargs grep \"youmeek\"，递归查找当前文件夹下所有文件内容中包含 youmeek 的文件 find . -size 0 | xargs rm -f &，删除当前目录下文件大小为0的文件 du -hm --max-depth=2 | sort -nr | head -12，找出系统中占用容量最大的前 12 个目录 cat /etc/resolv.conf，查看 DNS 设置 netstat -tlunp，查看当前运行的服务，同时可以查看到：运行的程序已使用端口情况 env，查看所有系统变量 export，查看所有系统变量 echo echo $JAVA_HOME，查看指定系统变量的值，这里查看的是自己配置的 JAVA_HOME。 echo \"字符串内容\"，输出 \"字符串内容\" echo > aa.txt，清空 aa.txt 文件内容（类似的还有：: > aa.txt，其中 : 是一个占位符, 不产生任何输出） unset $JAVA_HOME，删除指定的环境变量 ln -s /opt/data /opt/logs/data，表示给 /opt/logs 目录下创建一个名为 data 的软链接，该软链接指向到 /opt/data grep shell grep -H '安装' *.sh，查找当前目录下所有 sh 类型文件中，文件内容包含 安装 的当前行内容 grep 'test' java*，显示当前目录下所有以 java 开头的文件中包含 test 的行 grep 'test' spring.ini docker.sh，显示当前目录下 spring.ini docker.sh 两个文件中匹配 test 的行 ps ps –ef|grep java，查看当前系统中有关 java 的所有进程 ps -ef|grep --color java，高亮显示当前系统中有关 java 的所有进程 kill kill 1234，结束 pid 为 1234 的进程 kill -9 1234，强制结束 pid 为 1234 的进程（慎重） killall java，结束同一进程组内的所有为 java 进程 ps -ef|grep hadoop|grep -v grep|cut -c 9-15|xargs kill -9，结束包含关键字 hadoop 的所有进程 head head -n 10 spring.ini，查看当前文件的前 10 行内容 tail tail -n 10 spring.ini，查看当前文件的后 10 行内容 tail -200f 文件名，查看文件被更新的新内容尾 200 行，如果文件还有在新增可以动态查看到（一般用于查看日记文件） 用户、权限-相关命令 使用 pem 证书登录：ssh -i /opt/mykey.pem root@192.168.0.70 证书权限不能太大，不然无法使用：chmod 600 mykey.pem hostname，查看当前登陆用户全名 cat /etc/group，查看所有组 cat /etc/passwd，查看所有用户 groups youmeek，查看 youmeek 用户属于哪个组 useradd youmeek -g judasn，添加用户并绑定到 judasn 组下 userdel -r youmeek，删除名字为 youmeek 的用户 参数：-r，表示删除用户的时候连同用户的家目录一起删除 修改普通用户 youmeek 的权限跟 root 权限一样： 常用方法（原理是把该用户加到可以直接使用 sudo 的一个权限状态而已）： 编辑配置文件：vim /etc/sudoers 找到 98 行（预估），有一个：root ALL=(ALL) ALL，在这一行下面再增加一行，效果如下： root ALL=(ALL) ALL youmeek ALL=(ALL) ALL 另一种方法： 编辑系统用户的配置文件：vim /etc/passwd，找到 root 和 youmeek 各自开头的那一行，比如 root 是：root:x:0:0:root:/root:/bin/zsh，这个代表的含义为：用户名:密码:UserId:GroupId:描述:家目录:登录使用的 shell 通过这两行对比，我们可以直接修改 youmeek 所在行的 UserId 值 和 GroupId 值，都改为 0。 groupadd judasn，添加一个名为 judasn 的用户组 groupdel judasn，删除一个名为 judasn 的用户组（前提：先删除组下面的所有用户） usermod 用户名 -g 组名，把用户修改到其他组下 passwd youmeek，修改 youmeek 用户的密码（前提：只有 root 用户才有修改其他用户的权限，其他用户只能修改自己的） chmod 777 文件名/目录，给指定文件增加最高权限，系统中的所有人都可以进行读写。 linux 的权限分为 rwx。r 代表：可读，w 代表：可写，x 代表：可执行 这三个权限都可以转换成数值表示，r = 4，w = 2，x = 1，- = 0，所以总和是 7，也就是最大权限。第一个 7 是所属主（user）的权限，第二个 7 是所属组（group）的权限，最后一位 7 是非本群组用户（others）的权限。 chmod -R 777 目录 表示递归目录下的所有文件夹，都赋予 777 权限 su：切换到 root 用户，终端目录还是原来的地方（常用） su -：切换到 root 用户，其中 - 号另起一个终端并切换账号 su 用户名，切换指定用户帐号登陆，终端目录还是原来地方。 su - 用户名，切换到指定用户帐号登陆，其中 - 号另起一个终端并切换账号 exit，注销当前用户（常用） sudo 某个命令，使用管理员权限使用命令，使用 sudo 回车之后需要输入当前登录账号的密码。（常用） passwd，修改当前用户密码（常用） 添加临时账号，并指定用户根目录，并只有可读权限方法 添加账号并指定根目录（用户名 tempuser）：useradd -d /data/logs -m tempuser 设置密码：passwd tempuser 回车设置密码 删除用户（该用户必须退出 SSH 才能删除成功），也会同时删除组：userdel tempuser 磁盘管理 df -h，自动以合适的磁盘容量单位查看磁盘大小和使用空间 df -k，以磁盘容量单位 K 为数值结果查看磁盘使用情况 df -m，以磁盘容量单位 M 为数值结果查看磁盘使用情况 du -sh /opt，查看 opt 这个文件夹大小 （h 的意思 human-readable 用人类可读性较好方式显示，系统会自动调节单位，显示合适大小的单位） du -sh ./*，查看当前目录下所有文件夹大小 （h 的意思 human-readable 用人类可读性较好方式显示，系统会自动调节单位，显示合适大小的单位） du -sh /opt/setups/，显示 /opt/setups/ 目录所占硬盘空间大小（s 表示 –summarize 仅显示总计，即当前目录的大小。h 表示 –human-readable 以 KB，MB，GB 为单位，提高信息的可读性） mount /dev/sdb5 /newDir/，把分区 sdb5 挂载在根目录下的一个名为 newDir 的空目录下，需要注意的是：这个目录最好为空，不然已有的那些文件将看不到，除非卸载挂载。 挂载好之后，通过：df -h，查看挂载情况。 umount /newDir/，卸载挂载，用目录名 如果这样卸载不了可以使用：umount -l /newDir/ umount /dev/sdb5，卸载挂载，用分区名 wget 下载文件 常规下载：wget http://www.gitnavi.com/index.html 自动断点下载：wget -c http://www.gitnavi.com/index.html 后台下载：wget -b http://www.gitnavi.com/index.html 伪装代理名称下载：wget --user-agent=\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\" http://www.gitnavi.com/index.html 限速下载：wget --limit-rate=300k http://www.gitnavi.com/index.html 批量下载：wget -i /opt/download.txt，一个下载地址一行 后台批量下载：wget -b -c -i /opt/download.txt，一个下载地址一行 资料 http://wenku.baidu.com/view/1ad19bd226fff705cc170af3.html http://blog.csdn.net/nzing/article/details/9166057 http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/wget.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Bash-Other-Bash.html":{"url":"Linux-Tutorial/markdown-file/Bash-Other-Bash.html","title":"Bash 其他常用命令","keywords":"","body":"Bash 其他常用命令 其他常用命令 编辑 hosts 文件：vim /etc/hosts，添加内容格式：127.0.0.1 www.youmeek.com RPM 文件操作命令： 安装 rpm -i example.rpm，安装 example.rpm 包 rpm -iv example.rpm，安装 example.rpm 包并在安装过程中显示正在安装的文件信息 rpm -ivh example.rpm，安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度 查询 rpm -qa | grep jdk，查看 jdk 是否被安装 rpm -ql jdk，查看 jdk 是否被安装 卸载 rpm -e jdk，卸载 jdk（一般卸载的时候都要先用 rpm -qa 看下整个软件的全名） YUM 软件管理： yum install -y httpd，安装 apache yum remove -y httpd，卸载 apache yum info -y httpd，查看 apache 版本信息 yum list --showduplicates httpd，查看可以安装的版本 yum install httpd-查询到的版本号，安装指定版本 更多命令可以看：http://man.linuxde.net/yum 查看某个配置文件，排除掉里面以 # 开头的注释内容： grep '^[^#]' /etc/openvpn/server.conf 查看某个配置文件，排除掉里面以 # 开头和 ; 开头的注释内容： grep '^[^#;]' /etc/openvpn/server.conf 资料 https://www.jianshu.com/p/180fb11a5b96 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/shell-safe-rm.html":{"url":"Linux-Tutorial/markdown-file/shell-safe-rm.html","title":"安装的 rm（删除）","keywords":"","body":"安装的 rm（删除） 由来 我们都知道 rm -rf 是一个危险的操作，所以我们应该尽可能养成一个不要 rm 的习惯，而是 mv。 设置 创建一个用来存放要被我们删除的文件夹存放地：cd $home && mkdir .trash 赋予最高权限（个人习惯）：chmod 777 .trash 如果你使用 bash，你需要修改你的 home 目录下的：.bashrc 我使用的是 zsh，所以我修改：vim .zshrc，在文件的最后面增加下面内容： # rm transform function rm() { # garbage collect now=$(date +%s) for s in $(ls --indicator-style=none $HOME/.trash/) ;do dir_name=${s//_/-} dir_time=$(date +%s -d $dir_name) # if big than one month then delete if [[ 0 -eq dir_time || $(($now - $dir_time)) -gt 2592000 ]] ;then echo \"Trash \" $dir_name \" has Gone \" /bin/rm $s -rf fi done # add new folder prefix=$(date +%Y_%m_%d) hour=$(date +%H) mkdir -p $HOME/.trash/$prefix/$hour if [[ -z $1 ]] ;then echo 'Missing Args' return fi echo \"Hi, Trashing\" $1 \"to /root/.trash\" mv $1 $HOME/.trash/$prefix/$hour } 刷新配置：source ~/.zshrc 然后断开终端，重新连接 此时如果你使用：rm -rf a.txt 会出现这样的提示： Hi, Trashing -rf to /root/.trash mv: invalid option -- 'r' Try 'mv --help' for more information. 现在我们删除一个测试文件：rm a.txt，会事显示：Hi, Trashing a.txt to /root/.trash 因为我们上面的 shell 每次触发 rm 明白的时候都会去删除一个月前的目录，所以就不需要定时器来删除 .trash 里面的文件了。 如果你要强制删除，清空 .trash 目录，可以使用真正的 rm 命令：/usr/bin/rm -rf ~/.trash/* 资料 http://www.linuxde.net/2013/02/11915.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Sed.html":{"url":"Linux-Tutorial/markdown-file/Sed.html","title":"Sed 命令","keywords":"","body":"Sed 常用命令 轻量级流编辑器，一般用来处理文本类文件 sed 是非交互式的编辑器。它不会修改文件，除非使用 shell 重定向来保存结果。默认情况下，所有的输出行都被打印到屏幕上 用 sed -i 会实际写入，下面为了演示，都没加该参数，有需要可以自行添加。 基础例子 有一个文件：/opt/log4j2.properties status = error # log action execution errors for easier debugging logger.action.name = org.elasticsearch.action logger.action.level = debug appender.console.type = Console appender.console.name = console appender.console.layout.type = PatternLayout appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n appender.rolling.type = RollingFile appender.rolling.name = rolling appender.rolling.fileName = ${sys:es.logs}.log appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%.-10000m%n appender.rolling.filePattern = ${sys:es.logs}-%d{yyyy-MM-dd}.log appender.rolling.policies.type = Policies appender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.time.modulate = true rootLogger.level = info rootLogger.appenderRef.console.ref = console rootLogger.appenderRef.rolling.ref = rolling p 参数表示打印，一般配合 -n（安静模式）进行使用 sed -n '7,10p' /opt/log4j2.properties：显示第 7 ~ 10 行内容 sed -n '7p' /opt/log4j2.properties：显示第 7 行内容 d 删除 cat -n /opt/log4j2.properties |sed '7,10d'：剔除 7 ~ 10 行内容，然后显示文件所有内容出来（实际文件是未删除的） a 追加 cat -n /opt/log4j2.properties |sed '1a GitNavi.com'：追加 GitNavi.com 内容（追加在下一行展示），然后显示文件所有内容出来 c 替换 cat -n /opt/log4j2.properties |sed '1,4c GitNavi.com'：将 1 ~ 4 行内容替换成 GitNavi.com s： 搜索并替换 sed 's/time/timeing/g' /opt/log4j2.properties：将文件中所有 time 替换成 timeing 并展示 sed 's/^#*//g' /opt/log4j2.properties：将文件中每一行以 # 开头的都替换掉空字符并展示 sed 's/^#[ ]*//g' /opt/log4j2.properties：将文件中每一行以 # 开头的，并且后面的一个空格，都替换掉空字符并展示 sed 's/^[ ]*//g' /opt/log4j2.properties：将文件中每一行以空格开头，都替换掉空字符并展示 sed 's/^[0-9][0-9]*//g' /opt/log4j2.properties：将文件中每一行以数字开头，都替换掉空字符并展示 sed '4,6s/^/#/g' /opt/log4j2.properties：将文件中 4 ~ 6 行添加 # 开头 sed '4,6s/^#//g' /opt/log4j2.properties：将文件中 4 ~ 6 行 # 开头去掉 实用例子 ifconfig eth0 |grep 'inet addr' |sed 's/^.*addr://g' |sed 's/Bcast.*$//g'：CentOS 6 只显示 IP ifconfig ens33 |grep 'inet' |sed 's/^.*inet//g' |sed 's/netmask.*$//g' |sed -n '1p'：CentOS 7.3 只显示 IP。先用 grep 筛选中包含 inet 的数据。 s 参数开头表示的是搜索替换，/^.*inet 表示从开头到 inet 之间，// 为空内容，/g，表示处理这一行所有匹配的内容。/netmask.*$ 表示从 netmask 到这一行结束的内容 资料 《构建高可用 Linux服务器》 http://www.cnblogs.com/edwardlost/archive/2010/09/17/1829145.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/File-Extract-Compress.html":{"url":"Linux-Tutorial/markdown-file/File-Extract-Compress.html","title":"Linux 下常用压缩文件的解压、压缩","keywords":"","body":"Linux 下常用压缩文件的解压、压缩 常用压缩包--解压--令整理 Linux 后缀为 .war 格式的文件（一般用在部署 Tomcat 项目的时候） 命令：unzip -oq XXXXXX.war -d ROOT 如果没有 ROOT 目录会自动创建 ROOT 目录。 Linux 后缀为 .tar.gz 格式的文件-解压 命令：tar zxvf XXXXXX.tar.gz Linux 后缀为 .bz2 格式的文件-解压 命令：bzip2 -d XXXXXX.bz2 Linux 后缀为 .tar.bz2 格式的文件-解压 命令：tar jxvf XXXXXX.tar.bz2 Linux 后缀为 .tar 格式的文件-解压 命令：tar zxvf XXXXXX.tar Linux 后缀为 .gz 格式的文件-解压 命令：gunzip XXXXXX.gz Linux 后缀为 .zip 格式的文件-解压 命令：unzip XXXXXX.zip 命令：unzip XXXXXX.zip -d /opt/，解压到指定目录 Linux 后缀为 .7z 格式的文件-解压 命令：7za x XXXXXX.7z Linux 后缀为 .tar.xz 格式的文件-解压，解压出来是tar，再对tar进行解压 命令：tar xf XXXXXX.tar.xz 常用文件进行--压缩--命令整理 Linux 压缩文件夹为后缀 .war 格式的文件（最好不要对根目录进行压缩，不然会多出一级目录） 命令：jar -cvfM0 cas.war /opt/cas/META-INF /opt/cas/WEB-INF /opt/cas/index.jsp 或者命令：cd 项目根目录 ; jar -cvfM0 cas.war ./* Linux 压缩文件为后缀 .tar 格式的文件 命令：tar -zcvf test11.tar test11 Linux 压缩文件为后缀 .tar.gz 格式的文件 命令：tar -zcvf test11.tar.gz test11 Linux 压缩文件为后缀 .bz2 格式的文件 命令：bzip2 -v test.txt Linux 压缩文件为后缀 .tar.bz2 格式的文件 命令：tar -jcvf test11.tar.gz test11 Linux 压缩文件为后缀 .zip 格式的文件 命令：zip -r test1.zip /opt/test1/ Linux 压缩文件为后缀 .7z 格式的文件 命令：7za a test1.7z /opt/test1/ 分卷压缩 分卷压缩：zip -s 100M myFile.zip --out newFile.zip 最终效果： newFile.z01 newFile.z02 newFile.z03 newFile.z04 newFile.zip 特殊格式 7z 7z 的安装： 访问官网下载解压包：http://sourceforge.net/projects/p7zip/files/p7zip/ 解压压缩包：tar jxvf p7zip_15.14_src_all.tar.bz2 进入目录：cd p7zip_15.14 执行安装：sh install.sh rar rar 的安装： 下载：wget http://www.rarlab.com/rar/rarlinux-3.8.0.tar.gz 解压下载下来的压缩包：tar zxvf rarlinux-3.8.0.tar.gz 进入解压后目录：cd rar 编译：make 安装：make install rar 解压：rar x 文件名.rar jar 包操作 修改 jar 包配置文件 命令：vim mytest.jar，这时候会展示 jar 中所有层级目录下的所有文件 输入：/log4j2.xml 回车，光标定位到该文件，然后再回车，进入编辑该文件状态 此时可以修改配置文件了，修改后 :wq 保存退出，接着 ：q 退出 jar 编辑状态 更新 Jar 包中的文件 替换（新增）jar 根目录下的文件：jar uvf mytest.jar ClassToAdd.class 一般 class 文件都是在多层目录里面的，需要这样做：jar uvf mytest.jar com/youmeek/ClassToAdd.class 需要在 jar 所在的文件夹下创建：mkdir -p ./com/youmeek，该目录必须和原 jar 里面的层级目录结构一致 资料 http://forum.ubuntu.org.cn/viewtopic.php?f=50&t=158893 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Off-line-Yum-Install.html":{"url":"Linux-Tutorial/markdown-file/Off-line-Yum-Install.html","title":"Yum 下载安装包及对应依赖包","keywords":"","body":"Yum 下载安装包及对应依赖包 安装 安装该软件：yum install -y yum-plugin-downloadonly 以下载 openssh-server 为例： yum install -y openssh-server --downloadonly --downloaddir=/opt/ssh 在 /opt/ssh 目录下有如下内容： -rw-r--r--. 1 root root 280524 Aug 13 2015 openssh-5.3p1-112.el6_7.x86_64.rpm -rw-r--r--. 1 root root 448872 Aug 13 2015 openssh-clients-5.3p1-112.el6_7.x86_64.rpm -rw-r--r--. 1 root root 331544 Aug 13 2015 openssh-server-5.3p1-112.el6_7.x86_64.rpm 安装下载的 rpm 文件：sudo rpm -ivh *.rpm 利用 yum 安装 rpm 文件，并自动满足依赖的 rpm 文件：sudo yum localinstall *.rpm 资料 http://www.jianshu.com/p/5930545b5591 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Zsh.html":{"url":"Linux-Tutorial/markdown-file/Zsh.html","title":"Zsh 入门","keywords":"","body":"Zsh 入门 本文前提 CentOS 6.7 64 bit root 用户 Zsh 介绍 Zsh 兼容 Bash，据传说 99% 的 Bash 操作 和 Zsh 是相同的 Zsh 官网：http://www.zsh.org/ 先看下你的 CentOS 支持哪些 shell：cat /etc/shells，正常结果应该是这样的： /bin/sh /bin/bash /sbin/nologin /bin/dash /bin/tcsh /bin/csh 默认 CentOS / Ubuntu / Mac 系统用的是 Bash，倒也不是说 Bash 不好，而是说我们有更好的选择。 Zsh 安装 CentOS 安装：sudo yum install -y zsh Ubuntu 安装：sudo apt-get install -y zsh 在检查下系统的 shell：cat /etc/shells，你会发现多了一个：/bin/zsh 使用 Zsh 扩展集合：oh-my-zsh oh-my-zsh 帮我们整理了一些常用的 Zsh 扩展功能和主题：https://github.com/robbyrussell/oh-my-zsh 我们无需自己去捣搞 Zsh，直接用 oh-my-zsh 就足够了，如果你想继续深造的话那再去弄。 先安装 git：sudo yum install -y git 安装 oh-my-zsh（这个过程可能会有点慢，或者需要重试几次）：wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh 整个过程效果如下图： 在以 root 用户为前提下，oh-my-zsh 的安装目录：/root/.oh-my-zsh 在以 root 用户为前提下，Zsh 的配置文件位置：/root/.zshrc 为 root 用户设置 zsh 为系统默认 shell：chsh -s /bin/zsh root 如果你要重新恢复到 bash：chsh -s /bin/bash root 现在你关掉终端或是重新连上 shell，现在开头是一个箭头了，如下图： Home / End 失灵问题 虽然可以通过配置解决，但是建议还是直接记快捷键吧： Home = Ctrl + a End = Ctrl + e Zsh 配置 插件 启用 oh-my-zsh 中自带的插件。 oh-my-zsh 的插件列表介绍（太长了，用源码不精准地统计下有 149 个）：https://github.com/robbyrussell/oh-my-zsh/wiki/Plugins 我们看下安装 oh-my-zsh 的时候自带有多少个插件：ls -l /root/.oh-my-zsh/plugins |grep \"^d\"|wc -l，我这边得到的结果是：211 编辑配置文件：vim /root/.zshrc，找到下图的地方，怎么安装，原作者注释写得很清楚了，别装太多了，默认 git 是安装的。 插件推荐： wd 简单地讲就是给指定目录映射一个全局的名字，以后方便直接跳转到这个目录，比如： 编辑配置文件，添加上 wd 的名字：vim /root/.zshrc 我常去目录：/opt/setups，每次进入该目录下都需要这样：cd /opt/setups 现在用 wd 给他映射一个快捷方式：cd /opt/setups ; wd add setups 以后我在任何目录下只要运行：wd setups 就自动跑到 /opt/setups 目录下了 插件官网：https://github.com/mfaerevaag/wd autojump 这个插件会记录你常去的那些目录，然后做一下权重记录，你可以用这个命令看到你的习惯：j --stat，如果这个里面有你的记录，那你就只要敲最后一个文件夹名字即可进入，比如我个人习惯的 program：j program，就可以直接到：/usr/program 插件官网：https://github.com/wting/autojump 官网插件下载地址：https://github.com/wting/autojump/downloads 插件下载：wget https://github.com/downloads/wting/autojump/autojump_v21.1.2.tar.gz 解压：tar zxvf autojump_v21.1.2.tar.gz 进入解压后目录并安装：cd autojump_v21.1.2/ ; ./install.sh 再执行下这个：source /etc/profile.d/autojump.sh 编辑配置文件，添加上 autojump 的名字：vim /root/.zshrc zsh-syntax-highlighting 这个插件会对终端命令高亮显示,比如正确的拼写会是绿色标识,否则是红色,另外对于一些shell输出语句也会有高亮显示,算是不错的辅助插件 插件官网：https://github.com/zsh-users/zsh-syntax-highlighting 安装，复制该命令：'git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting' 编辑：vim ~/.zshrc，找到这一行，后括号里面的后面添加：plugins=( 前面的一些插件名称 zsh-syntax-highlighting) 刷新下配置：source ~/.zshrc 主题 很多人喜欢捣搞这个 ╮(￣▽￣)╭ 捣搞主题和插件思路一样 oh-my-zsh 的主题列表介绍（还是太长了）：https://github.com/robbyrussell/oh-my-zsh/wiki/Themes 我们看下安装 oh-my-zsh 的时候，自带有多少个：ls -l /root/.oh-my-zsh/themes |grep \"^-\"|wc -l，我这边得到的结果是：140 我个人品味地推荐的是（排名有先后）： ys agnoster avit blinks 编辑配置文件：vim /root/.zshrc，找到下图的地方，怎么安装，原作者注释写得很清楚了，如果你没特别的喜欢那就选择随机吧。 配置好新主题需要重新连接 shell 才能看到效果 一些人性化功能 呃，这个其实可以不用讲的，你自己用的时候你自己会发现的，各种便捷，特别是用 Tab 多的人一定会有各种惊喜的。 使用 ctrl-r 来搜索命令历史记录。按完此快捷键后，可以输入关键命令词语，如果历史记录有含有此词语会显示出来。 命令别名： 在命令行中输入 alias 可以查看已经有的命令别名 自己新增一些别名，编辑文件：vim ~/.zshrc，在文件加入下面格式的命令，比如以下是网友提供的一些思路： alias cls='clear' alias ll='ls -l' alias la='ls -a' alias grep=\"grep --color=auto\" alias -s html='vim' # 在命令行直接输入后缀为 html 的文件名，会在 Vim 中打开 alias -s rb='vim' # 在命令行直接输入 ruby 文件，会在 Vim 中打开 alias -s py='vim' # 在命令行直接输入 python 文件，会用 vim 中打开，以下类似 alias -s js='vim' alias -s c='vim' alias -s java='vim' alias -s txt='vim' alias -s gz='tar -xzvf' # 在命令行直接输入后缀为 gz 的文件名，会自动解压打开 alias -s tgz='tar -xzvf' alias -s zip='unzip' alias -s bz2='tar -xjvf' 差异 我们现在增加系统变量在：/etc/profile 后，输入命令：source /etc/profile 之后，重启服务器发现刚刚的系统变量现在没效果。 解决办法：vim ~/.zshrc，在该配置文件里面增加一行：source /etc/profile，然后刷新 zsh 的配置：source ~/.zshrc。 资料 http://macshuo.com/?p=676 https://aaaaaashu.gitbooks.io/mac-dev-setup/content/iTerm/zsh.html http://hackerxu.com/2014/11/19/ZSH.html https://blog.phpgao.com/oh-my-zsh.html http://www.bkjia.com/Linuxjc/1033947.html http://swiftcafe.io/2015/10/31/cafe-time-omz/ http://swiftcafe.io/2015/12/04/omz-plugin/ http://www.hackbase.com/article-206940-1.html http://hahack.com/wiki/shell-zsh.html http://blog.jobbole.com/86820/ http://uecss.com/zsh-brew-autojump-plugins-shell-for-mac.html http://www.cnblogs.com/westfly/p/3283525.html http://wdxtub.com/2016/02/18/oh-my-zsh/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/speedtest.html":{"url":"Linux-Tutorial/markdown-file/speedtest.html","title":"终端测速","keywords":"","body":"终端测速 目前大家主推这个项目：https://github.com/sivel/speedtest-cli 用起来也比较简单，Python 2 ~ 3 都支持。 简单安装方式 命令：pip install speedtest-cli 官网还介绍了其他很多安装使用方式，大家可以自行看下。 运行 命令：speedtest-cli 结果如下： Retrieving speedtest.net configuration... Testing from China Telecom Guangdong (113.67.181.234)... Retrieving speedtest.net server list... Selecting best server based on ping... Hosted by CTM Internet Services (Macau) [106.48 km]: 64.783 ms Testing download speed................................................................................ Download: 1.05 Mbit/s Testing upload speed................................................................................................ Upload: 2.28 Mbit/s Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/maintenance.html":{"url":"Linux-Tutorial/markdown-file/maintenance.html","title":"日常维护","keywords":"","body":"常见日常维护 Vim 编辑文件报：Swap file \"Hello.java.swp\" already exists! 问题原因： Vim 编辑 Hello.java 文件的时候，非正常退出，然后又重新再 Vim 这个文件一般都会提示这个。 解决办法： 进入被编辑的文件目录，比如：Hello.java 我放在 /opt 目录下，那就先：cd /opt， 然后：ls -A，会发现有一个：.Hello.java.swp，把这个文件删除掉：rm -rf .Hello.java.swp，然后重新 Vim 文件即可。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/monitor.html":{"url":"Linux-Tutorial/markdown-file/monitor.html","title":"日常监控","keywords":"","body":"常见日常监控 系统信息 查看 CentOS 版本号：cat /etc/redhat-release 综合监控 nmon 系统负载 命令：w（判断整体瓶颈） 12:04:52 up 16 days, 12:54, 1 user, load average: 0.06, 0.13, 0.12 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 116.21.24.85 11:57 4.00s 16:18 0.01s w 第一行： 12:04:52 表示当前系统时间 up 16 days 表示系统运行时间 1 user 表示登录用户数 load average 表示平均负载，0.06 表示一分钟内系统的平均负载值，0.13 表示五分钟内系统的平均负载值，0.12 表示十五分钟内系统的平均负载值。一般这个字不要超过服务器的 CPU 线程数（process）就没有关系。 查看 CPU 总的线程数：grep 'processor' /proc/cpuinfo | sort -u | wc -l 第二行： 开始表示各个登录用户的情况，当前登录者是 root，登录者 IP 116.21.24.85 还有一个简化版本的命令：uptime 10:56:16 up 26 days, 20:05, 1 user, load average: 0.00, 0.01, 0.05 命令：vmstat（判断 RAM 和 I/0 瓶颈） 命令：vmstat 5 10，每 5 秒采样一次，共 10 次。 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 72648 0 674564 0 0 0 7 0 26 1 1 99 0 0 0 0 0 72648 0 674596 0 0 0 0 442 557 1 0 99 0 0 0 0 0 72648 0 674596 0 0 0 12 438 574 0 1 99 0 0 0 0 0 72648 0 674596 0 0 0 0 430 540 0 0 100 0 0 0 0 0 72648 0 674596 0 0 0 0 448 567 0 1 99 0 0 0 0 0 72648 0 674596 0 0 0 0 459 574 1 0 99 0 0 0 0 0 72648 0 674596 0 0 0 0 425 543 0 1 99 0 0 0 0 0 72276 0 674600 0 0 0 0 480 643 2 3 95 0 0 第二行： r 表示运行和等待CPU时间片的进程数，该数字如果长期大于服务器CPU的进程数，则说明CPU不够用了。 b 表示等待资源的进程数，比如等I/O，内存等。该数字如果长时间大于 1，则需要关注一下。 si 表示由交换区写入到内存的数据量 so 表示由内存写入到交换区的数据量 如果 si 和 so 的数字比较高，并且不断变化时，说明内存不够了。而且不断变化也表示对系统性能影响很大。 bi 表示从块设备读取数据的量（读磁盘） bo 表示从块设备写入数据的量（写磁盘） 如果bi和bo两个数字比较高，则说明，磁盘IO压力大。 wa 表示I/O等待所占用CPU的时间比 命令：sar（综合） sar(system activity reporter 系统活动情况报告) sar 是目前 linux 上最为全面的系统性能分析工具之一，可以从多方面对系统的活动情况进行报告。包括（文件的读写、系统调用、磁盘I/O、cpu效率、内存使用、进程活动以及IPC有关的活动） 如果没安装，运行：yum install -y sysstat sar 之 CPU 使用情况（判断 CPU 瓶颈） 命令：sar -u 5 10，每 5 秒采样一次，共 10 次 01:57:29 PM CPU %user %nice %system %iowait %steal %idle 01:57:34 PM all 1.81 0.00 0.40 0.00 0.00 97.78 01:57:39 PM all 0.20 0.00 0.40 0.00 0.00 99.39 01:57:44 PM all 0.40 0.00 0.60 0.00 0.00 98.99 01:57:49 PM all 0.20 0.00 0.40 0.00 0.00 99.39 01:57:54 PM all 0.80 0.00 1.41 0.00 0.00 97.79 01:57:59 PM all 0.40 0.00 0.60 0.00 0.00 98.99 01:58:04 PM all 0.20 0.00 0.40 0.00 0.00 99.39 01:58:09 PM all 0.20 0.00 0.40 0.00 0.00 99.39 01:58:14 PM all 0.40 0.00 0.61 0.00 0.00 98.99 01:58:19 PM all 0.20 0.00 0.61 0.00 0.00 99.19 Average: all 0.48 0.00 0.59 0.00 0.00 98.93 列说明： CPU：all 表示统计信息为所有 CPU的平均值。 %user：显示在用户级别(application)运行使用 CPU 总时间的百分比。 %nice：显示在用户级别，用于nice操作，所占用 CPU总时间的百分比。 %system：在核心级别(kernel)运行所使用 CPU总时间的百分比。 %iowait：显示用于等待I/O操作占用 CPU总时间的百分比。 %steal：管理程序(hypervisor)为另一个虚拟进程提供服务而等待虚拟 CPU 的百分比。 %idle：显示 CPU空闲时间占用 CPU总时间的百分比。 总结： 1.若 %iowait 的值过高，表示硬盘存在I/O瓶颈 2.若 %idle 的值高但系统响应慢时，有可能是 CPU 等待分配内存，此时应加大内存容量，可以使用内存监控命令分析内存。 3.若 %idle 的值持续低于1，则系统的 CPU 处理能力相对较低，表明系统中最需要解决的资源是 CPU。 sar 之 RAM 使用情况（判断内存瓶颈） 命令：sar -B 5 10，每 5 秒采样一次，共 10 次 02:32:15 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 02:32:20 PM 0.00 0.81 258.47 0.00 27.22 0.00 0.00 0.00 0.00 02:32:25 PM 0.00 0.00 611.54 0.00 300.20 0.00 0.00 0.00 0.00 02:32:30 PM 0.00 26.61 10.08 0.00 11.90 0.00 0.00 0.00 0.00 02:32:35 PM 0.00 1.62 3.64 0.00 3.84 0.00 0.00 0.00 0.00 02:32:40 PM 0.00 0.00 3.42 0.00 4.43 0.00 0.00 0.00 0.00 02:32:45 PM 0.00 0.00 3.43 0.00 3.83 0.00 0.00 0.00 0.00 02:32:50 PM 0.00 1.62 3.84 0.00 5.86 0.00 0.00 0.00 0.00 02:32:55 PM 0.00 0.00 3.41 0.00 3.82 0.00 0.00 0.00 0.00 02:33:00 PM 0.00 2.42 763.84 0.00 208.69 0.00 0.00 0.00 0.00 02:33:05 PM 0.00 13.74 2409.70 0.00 929.70 0.00 0.00 0.00 0.00 Average: 0.00 4.68 406.50 0.00 149.69 0.00 0.00 0.00 0.00 pgpgin/s：表示每秒从磁盘或SWAP置换到内存的字节数(KB) pgpgout/s：表示每秒从内存置换到磁盘或SWAP的字节数(KB) fault/s：每秒钟系统产生的缺页数,即主缺页与次缺页之和(major + minor) majflt/s：每秒钟产生的主缺页数 pgfree/s：每秒被放入空闲队列中的页个数 pgscank/s：每秒被kswapd扫描的页个数 pgscand/s：每秒直接被扫描的页个数 pgsteal/s：每秒钟从cache中被清除来满足内存需要的页个数 %vmeff：每秒清除的页(pgsteal)占总扫描页(pgscank+pgscand)的百分比 sar 之 I/O 使用情况（判断 I/O 瓶颈） 命令：sar -b 5 10，每 5 秒采样一次，共 10 次 02:34:13 PM tps rtps wtps bread/s bwrtn/s 02:34:18 PM 3.03 0.00 3.03 0.00 59.80 02:34:23 PM 0.00 0.00 0.00 0.00 0.00 02:34:28 PM 0.00 0.00 0.00 0.00 0.00 02:34:33 PM 0.00 0.00 0.00 0.00 0.00 02:34:38 PM 1.61 0.00 1.61 0.00 24.80 02:34:43 PM 0.00 0.00 0.00 0.00 0.00 02:34:48 PM 0.40 0.00 0.40 0.00 4.86 02:34:53 PM 0.00 0.00 0.00 0.00 0.00 02:34:58 PM 0.00 0.00 0.00 0.00 0.00 02:35:03 PM 0.00 0.00 0.00 0.00 0.00 Average: 0.50 0.00 0.50 0.00 8.94 tps：每秒钟物理设备的 I/O 传输总量 rtps：每秒钟从物理设备读入的数据总量 wtps：每秒钟向物理设备写入的数据总量 bread/s：每秒钟从物理设备读入的数据量，单位为块/s bwrtn/s：每秒钟向物理设备写入的数据量，单位为块/s sar 之 DEV（网卡）流量查看（判断网络瓶颈） 命令：sar -n DEV，查看网卡历史流量（因为是按时间显示每棵的流量，所以有很多） 如果要动态显示当前的网卡流量：sar -n DEV 1 采样收集网卡流量：sar -n DEV 5 10，每 5 秒采样一次，共 10 次 如果要查看其他日期下的记录，可以到这个目录下：cd /var/log/sa 查看下记录的文件，然后选择一个文件，比如：sar -n DEV -f /var/log/sa/sa01） 01:46:24 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 01:46:25 PM lo 3.00 3.00 0.18 0.18 0.00 0.00 0.00 01:46:25 PM eth0 4.00 4.00 0.55 0.56 0.00 0.00 0.00 01:46:25 PM 表示时间 IFACE 表示网卡名称 rxpck/s 每秒钟接收到的 包数目，一般如果这个数字大于 4000 一般是被攻击了。 txpck/s 每秒钟发送出去的 包数目 rxkB/s 每秒钟接收到的数据量(单位kb)，一般如果这个数字大于 5000 一般是被攻击了。 txkB/s 每秒钟发送出去的数据量(单位kb) rxcmp/s：每秒钟接收到的压缩包数目 txcmp/s：每秒钟发送出去的压缩包数目 txmcst/s：每秒钟接收到的多播包的包数目 查看 TCP 相关的一些数据（每隔 1 秒采样一次，一共 5 次）：sar -n TCP,ETCP 1 5 Linux 3.10.0-693.2.2.el7.x86_64 (youmeek) 07/17/2018 _x86_64_ (2 CPU) 12:05:47 PM active/s passive/s iseg/s oseg/s 12:05:48 PM 0.00 0.00 1.00 0.00 12:05:47 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:05:48 PM 0.00 0.00 0.00 0.00 0.00 12:05:48 PM active/s passive/s iseg/s oseg/s 12:05:49 PM 0.00 0.00 1.00 1.00 12:05:48 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:05:49 PM 0.00 0.00 0.00 0.00 0.00 12:05:49 PM active/s passive/s iseg/s oseg/s 12:05:50 PM 0.00 0.00 1.00 1.00 12:05:49 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:05:50 PM 0.00 0.00 0.00 0.00 0.00 12:05:50 PM active/s passive/s iseg/s oseg/s 12:05:51 PM 0.00 0.00 3.00 3.00 12:05:50 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:05:51 PM 0.00 0.00 0.00 0.00 0.00 12:05:51 PM active/s passive/s iseg/s oseg/s 12:05:52 PM 0.00 0.00 1.00 1.00 12:05:51 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 12:05:52 PM 0.00 0.00 0.00 0.00 0.00 Average: active/s passive/s iseg/s oseg/s Average: 0.00 0.00 1.40 1.20 Average: atmptf/s estres/s retrans/s isegerr/s orsts/s Average: 0.00 0.00 0.00 0.00 0.00 - active/s：每秒钟本地主动开启的 tcp 连接，也就是本地程序使用 connect() 系统调用 - passive/s：每秒钟从源端发起的 tcp 连接，也就是本地程序使用 accept() 所接受的连接 - retrans/s: 每秒钟的 tcp 重传次数 atctive 和 passive 的数目通常可以用来衡量服务器的负载：接受连接的个数（passive），下游连接的个数（active）。可以简单认为 active 为出主机的连接，passive 为入主机的连接；但这个不是很严格的说法，比如 loalhost 和 localhost 之间的连接。 来自：https://zhuanlan.zhihu.com/p/39893236 CPU 监控 CPU 的基本信息查看 Demo CPU 型号：Intel® Xeon® Processor E5-2620 v2(15M Cache, 2.10 GHz) 该 CPU 显示的数据中有一项这个要注意：Intel® Hyper-Threading Technology 是 Yes。表示该 CPU 支持超线程 cat /proc/cpuinfo，查看 CPU 总体信息 grep 'physical id' /proc/cpuinfo | sort -u | wc -l，查看物理 CPU 个数 结果：2 物理 CPU：物理 CPU 也就是机器外面就能看到的一个个 CPU，每个物理 CPU 还带有单独的风扇 grep 'core id' /proc/cpuinfo | sort -u | wc -l，查看每个物理 CPU 的核心数量 结果：6，因为每个物理 CPU 是 6，所有 2 个物理 CPU 的总核心数量应该是：12 核心数：一个核心就是一个物理线程，英特尔有个超线程技术可以把一个物理线程模拟出两个线程来用，充分发挥 CPU 性能，意思是一个核心可以有多个线程。 grep 'processor' /proc/cpuinfo | sort -u | wc -l，查看 CPU 总的线程数，一般也叫做：逻辑 CPU 数量 结果：24，正常情况下：CPU 的总核心数量 == CPU 线程数，但是如果该 CPU 支持超线程，那结果是：CPU 的总核心数量 X 2 == CPU 线程数 线程数：线程数是一种逻辑的概念，简单地说，就是模拟出的 CPU 核心数。比如，可以通过一个 CPU 核心数模拟出 2 线程的 CPU，也就是说，这个单核心的 CPU 被模拟成了一个类似双核心 CPU 的功能。 CPU 监控 Linux 的 CPU 简单监控一般简单 常用命令就是 top 命令：top -bn1，可以完全显示所有进程出来，但是不能实时展示数据，只能暂时命令当时的数据。 top 可以动态显示进程所占的系统资源，每隔 3 秒变一次，占用系统资源最高的进程放最前面。 在 top 命令状态下还可以按数字键 1 显示各个 CPU 线程使用状态 在 top 命令状态下按 shfit + m 可以按照 内存使用 大小排序 在 top 命令状态下按 shfit + p 可以按照 CPU 使用 大小排序 展示数据上，%CPU 表示进程占用的 CPU 百分比，%MEM 表示进程占用的内存百分比 CPU 其他工具 htop 综合工具：yum install -y htop 这几篇文章讲得很好，我没必要再贴过来了，大家自己看： htop 命令完胜 top 命令 htop 命令详解 mpstat 实时监控 CPU 状态：yum install -y sysstat 可以具体到某个核心，比如我有 2 核的 CPU，因为 CPU 核心下标是从 0 开始，所以我要查看 0 的状况（间隔 3 秒获取一次指标，一共获取 5 次）：mpstat -P 0 3 5 打印总 CPU 和各个核心指标：mpstat -P ALL 1 获取所有核心的平均值：mpstat 3 5 Linux 3.10.0-693.2.2.el7.x86_64 (iZwz998aag1ggy168n3wg2Z) 06/23/2018 _x86_64_ (2 CPU) 11:44:52 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 11:44:53 AM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 11:44:54 AM 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.00 11:44:55 AM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 11:44:56 AM 0 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 99.00 11:44:57 AM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 Average: 0 0.20 0.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 99.60 %usr 用户进程消耗 CPU 情况 %sys 系统进程消耗 CPU 情况 %iowait 表示 CPU 等待 IO 时间占整个 CPU 周期的百分比 %idle 显示 CPU 空闲时间占用 CPU 总时间的百分比 类似 top 的 pidstat 安装：yum install -y sysstat 每隔 2 秒采样一次，一共 5 次：pidstat 2 5 Linux 3.10.0-693.el7.x86_64 (youmeek) 07/17/2018 _x86_64_ (8 CPU) 11:52:58 AM UID PID %usr %system %guest %CPU CPU Command 11:53:00 AM 0 16813 0.50 0.99 0.00 1.49 1 pidstat 11:53:00 AM 0 24757 50.99 12.87 0.00 63.86 0 java 11:53:00 AM 0 24799 60.40 3.47 0.00 63.86 5 java 11:53:00 AM 0 24841 99.50 7.43 0.00 100.00 0 java 11:53:00 AM UID PID %usr %system %guest %CPU CPU Command 11:53:02 AM 0 24757 56.50 0.50 0.00 57.00 0 java 11:53:02 AM 0 24799 100.00 6.50 0.00 100.00 5 java 11:53:02 AM 0 24841 58.00 2.50 0.00 60.50 0 java 11:53:02 AM UID PID %usr %system %guest %CPU CPU Command 11:53:04 AM 0 16813 0.00 1.00 0.00 1.00 2 pidstat 11:53:04 AM 0 24757 62.00 5.50 0.00 67.50 0 java 11:53:04 AM 0 24799 54.00 14.00 0.00 68.00 5 java 11:53:04 AM 0 24841 39.50 9.00 0.00 48.50 0 java 11:53:04 AM UID PID %usr %system %guest %CPU CPU Command 11:53:06 AM 0 16813 0.50 0.50 0.00 1.00 2 pidstat 11:53:06 AM 0 24757 80.00 13.50 0.00 93.50 0 java 11:53:06 AM 0 24799 56.50 0.50 0.00 57.00 5 java 11:53:06 AM 0 24841 1.00 0.50 0.00 1.50 0 java 11:53:06 AM UID PID %usr %system %guest %CPU CPU Command 11:53:08 AM 0 16813 0.00 0.50 0.00 0.50 2 pidstat 11:53:08 AM 0 24757 58.50 1.00 0.00 59.50 0 java 11:53:08 AM 0 24799 60.00 1.50 0.00 61.50 5 java 11:53:08 AM 0 24841 1.00 0.50 0.00 1.50 0 java Average: UID PID %usr %system %guest %CPU CPU Command Average: 0 16813 0.20 0.60 0.00 0.80 - pidstat Average: 0 24757 61.58 6.69 0.00 68.26 - java Average: 0 24799 66.47 5.19 0.00 71.66 - java Average: 0 24841 39.92 3.99 0.00 43.91 - java 内存监控 Linux 的内存本质是虚拟内存，这样说是因为它的内存是：物理内存 + 交换分区。有一个内存模块来管理应用的内存使用。 如果所以你内存大，可以考虑把 swap 分区改得小点或者直接关掉。 但是，如果是用的云主机，一般是没交换分区的，free -g 中的 Swap 都是 0。 查看内存使用命令： 以 M 为容量单位展示数据：free -m 以 G 为容量单位展示数据：free -g CentOS 6 和 CentOS 7 展示出来的数据有差别，CentOS 7 比较容易看，比如下面的数据格式是 CentOS 7 的 free -g： total used free shared buff/cache available Mem: 11 0 10 0 0 10 Swap: 5 0 5 在以上结果中，其中可以用的内存是看 available 列。 对于 CentOS 6 的系统可以使用下面命令： [root@bogon ~]# free -mlt total used free shared buffers cached Mem: 16080 15919 160 0 278 11934 Low: 16080 15919 160 High: 0 0 0 -/+ buffers/cache: 3706 12373 Swap: 0 0 0 Total: 16080 15919 160 以上的结果重点关注是：-/+ buffers/cache，这一行代表实际使用情况。 pidstat 采样内存使用情况 安装：yum install -y sysstat 每隔 2 秒采样一次，一共 3 次：pidstat -r 2 3 Linux 3.10.0-693.el7.x86_64 (youmeek) 07/17/2018 _x86_64_ (8 CPU) 11:56:34 AM UID PID minflt/s majflt/s VSZ RSS %MEM Command 11:56:36 AM 0 23960 168.81 0.00 108312 1124 0.01 pidstat 11:56:36 AM 0 24757 8.42 0.00 9360696 3862788 23.75 java 11:56:36 AM 0 24799 8.91 0.00 10424088 4988468 30.67 java 11:56:36 AM 0 24841 11.39 0.00 10423576 4968428 30.54 java 11:56:36 AM UID PID minflt/s majflt/s VSZ RSS %MEM Command 11:56:38 AM 0 23960 169.50 0.00 108312 1200 0.01 pidstat 11:56:38 AM 0 24757 6.00 0.00 9360696 3862788 23.75 java 11:56:38 AM 0 24799 5.50 0.00 10424088 4988468 30.67 java 11:56:38 AM 0 24841 7.00 0.00 10423576 4968428 30.54 java 11:56:38 AM UID PID minflt/s majflt/s VSZ RSS %MEM Command 11:56:40 AM 0 23960 160.00 0.00 108312 1200 0.01 pidstat 11:56:40 AM 0 24757 6.50 0.00 9360696 3862788 23.75 java 11:56:40 AM 0 24799 6.00 0.00 10424088 4988468 30.67 java 11:56:40 AM 0 24841 8.00 0.00 10423576 4968428 30.54 java Average: UID PID minflt/s majflt/s VSZ RSS %MEM Command Average: 0 23960 166.11 0.00 108312 1175 0.01 pidstat Average: 0 24757 6.98 0.00 9360696 3862788 23.75 java Average: 0 24799 6.81 0.00 10424088 4988468 30.67 java Average: 0 24841 8.80 0.00 10423576 4968428 30.54 java 硬盘监控 硬盘容量相关查看 df -h：自动以合适的磁盘容量单位查看磁盘大小和使用空间 df -m：以磁盘容量单位 M 为数值结果查看磁盘使用情况 du -sh /opt/tomcat6：查看tomcat6这个文件夹大小 (h的意思human-readable用人类可读性较好方式显示，系统会自动调节单位，显示合适大小的单位) du /opt --max-depth=1 -h：查看指定录入下包括子目录的各个文件大小情况 命令：iostat（判断 I/0 瓶颈） 命令：iostat -x -k 3 3，每 3 秒采样一次，共 3 次。 avg-cpu: %user %nice %system %iowait %steal %idle 0.55 0.00 0.52 0.00 0.00 98.93 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 0.04 0.02 0.62 0.44 6.49 21.65 0.00 1.42 1.17 1.42 0.25 0.02 avg-cpu: %user %nice %system %iowait %steal %idle 0.34 0.00 0.00 0.00 0.00 99.66 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 avg-cpu: %user %nice %system %iowait %steal %idle 2.02 0.00 0.34 0.00 0.00 97.64 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util vda 0.00 0.00 0.00 1.68 0.00 16.16 19.20 0.00 0.20 0.00 0.20 0.20 0.03 列说明： rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm/s: 每秒对该设备的写请求被合并次数 r/s: 每秒完成的读次数 w/s: 每秒完成的写次数 rkB/s: 每秒读数据量(kB为单位) wkB/s: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度（队列长度大于 1 表示设备处于饱和状态。） await: 系统发往 IO 设备的请求的平均响应时间(毫秒为单位)。这包括请求排队的时间，以及请求处理的时间。超过经验值的平均响应时间表明设备处于饱和状态，或者设备有问题。 svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率（就是繁忙程度，值越高表示越繁忙） 总结 iowait% 表示CPU等待IO时间占整个CPU周期的百分比，如果iowait值超过50%，或者明显大于%system、%user以及%idle，表示IO可能存在问题。 %util （重点参数）表示磁盘忙碌情况，一般该值超过80%表示该磁盘可能处于繁忙状态 硬盘 IO 监控 安装 iotop：yum install -y iotop 查看所有进程 I/O 情况命令：iotop 只查看当前正在处理 I/O 的进程：iotop -o 只查看当前正在处理 I/O 的线程，每隔 5 秒刷新一次：iotop -o -d 5 只查看当前正在处理 I/O 的进程（-P 参数决定），每隔 5 秒刷新一次：iotop -o -P -d 5 只查看当前正在处理 I/O 的进程（-P 参数决定），每隔 5 秒刷新一次，使用 KB/s 单位（默认是 B/s）：iotop -o -P -k -d 5 使用 dd 命令测量服务器延迟：dd if=/dev/zero of=/opt/ioTest2.txt bs=512 count=1000 oflag=dsync 使用 dd 命令来测量服务器的吞吐率（写速度)：dd if=/dev/zero of=/opt/ioTest1.txt bs=1G count=1 oflag=dsync 该命令创建了一个 10M 大小的文件 ioTest1.txt，其中参数解释： if 代表输入文件。如果不指定 if，默认就会从 stdin 中读取输入。 of 代表输出文件。如果不指定 of，默认就会将 stdout 作为默认输出。 bs 代表字节为单位的块大小。 count 代表被复制的块数。 /dev/zero 是一个字符设备，会不断返回0值字节（\\0）。 oflag=dsync：使用同步I/O。不要省略这个选项。这个选项能够帮助你去除 caching 的影响，以便呈现给你精准的结果。 conv=fdatasyn: 这个选项和 oflag=dsync 含义一样。 该命令执行完成后展示的数据： [root@youmeek ~]# dd if=/dev/zero of=/opt/ioTest1.txt bs=1G count=1 oflag=dsync 记录了1+0 的读入 记录了1+0 的写出 1073741824字节(1.1 GB)已复制，5.43328 秒，198 MB/秒 利用 hdparm 测试硬盘速度：yum install -y hdparm 查看硬盘分区情况：df -h，然后根据分区测试： 测试硬盘分区的读取速度：hdparm -T /dev/sda 测试硬盘分区缓存的读取速度：hdparm -t /dev/sda 也可以以上两个参数一起测试：hdparm -Tt /dev/sda，结果数据如下： /dev/sda: Timing cached reads: 3462 MB in 2.00 seconds = 1731.24 MB/sec Timing buffered disk reads: 806 MB in 3.00 seconds = 268.52 MB/sec pidstat 采样硬盘使用情况 安装：yum install -y sysstat 每隔 2 秒采样一次，一共 3 次：pidstat -d 2 3 Linux 3.10.0-693.el7.x86_64 (youmeek) 07/17/2018 _x86_64_ (8 CPU) 11:57:29 AM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 11:57:31 AM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 11:57:33 AM 0 24757 0.00 2.00 0.00 java 11:57:33 AM 0 24799 0.00 14.00 0.00 java 11:57:33 AM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 11:57:35 AM 0 24841 0.00 8.00 0.00 java Average: UID PID kB_rd/s kB_wr/s kB_ccwr/s Command Average: 0 24757 0.00 0.66 0.00 java Average: 0 24799 0.00 4.65 0.00 java Average: 0 24841 0.00 2.66 0.00 java 输出指标含义： kB_rd/s: 每秒进程从磁盘读取的数据量(以 kB 为单位) kB_wr/s: 每秒进程向磁盘写的数据量(以 kB 为单位) kB_ccwr/s：任务取消的写入磁盘的 KB。当任务截断脏的 pagecache 的时候会发生。 网络监控 网络监控常用 安装 iftop（需要有 EPEL 源）：yum install -y iftop 如果没有 EPEL 源：yum install -y epel-release 常用命令： iftop：默认是监控第一块网卡的流量 iftop -i eth0：监控 eth0 iftop -n：直接显示IP, 不进行DNS反解析 iftop -N：直接显示连接埠编号, 不显示服务名称 iftop -F 192.168.1.0/24 or 192.168.1.0/255.255.255.0：显示某个网段进出封包流量 iftop -nP：显示端口与 IP 信息 中间部分：外部连接列表，即记录了哪些ip正在和本机的网络连接 右边部分：实时参数分别是该访问 ip 连接到本机 2 秒，10 秒和 40 秒的平均流量 => 代表发送数据， 端口使用情况（也可以用来查看端口占用） lsof 安装 lsof：yum install -y lsof 查看 3316 端口是否有被使用：lsof -i:3316，有被使用会输出类似如下信息，如果没被使用会没有任何信息返回 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 12011 root 77u IPv6 4506842 0t0 TCP JDu4e00u53f7:58560->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 78u IPv6 4506843 0t0 TCP JDu4e00u53f7:58576->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 79u IPv6 4506844 0t0 TCP JDu4e00u53f7:58578->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 80u IPv6 4506845 0t0 TCP JDu4e00u53f7:58574->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 82u IPv6 4506846 0t0 TCP JDu4e00u53f7:58562->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 83u IPv6 4506847 0t0 TCP JDu4e00u53f7:58564->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 84u IPv6 4506848 0t0 TCP JDu4e00u53f7:58566->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 85u IPv6 4506849 0t0 TCP JDu4e00u53f7:58568->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 86u IPv6 4506850 0t0 TCP JDu4e00u53f7:58570->116.196.110.69:aicc-cmi (ESTABLISHED) java 12011 root 87u IPv6 4506851 0t0 TCP JDu4e00u53f7:58572->116.196.110.69:aicc-cmi (ESTABLISHED) docker-pr 13551 root 4u IPv6 2116824 0t0 TCP *:aicc-cmi (LISTEN) netstat 更多用法可以看：netstat 的10个基本用法 查看所有在用的端口：netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1/systemd tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 746/sshd tcp 0 0 127.0.0.1:32000 0.0.0.0:* LISTEN 12011/java tcp6 0 0 :::9066 :::* LISTEN 12011/java tcp6 0 0 :::6379 :::* LISTEN 28668/docker-proxy tcp6 0 0 :::111 :::* LISTEN 1/systemd tcp6 0 0 :::3316 :::* LISTEN 13551/docker-proxy tcp6 0 0 :::22 :::* LISTEN 746/sshd tcp6 0 0 :::35224 :::* LISTEN 12011/java tcp6 0 0 :::3326 :::* LISTEN 14203/docker-proxy tcp6 0 0 :::1984 :::* LISTEN 12011/java tcp6 0 0 :::8066 :::* LISTEN 12011/java tcp6 0 0 :::43107 :::* LISTEN 12011/java 查看当前连接80端口的机子有多少，并且是属于什么状态：netstat -an|grep 80|sort -r 查看已经连接的IP有多少连接数：netstat -ntu | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -n 查看已经连接的IP有多少连接数，只显示前 5 个：netstat -ntu | awk '{print $5}' | cut -d: -f1 | sort | uniq -c | sort -n | head -5 查看每个 ip 跟服务器建立的连接数：netstat -nat|awk '{print$5}'|awk -F : '{print$1}'|sort|uniq -c|sort -rn 262 127.0.0.1 118 103 172.22.100.141 12 172.22.100.29 7 172.22.100.183 6 116.21.17.144 6 0.0.0.0 5 192.168.1.109 4 172.22.100.32 4 172.22.100.121 4 172.22.100.108 4 172.18.1.39 3 172.22.100.2 3 172.22.100.190 统计当前连接的一些状态情况：netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 或者 netstat -nat |awk '{print $6}'|sort|uniq -c|sort -rn TIME_WAIT 96（是表示系统在等待客户端响应，以便再次连接时候能快速响应，如果积压很多，要开始注意了，准备阻塞了。这篇文章可以看下：http://blog.51cto.com/jschu/1728001） CLOSE_WAIT 11（如果积压很多，要开始注意了，准备阻塞了。可以看这篇文章：http://blog.51cto.com/net881004/2164020） FIN_WAIT2 17 ESTABLISHED 102（表示正常数据传输状态） TIME_WAIT 和 CLOSE_WAIT 说明： Linux 系统下，TCP连接断开后，会以TIME_WAIT状态保留一定的时间，然后才会释放端口。当并发请求过多的时候，就会产生大量的TIME_WAIT状态 的连接，无法及时断开的话，会占用大量的端口资源和服务器资源。这个时候我们可以优化TCP的内核参数，来及时将TIME_WAIT状态的端口清理掉。 来源：http://zhangbin.junxilinux.com/?p=219 ================================= 出现大量close_wait的现象，主要原因是某种情况下对方关闭了socket链接，但是另一端由于正在读写，没有关闭连接。代码需要判断socket，一旦读到0，断开连接，read返回负，检查一下errno，如果不是AGAIN，就断开连接。 Linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常，导致tomcat崩溃。关于TIME_WAIT过多的解决方案参见TIME_WAIT数量太多。 常见错误原因： 1.代码层面上未对连接进行关闭，比如关闭代码未写在 finally 块关闭，如果程序中发生异常就会跳过关闭代码，自然未发出指令关闭，连接一直由程序托管，内核也无权处理，自然不会发出 FIN 请求，导致连接一直在 CLOSE_WAIT 。 2.程序响应过慢，比如双方进行通讯，当客户端请求服务端迟迟得不到响应，就断开连接，重新发起请求，导致服务端一直忙于业务处理，没空去关闭连接。这种情况也会导致这个问题。一般如果有多个节点，nginx 进行负载，其中某个节点很高，其他节点不高，那可能就是负载算法不正常，都落在一台机子上了，以至于它忙不过来。 来源：https://juejin.im/post/5b59e61ae51d4519634fe257 查看网络接口接受、发送的数据包情况（每隔 3 秒统计一次）：netstat -i 3 Kernel Interface table Iface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flg eth0 1500 10903298 0 0 0 10847741 0 0 0 BMRU lo 65536 453650 0 0 0 453650 0 0 0 LRU eth0 1500 10903335 0 0 0 10847777 0 0 0 BMRU lo 65536 453650 0 0 0 453650 0 0 0 LRU eth0 1500 10903363 0 0 0 10847798 0 0 0 BMRU lo 65536 453650 0 0 0 453650 0 0 0 LRU eth0 1500 10903393 0 0 0 10847836 0 0 0 BMRU lo 65536 453650 0 0 0 453650 0 0 0 LRU eth0 1500 10903437 0 0 0 10847867 0 0 0 BMRU lo 65536 453650 0 0 0 453650 0 0 0 LRU 接收（该值是历史累加数据，不是瞬间数据，要计算时间内的差值需要自己减）： RX-OK 已接收字节数 RX-ERR 已接收错误字节数（数据值大说明网络存在问题） RX-DRP 已丢失字节数（数据值大说明网络存在问题） RX-OVR 由于误差而遗失字节数（数据值大说明网络存在问题） 发送（该值是历史累加数据，不是瞬间数据，要计算时间内的差值需要自己减）： TX-OK 已发送字节数 TX-ERR 已发送错误字节数（数据值大说明网络存在问题） TX-DRP 已丢失字节数（数据值大说明网络存在问题） TX-OVR 由于误差而遗失字节数（数据值大说明网络存在问题） 网络排查 ping 命令查看丢包、域名解析地址 ping 116.196.110.69 ping www.GitNavi.com telnet 测试端口的连通性（验证服务的可用性） yum install -y telnet telnet 116.196.110.68 3306 telnet www.youmeek.com 80 tracert（跟踪路由）查看网络请求节点访问情况，用于确定 IP 数据报访问目标所采取的路径。 yum install -y traceroute traceroute gitnavi.com nslookup 命令查看 DNS 是否可用 yum install -y bind-utils 输入：nslookup，然后终端进入交互模式，然后输入：www.baidu.com，此时会展示类似这样的信息： Server: 103.224.222.221（这个是你本机的信息） Address: 103.224.222.221#53（这个是你本机的信息） （下面是百度的信息） Non-authoritative answer: www.baidu.COM canonical name = www.a.shifen.COM. Name: www.a.shifen.COM Address: 220.181.112.244 Name: www.a.shifen.COM Address: 220.181.111.188 此时我们假设换个 DNS，我们在刚刚的交互阶段继续输入：server 8.8.8.8，表示我们此时用 8.8.8.8 的 DNS，然后我们在交互中再输入：www.baidu.com，此时会出现这个信息： Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: www.baidu.com canonical name = www.a.shifen.com. Name: www.a.shifen.com Address: 180.97.33.108 Name: www.a.shifen.com Address: 180.97.33.107 以上表明，不同的 DNS 情况下，我们获取到的域名所属 IP 是不同的。 查看 Linux 内核版本 对于一些复杂的层面问题，一般都要先确认内核版本，好帮助分析：uname -r 3.10.0-693.2.2.el7.x86_64 dmesg 打印内核信息 开机信息存在：tail -500f /var/log/dmesg 查看尾部信息：dmesg -T | tail 参数 -T 表示显示时间 只显示 error 和 warning 信息：dmesg --level=err,warn -T 有些 OOM 的错误会在这里显示，比如： [1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child [1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB 查看系统日志 查看系统日志：tail -400f /var/log/messages 可能会看到类似以下异常： Out of memory: Kill process 19452 (java) score 264 or sacrifice child 服务器故障排查顺序 请求时好时坏 系统层面 查看负载、CPU、内存、上线时间、高资源进程 PID：htop 查看网络丢失情况：netstat -i 3，关注：RX-DRP、TX-DRP，如果两个任何一个有值，或者都有值，肯定是网络出了问题（该值是历史累加数据，不是瞬间数据）。 应用层面 临时修改 nginx log 输出格式，输出完整信息，包括请求头 $request_body 请求体（含POST数据） $http_XXX 指定某个请求头（XXX为字段名，全小写） $cookie_XXX 指定某个cookie值（XXX为字段名，全小写） 类似用法： log_format special_main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$request_body\" \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for \"appid=$http_appid,appver=$http_appver,vuser=$http_vuser\" ' '\"phpsessid=$cookie_phpsessid,vuser_cookie=$cookie___vuser\" '; access_log /home/wwwlogs/hicrew.log special_main; CPU 高，负载高，访问慢（没有数据库） 记录负载开始升高的时间 常见场景 虚拟机所在的宿主机资源瓶颈，多个虚拟机竞争资源 定时任务大量的任务并发 消息、请求堆积后恢复时的瞬时流量引起 持久化任务引起 更多可以看这篇：线上异常排查总结 系统层面 查看负载、CPU、内存、上线时间、高资源进程 PID：htop 查看磁盘使用情况：df -h 查看磁盘当前情况：iostat -x -k 3 3。如果发现当前磁盘忙碌，则查看是哪个 PID 在忙碌：iotop -o -P -k -d 5 查看 PID 具体在写什么东西：lsof -p PID 查看系统日志：tail -400f /var/log/messages 查看简化线程树：pstree -a >> /opt/pstree-20180915.log 其他机子 ping（多个地区 ping），看下解析 IP 与网络丢包 查看网络节点情况：traceroute www.youmeek.com ifconfig 查看 dropped 和 error 是否在不断增加，判断网卡是否出现问题 nslookup 命令查看 DNS 是否可用 如果 nginx 有安装：http_stub_status_module 模块，则查看当前统计 查看 TCP 和 UDP 应用 netstat -ntlp netstat -nulp 统计当前连接的一些状态情况：netstat -nat |awk '{print $6}'|sort|uniq -c|sort -rn 查看每个 ip 跟服务器建立的连接数：netstat -nat|awk '{print$5}'|awk -F : '{print$1}'|sort|uniq -c|sort -rn 查看与后端应用端口连接的有多少：lsof -i:8080|grep 'TCP'|wc -l 跟踪程序（按 Ctrl + C 停止跟踪）：strace -tt -T -v -f -e trace=file -o /opt/strace-20180915.log -s 1024 -p PID 看下谁在线：w，last 看下执行了哪些命令：history 程序、JVM 层面 保存、查看 Nginx 程序 log 通过 GoAccess 分析 log 保存、查看 Java 程序 log 使用内置 tomcat-manager 监控配置，或者使用类似工具：psi-probe 使用 ps -ef | grep java，查看进程 PID 根据高 CPU 的进程 PID，查看其线程 CPU 使用情况：top -Hp PID，找到占用 CPU 资源高的线程 PID 保存堆栈情况：jstack -l PID >> /opt/jstack-tomcat1-PID-20180917.log 把占用 CPU 资源高的线程十进制的 PID 转换成 16 进制：printf \"%x\\n\" PID，比如：printf \"%x\\n\" 12401 得到结果是：3071 在刚刚输出的那个 log 文件中搜索：3071，可以找到：nid=0x3071 使用 jstat -gc PID 10000 10，查看gc情况（截图） 使用 jstat -gccause PID：额外输出上次GC原因（截图） 使用 jstat -gccause PID 10000 10：额外输出上次GC原因，收集 10 次，每隔 10 秒 使用 jmap -dump:format=b,file=/opt/dumpfile-tomcat1-PID-20180917.hprof PID，生成堆转储文件 使用 jhat 或者可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）分析堆情况。 结合代码解决内存溢出或泄露问题。 给 VM 增加 dump 触发参数：-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/tomcat-1.hprof 一次 JVM 引起的 CPU 高排查 使用 ps -ef | grep java，查看进程 PID 根据高 CPU 的进程 PID，查看其线程 CPU 使用情况：top -Hp PID，找到占用 CPU 资源高的线程 PID 保存堆栈情况：jstack -l PID >> /opt/jstack-tomcat1-PID-20181017.log 把占用 CPU 资源高的线程十进制的 PID 转换成 16 进制：printf \"%x\\n\" PID，比如：printf \"%x\\n\" 12401 得到结果是：3071 在刚刚输出的那个 log 文件中搜索：3071，可以找到：nid=0x3071 也可以在终端中直接看：jstack PID |grep 十六进制线程 -A 30，此时如果发现如下： \"GC task thread#0 (ParallelGC)\" os_prio=0 tid=0x00007fd0ac01f000 nid=0x66f runnable 这种情况一般是 heap 设置得过小，而又要频繁分配对象；二是内存泄露，对象一直不能被回收，导致 CPU 占用过高 使用：jstat -gcutil PID 3000 10： 正常情况结果应该是这样的： S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 0.00 67.63 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.71 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.71 38.09 78.03 68.82 124 0.966 5 0.778 1.744 S0：SO 当前使用比例 S1：S1 当前使用比例 E：Eden 区使用比例（百分比）（异常的时候，这里可能会接近 100%） O：old 区使用比例（百分比）（异常的时候，这里可能会接近 100%） M：Metaspace 区使用比例（百分比）（异常的时候，这里可能会接近 100%） CCS：压缩使用比例 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间（单位秒） GCT：垃圾回收消耗总时间（单位秒） 异常的时候每次 Full GC 时间也可能非常长，每次时间计算公式=FGCT值/FGC指） jmap -heap PID，查看具体占用量是多大 使用 jmap -dump:format=b,file=/opt/dumpfile-tomcat1-PID-20180917.hprof PID，生成堆转储文件（如果设置的 heap 过大，dump 下来会也会非常大） 使用 jhat 或者可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）分析堆情况。 一般这时候就只能根据 jhat 的分析，看源码了 这里有几篇类似经历的文章推荐给大家： 三个神奇bug之Java占满CPU CPU 负载过高问题排查 CPU 低，负载高，访问慢（带数据库） 基于上面，但是侧重点在于 I/O 读写，以及是否有 MySQL 死锁，或者挂载了 NFS，而 NFS Server 出现问题 mysql 下查看当前的连接数与执行的sql 语句：show full processlist; 检查慢查询日志，可能是慢查询引起负载高，根据配置文件查看存放位置：log_slow_queries 查看 MySQL 设置的最大连接数：show variables like 'max_connections'; 重新设置最大连接数：set GLOBAL max_connections=300 参考资料 http://man.linuxde.net/dd https://linux.cn/article-6104-1.html http://www.cnblogs.com/ggjucheng/archive/2013/01/13/2858923.html http://coolnull.com/3649.html http://www.rfyy.net/archives/2456.html http://programmerfamily.com/blog/linux/sav.html https://www.jianshu.com/p/3991c0dba094 https://www.jianshu.com/p/3667157d63bb https://www.cnblogs.com/yjd_hycf_space/p/7755633.html http://silverd.cn/2016/05/27/nginx-access-log.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Nmon.html":{"url":"Linux-Tutorial/markdown-file/Nmon.html","title":"nmon 系统性能监控工具","keywords":"","body":"nmon 系统性能监控工具的使用 nmon 说明 官网：http://nmon.sourceforge.net/pmwiki.php 分析工具 nmon analyser：https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/Power+Systems/page/nmon_analyser nmon是一种在AIX与各种Linux操作系统上广泛使用的监控与分析工具， nmon所记录的信息是比较全面的，它能在系统运行过程中实时地捕捉系统资源的使用情况，并且能输出结果到文件中。 下载/安装 Ubuntu：sudo apt-get install -y nmon CentOS：sudo yum install -y nmon，前提是你已经有安装 epel 源 或者使用 RPM 包：http://pan.baidu.com/s/1hsFEoeg 安装命令：rpm -ivh nmon-14i-8.el6.x86_64.rpm 分析工具 nmon analyser：http://pan.baidu.com/s/1pKBLXrX 运行 实时监控：nmon 后台监控：cd /opt ; nmon -f -s 10 -c 360 前面的 cd /opt 表示，进入 opt 目录，nmon 生成的文件是在当前目录下。 -f ：按标准格式输出文件名称：_YYYYMMDD_HHMM.nmon -s ：每隔n秒抽样一次，这里为10秒 -c ：取出多少个抽样数量，这里为360，即监控=10*360/3600=1小时 该命令启动后，nmon 会在当前目录下生成监控文件，并持续写入资源数据，直至360个监控点收集完成——即监控1小时，这些操作均自动完成，无需手工干预，测试人员可以继续完成其他操作。如果想停止该监控，需要通过 ps -ef | grep nmon 查询进程号，然后杀掉该进程以停止监控。 定期监控：本质是 crontab 加上后台监控命令 解析监控文件 把 nmon 文件转换成 csv 文件：sort localhost_120427_0922.nmon > localhost_120427_0922.csv 把 csv 转换成 Excel 图表文件： 打开 nmon analyser 分析工具：nmon analyser v50_2.xlsm 点击 Analyse nmon data 会弹出一个弹出框，选择刚刚转换的 csv 文件，然后就会自动再转化成 excel 文件 导出的综合报表的参数说明：http://www.51testing.com/html/25/15146625-3714909.html 资料 Nmon命令行：Linux系统性能的监测利器 性能监控和分析工具--nmon nmon以及nmon analyser 教程 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Glances-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Glances-Install-And-Settings.html","title":"Glances 安装和配置","keywords":"","body":"Glances 安装和配置 Glances 介绍 相对 top、htop，它比较重，因此内容也比较多。小机子一般不建议安装。大机子一般也不建议一直开着。 官网：https://nicolargo.github.io/glances/ 官网 Github：https://github.com/nicolargo/glances 官网文档：https://glances.readthedocs.io/en/latest/ 当前（201810）最新版本为 3.0.2 Glances Linux 安装 curl -L https://bit.ly/glances | /bin/bash 需要 5 ~ 10 分钟左右。 用法 本地监控 进入实时监控面板（默认 3 秒一次指标）：glances 每间隔 5 秒获取一次指标：glances -t 5 在控制面板中可以按快捷键进行排序、筛选 m : 按内存占用排序进程 p : 按进程名称排序进程 c : 按 CPU 占用率排序进程 i : 按 I/O 频率排序进程 a : 自动排序进程 d : 显示/隐藏磁盘 I/O 统计信息 f : 显示/隐藏文件系统统计信息 s : 显示/隐藏传感器统计信息 y : 显示/隐藏硬盘温度统计信息 l : 显示/隐藏日志 n : 显示/隐藏网络统计信息 x : 删除警告和严重日志 h : 显示/隐藏帮助界面 q : 退出 w : 删除警告记录 监控远程机子 这里面的检控方和被监控的概念要弄清楚 作为服务端的机子运行（也就是被监控方）：glances -s 假设它的 IP 为：192.168.1.44 必需打开 61209 端口 作为客户端的机子运行（要查看被检控方的数据）：glances -c 192.168.1.44 这时候控制台输出的内容是被监控机子的数据 导出数据 个人测试没效果，后续再看下吧。 官网文档：https://glances.readthedocs.io/en/latest/search.html?q=export&check_keywords=yes&area=default 导出 CSV：glances --export-csv /tmp/glances.csv 导出 JSON：glances --export-json /tmp/glances.json 资料 https://linux.cn/article-6882-1.html http://www.qingpingshan.com/pc/fwq/394078.html https://www.imooc.com/article/81038?block_id=tuijian_wz http://pdf.us/2018/02/28/684.html https://www.sysgeek.cn/monitor-linux-servers-glances-tool/ https://www.jianshu.com/p/639581a96512 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/SSH.html":{"url":"Linux-Tutorial/markdown-file/SSH.html","title":"SSH（Secure Shell）介绍","keywords":"","body":"SSH（Secure Shell）介绍 SSH 安装 查看是否已安装： CentOS：rpm -qa | grep openssh Ubuntu：dpkg -l | grep openssh 安装： CentOS 6：sudo yum install -y openssh-server openssh-clients Ubuntu：sudo apt-get install -y openssh-server openssh-client 特别注意：本文内容，修改完配置都要记得重启服务 CentOS 6 命令：service sshd restart CentOS 7 命令：systemctl restart sshd.service SSH 修改连接端口 配置文件介绍（记得先备份）：sudo vim /etc/ssh/sshd_config 打开这一行注释：Port 22 自定义端口选择建议在万位的端口，如：10000-65535之间，假设这里我改为 60001 CentOS 6 给新端口加到防火墙中： 添加规则：iptables -I INPUT -p tcp -m tcp --dport 60001 -j ACCEPT 保存规则：service iptables save 重启 iptables：service iptables restart CentOS 7：添加端口：firewall-cmd --zone=public --add-port=60001/tcp --permanent 重启防火墙：firewall-cmd --reload 设置超时 ClientAliveInterval指定了服务器端向客户端请求消息的时间间隔, 默认是0，不发送。而ClientAliveInterval 300表示5分钟发送一次，然后客户端响应，这样就保持长连接了。 ClientAliveCountMax，默认值3。ClientAliveCountMax表示服务器发出请求后客户端没有响应的次数达到一定值，就自动断开，正常情况下，客户端不会不响应。 正常我们可以设置为： ClientAliveInterval 300 ClientAliveCountMax 3 按上面的配置的话，300*3＝900秒＝15分钟，即15分钟客户端不响应时，ssh连接会自动退出。 SSH 允许 root 账户登录 编辑配置文件（记得先备份）：sudo vim /etc/ssh/sshd_config 允许 root 账号登录 注释掉：PermitRootLogin without-password 新增一行：PermitRootLogin yes SSH 不允许 root 账户登录 新增用户和把新增的用户改为跟 root 同等权限方法：[Bash.md] 编辑配置文件（记得先备份）：sudo vim /etc/ssh/sshd_config 注释掉这一句（如果没有这一句则不管它）：PermitRootLogin yes SSH 密钥登录 生成秘钥和公钥文件，命令：sudo ssh-keygen，在交互提示中连续按三次回车，如果看得懂交互的表达，那就根据你自己需求来。默认生成密钥和公钥文件是在：/root/.ssh。 进入生成目录：cd /root/.ssh，可以看到有两个文件：id_rsa (私钥) 和 id_rsa.pub (公钥) 在 .ssh 目录下创建 SSH 认证文件，命令：touch /root/.ssh/authorized_keys 将公钥内容写到SSH认证文件里面，命令：cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys 修改SSH认证文件权限，命令： sudo chmod 700 /root/.ssh sudo chmod 600 /root/.ssh/authorized_keys 重启服务：sudo service sshd restart 设置 SSH 服务默认启动：sudo sysv-rc-conf ssh on 现在 SSH 客户端可以去拿着 SSH 服务器端上的 id_rsa，在客户端指定秘钥文件地址即可，这个一般由于你使用的客户端决定的，我这里推荐的是 Xshell 软件。 另外一种方法可以查看：SSH 免密登录（推荐） 限制只有某一个IP才能远程登录服务器 在该配置文件：vim /etc/hosts.deny 添加该内容：sshd:ALL 在该配置文件：vim /etc/hosts.allow 添加该内容：sshd:123.23.1.23 限制某些用户可以 SSH 访问 在该配置文件：vim /etc/ssh/sshd_config 添加该内容：AllowUsers root userName1 userName2 常用 SSH 连接终端 Windows -- Xshell：http://www.youmeek.com/ssh-terminal-emulator-recommend-xshell-and-xftp/ Mac -- ZOC：http://xclient.info/s/zoc-terminal.html 查看 SSH 登录日志 CentOS 6 查看登录失败记录：cat /var/log/auth.log | grep \"Failed password\" 如果数据太多可以用命令：tail -500f /var/log/auth.log | grep \"Failed password\" 统计哪些 IP 尝试多少次失败登录记录：grep \"Failed password\" /var/log/auth.log | awk ‘{print $11}’ | uniq -c | sort -nr 统计哪些 IP 尝试多少次失败登录记录，并排序：grep \"Failed password\" /var/log/auth.log | awk '{print $11}' | sort | uniq -c | sort -nr | more CentOS 7 查看登录失败：egrep \"Failed|Failure\" /var/log/secure，可以得到类似信息： 如果数据太多，可以用命令：tail -500f /var/log/secure | egrep \"Failed|Failure\" Feb 21 16:10:15 U5NHTIHOW67HKAH sshd[32753]: Failed password for invalid user oracle from 1.175.83.6 port 46560 ssh2 Feb 21 16:10:16 U5NHTIHOW67HKAH sshd[32750]: Failed password for root from 42.7.26.88 port 62468 ssh2 Feb 21 16:10:17 U5NHTIHOW67HKAH sshd[32744]: Failed password for root from 42.7.26.85 port 36086 ssh2 Feb 21 16:10:18 U5NHTIHOW67HKAH sshd[32756]: Failed password for invalid user oracle from 1.175.83.6 port 46671 ssh2 Feb 21 16:10:20 U5NHTIHOW67HKAH sshd[32744]: Failed password for root from 42.7.26.85 port 36086 ssh2 Feb 21 16:10:21 U5NHTIHOW67HKAH sshd[32750]: Failed password for root from 42.7.26.88 port 62468 ssh2 Feb 21 16:10:21 U5NHTIHOW67HKAH sshd[32758]: Failed password for invalid user oracle from 1.175.83.6 port 46811 ssh2 查看登录失败统计：grep \"authentication failure\" /var/log/secure，可以得到类似信息： 如果数据太多，可以用命令：tail -500f /var/log/secure | grep \"authentication failure\" Feb 21 02:01:46 U5NHTIHOW67HKAH sshd[16854]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:01:46 U5NHTIHOW67HKAH sshd[16854]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:01:47 U5NHTIHOW67HKAH sshd[16858]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:02:02 U5NHTIHOW67HKAH sshd[16858]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:02:02 U5NHTIHOW67HKAH sshd[16858]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:03:11 U5NHTIHOW67HKAH sshd[16870]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:03:25 U5NHTIHOW67HKAH sshd[16870]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:03:25 U5NHTIHOW67HKAH sshd[16870]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:03:29 U5NHTIHOW67HKAH sshd[16872]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:03:44 U5NHTIHOW67HKAH sshd[16872]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:03:44 U5NHTIHOW67HKAH sshd[16872]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:03:45 U5NHTIHOW67HKAH sshd[16875]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:01 U5NHTIHOW67HKAH sshd[16875]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:04:01 U5NHTIHOW67HKAH sshd[16875]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:05 U5NHTIHOW67HKAH sshd[16878]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:20 U5NHTIHOW67HKAH sshd[16878]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:04:20 U5NHTIHOW67HKAH sshd[16878]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:24 U5NHTIHOW67HKAH sshd[16883]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:40 U5NHTIHOW67HKAH sshd[16883]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:04:40 U5NHTIHOW67HKAH sshd[16883]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:43 U5NHTIHOW67HKAH sshd[16886]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:04:59 U5NHTIHOW67HKAH sshd[16886]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:04:59 U5NHTIHOW67HKAH sshd[16886]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:02 U5NHTIHOW67HKAH sshd[16888]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:08 U5NHTIHOW67HKAH sshd[16891]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=146.0.228.146 Feb 21 02:05:18 U5NHTIHOW67HKAH sshd[16888]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:05:18 U5NHTIHOW67HKAH sshd[16888]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:20 U5NHTIHOW67HKAH sshd[16899]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:34 U5NHTIHOW67HKAH sshd[16899]: Disconnecting: Too many authentication failures for root [preauth] Feb 21 02:05:34 U5NHTIHOW67HKAH sshd[16899]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:37 U5NHTIHOW67HKAH sshd[16901]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=42.7.26.85 user=root Feb 21 02:05:53 U5NHTIHOW67HKAH sshd[16901]: Disconnecting: Too many authentication failures for root [preauth] 防止 SSH 暴力破解：DenyHosts 官网地址：https://github.com/denyhosts/denyhosts 参考文章： http://blog.51cto.com/linuxroad/673425 http://blog.csdn.net/wanglei_storage/article/details/50849070 https://chegva.com/2338.html http://blog.csdn.net/miner_k/article/details/78948100 SSH 资料 http://www.jikexueyuan.com/course/861_1.html?ss=1 http://www.361way.com/ssh-autologout/4679.html http://www.osyunwei.com/archives/672.html https://www.tecmint.com/find-failed-ssh-login-attempts-in-linux/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/FTP.html":{"url":"Linux-Tutorial/markdown-file/FTP.html","title":"FTP（File Transfer Protocol）介绍","keywords":"","body":"FTP（File Transfer Protocol）介绍 FTP 安装 查看是否已安装： CentOS：rpm -qa | grep vsftpd Ubuntu：dpkg -l | grep vsftpd 安装： CentOS 6：sudo yum install -y vsftpd Ubuntu：sudo apt-get install -y vsftpd FTP 使用之前要点 关闭 CentOS 上的 SELinux 组件（Ubuntu 体系是没有这东西的）。 查看 SELinux 开启状态：sudo getenforce 有如下三种状态，默认是 Enforcing Enforcing（开启） Permissive（开启，但是只起到警告作用，属于比较轻的开启） Disabled（关闭） 临时关闭： 命令：sudo setenforce 0 临时开启： 命令：sudo setenforce 1 永久关闭： 命令：sudo vim /etc/selinux/config 将：SELINUX=enforcing 改为 SELINUX=disbaled，配置好之后需要重启系统。 FTP 服务器配置文件常用参数 vsftpd 默认是支持使用 Linux 系统里的账号进行登录的（登录上去可以看到自己的 home 目录内容），权限跟 Linux 的账号权限一样。但是建议使用软件提供的虚拟账号管理体系功能，用虚拟账号登录。 配置文件介绍（记得先备份）：sudo vim /etc/vsftpd/vsftpd.conf，比较旧的系统版本是：vim /etc/vsftpd.conf 该配置主要参数解释： anonymous_enable=NO #不允许匿名访问，改为YES即表示可以匿名登录 anon_upload_enable=YES #是否允许匿名用户上传 anon_mkdir_write_enable=YES #是否允许匿名用户创建目录 local_enable=YES #是否允许本地用户，也就是linux系统的已有账号，如果你要FTP的虚拟账号，那可以改为NO write_enable=YES #是否允许本地用户具有写权限 local_umask=022 #本地用户掩码 chroot_list_enable=YES #不锁定用户在自己的家目录，默认是注释，建议这个一定要开，比如本地用户judasn，我们只能看到/home/judasn，没办法看到/home目录 chroot_list_file=/etc/vsftpd/chroot_list #该选项是配合上面选项使用的。此文件中的用户将启用 chroot，如果上面的功能开启是不够的还要把用户名加到这个文件里面。配置好后，登录的用户，默认登录上去看到的根目录就是自己的home目录。 listen=YES #独立模式 userlist_enable=YES #用户访问控制，如果是YES，则表示启用vsftp的虚拟账号功能，虚拟账号配置文件是/etc/vsftpd/user_list userlist_deny=NO #这个属性在配置文件是没有的，当userlist_enable=YES，这个值也为YES，则user_list文件中的用户不能登录FTP，列表外的用户可以登录，也可以起到一个黑名单的作用。当userlist_enable=YES，这个值为NO，则user_list文件中的用户能登录FTP，列表外的用户不可以登录，也可以起到一个白名单的作用。如果同一个用户即在白名单中又在ftpusers黑名单文件中，那还是会以黑名单为前提，对应账号没法登录。 tcp_wrappers=YES #是否启用TCPWrappers管理服务 FTP用户黑名单配置文件：sudo vim /etc/vsftpd/ftpusers，默认root用户也在黑名单中 控制FTP用户登录配置文件：sudo vim /etc/vsftpd/user_list 启动服务： service vsftpd restart vsftpd 的两种传输模式 分为：主动模式（PORT）和被动模式（PASV）。这两个模式会涉及到一些端口问题，也就涉及到防火墙问题，所以要特别注意。主动模式比较简单，只要在防火墙上放开放开 21 和 20 端口即可。被动模式则要根据情况放开一个端口段。 上图箭头：xftp 新建连接默认都是勾选被动模式的，所以如果要使用主动模式，在该连接的属性中是要去掉勾选。 vsftpd 的两种运行模式 分为：xinetd 模式和 standalone 模式 xinetd 模式：由 xinetd 作为 FTP 的守护进程，负责 21 端口的监听，一旦外部发起对 21 端口的连接，则调用 FTP 的主程序处理，连接完成后，则关闭 FTP 主程序，释放内存资源。好处是资源占用少，适合 FTP 连接数较少的场合。 standalone 模式：直接使用 FTP 主程序作为 FTP 的守护进程，负责 21 端口的监听，由于无需经过 xinetd 的前端代理，响应速度快，适合连接数 较大的情况，但由于 FTP 主程序长期驻留内存，故较耗资源。 standalone 一次性启动，运行期间一直驻留在内存中，优点是对接入信号反应快，缺点是损耗了一定的系统资源，因此经常应用于对实时反应要求较高的 专业 FTP 服务器。 xinetd 恰恰相反，由于只在外部连接发送请求时才调用 FTP 进程，因此不适合应用在同时连接数量较多的系统。此外，xinetd 模式不占用系统资源。除了反应速度和占用资源两方面的影响外，vsftpd 还提供了一些额外的高级功能，如 xinetd 模式支持 per_IP (单一 IP)限制，而 standalone 模式则更有利于 PAM 验证功能的应用。 配置 xinetd 模式： 编辑配置文件：sudo vim /etc/xinetd.d/vsftpd 属性信息改为如下信息： disable = no socket_type = stream wait = no #这表示设备是激活的，它正在使用标准的TCP Sockets 编辑配置文件：sudo vim /etc/vsftpd/vsftpd.conf 如果该配置选项中的有 listen=YES，则要注释掉 重启 xinetd 服务，命令：sudo /etc/rc.d/init.d/xinetd restart 配置 standalone 模式： 编辑配置文件：sudo vim /etc/xinetd.d/vsftpd 属性信息改为如下信息： disable = yes 编辑配置文件：sudo vim /etc/vsftpd/vsftpd.conf 属性信息改为如下信息： Listen=YES（如果是注释掉则要打开注释） 重启服务：sudo service vsftpd restart FTP 资料 http://www.jikexueyuan.com/course/994.html http://www.while0.com/36.html http://www.cnblogs.com/CSGrandeur/p/3754126.html http://www.centoscn.com/image-text/config/2015/0613/5651.html http://wiki.ubuntu.org.cn/Vsftpd Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"work/note/linux/ftp.html":{"url":"work/note/linux/ftp.html","title":"ftp 配置文件详细介绍","keywords":"","body":"配置文件 基本介绍  匿名用户使用的登陆名为ftp或anonymous，口令为空；匿名用户不能离开匿名用户家目录/var/ftp,且只能下载不能上传 。 本地用户的登录名为本地用户名，口令为此本地用户的口令；本地用户可以在自己家目录中进行读写操作；本地用户可以离开自家目录切换至有权限访问的其他目录，并在权限允许的情况下进行上传/下载 。 写在文件/etc/vsftpd.ftpusers中的本地用户禁止登陆 。 配置文件格式：  vsftpd.conf?的内容非常单纯，每一行即为一项设定 。 若是空白行或是开头为#的一行，将会被忽略 。 内容的格式只有一种，如下所示 option=value 要注意的是，等号两边不能加空白 。 匿名用户（anonymous）设置 anonymous_enable=YES/NO（YES）控制是否允许匿名用户登入，YES?为允许匿名登入，NO?为不允许 。 默认值为YES 。 write_enable=YES/NO（YES）是否允许登陆用户有写权限 。 属于全局设置，默认值为YES 。 no_anon_password=YES/NO（NO）若是启动这项功能，则使用匿名登入时，不会询问密码 。 默认值为NO 。 ftpftp_username=ftp定义匿名登入的使用者名称 。 默认值为ftp 。 anon_root=/var/ftp使用匿名登入时，所登入的目录 。 默认值为/var/ftp 。 注意ftp目录不能是777的权限属性，即匿名用户的家目录不能有777的权限 。 anon_upload_enable=YES/NO（NO）如果设为YES，则允许匿名登入者有上传文件（非目录）的权限，只有在write_enable=YES时，此项才有效 。 当然，匿名用户必须要有对上层目录的写入权 。 默认值为NO 。 anon_world_readable_only=YES/NO（YES）如果设为YES，则允许匿名登入者下载可阅读的档案（可以下载到本机阅读，不能直接在FTP服务器中打开阅读） 。 默认值为YES 。 anon_mkdir_write_enable=YES/NO（NO）如果设为YES，则允许匿名登入者有新增目录的权限，只有在write_enable=YES时，此项才有效 。 当然，匿名用户必须要有对上层目录的写入权 。 默认值为NO 。 anon_other_write_enable=YES/NO（NO）如果设为YES，则允许匿名登入者更多于上传或者建立目录之外的权限，譬如删除或者重命名 。（如果anon_upload_enable=NO，则匿名用户?不能上传文件，但可以删除或者重命名已经存在的文件；如果anon_mkdir_write_enable=NO，则匿名用户不能上传或者新建文件夹，但?可以删除或者重命名已经存在的文件夹 。 ）默认值为NO 。 chown_uploads=YES/NO（NO）设置是否改变匿名用户上传文件（非目录）的属主 。 默认值为NO 。 chown_username=username设置匿名用户上传文件（非目录）的属主名 。 建议不要设置为root 。 anon_umask=077设置匿名登入者新增或上传档案时的umask?值 。 默认值为077，则新建档案的对应权限为700 。 deny_email_enable=YES/NO（NO）若是启动这项功能，则必须提供一个档案/etc/vsftpd/banner_emails，内容为email?address 。 若是使用匿名登入，则会要求输入email?address，若输入的email?address?在此档案内，则不允许进入 。 默认值为NO 。 banned_email_file=/etc/vsftpd/banner_emails此文件用来输入email?address，只有在deny_email_enable=YES时，才会使用到此档案 。 若是使用匿名登入，则会要求输入email?address，若输入的email?address?在此档案内，则不允许进入 。 本地用户设置 local_enable=YES/NO（YES）控制是否允许本地用户登入，YES?为允许本地用户登入，NO为不允许 。 默认值为YES 。 local_root=/home/username当本地用户登入时，将被更换到定义的目录下 。 默认值为各用户的家目录 。 write_enable=YES/NO（YES）是否允许登陆用户有写权限 。 属于全局设置，默认值为YES 。 local_umask=022本地用户新增档案时的umask?值 。 默认值为077 。 file_open_mode=0755本地用户上传档案后的档案权限，与chmod?所使用的数值相同 。 默认值为0666 。 欢迎语设置 dirmessage_enable=YES/NO（YES）如果启动这个选项，那么使用者第一次进入一个目录时，会检查该目录下是否有.message这个档案，如果有，则会出现此档案的内容，通常这个档案会放置欢迎话语，或是对该目录的说明 。 默认值为开启 。 message_file=.message设置目录消息文件，可将要显示的信息写入该文件 。 默认值为.message 。 banner_file=/etc/vsftpd/banner当使用者登入时，会显示此设定所在的档案内容，通常为欢迎话语或是说明 。 默认值为无 。 如果欢迎信息较多，则使用该配置项 。 ftpd_banner=Welcome?to?BOB’s?FTP?server这里用来定义欢迎话语的字符串，banner_file是档案的形式，而ftpd_banner?则是字符串的形式 。 预设为无 。 控制用户是否允许切换到上级目录 在默认配置下，本地用户登入FTP后可以使用cd命令切换到其他目录，这样会对系统带来安全隐患 。 可以通过以下三条配置文件来控制用户切换目录 。 chroot_list_enable=YES/NO（NO）设置是否启用chroot_list_file配置项指定的用户列表文件 。 默认值为NO 。 chroot_list_file=/etc/vsftpd.chroot_list用于指定用户列表文件，该文件用于控制哪些用户可以切换到用户家目录的上级目录 。 chroot_local_user=YES/NO（NO）用于指定用户列表文件中的用户是否允许切换到上级目录 。 默认值为NO 。 通过搭配能实现以下几种效果： ①当chroot_list_enable=YES，chroot_local_user=YES时，在/etc/vsftpd.chroot_list文件中列出的用户，可以切换到其他目录；未在文件中列出的用户，不能切换到其他目录 。②当chroot_list_enable=YES，chroot_local_user=NO时，在/etc/vsftpd.chroot_list文件中列出的用户，不能切换到其他目录；未在文件中列出的用户，可以切换到其他目录 。③当chroot_list_enable=NO，chroot_local_user=YES时，所有的用户均不能切换到其他目录 。④当chroot_list_enable=NO，chroot_local_user=NO时，所有的用户均可以切换到其他目录 。 数据传输模式设置 FTP在传输数据时，可以使用二进制方式，也可以使用ASCII模式来上传或下载数据 。 ascii_upload_enable=YES/NO（NO）设置是否启用ASCII?模式上传数据 。 默认值为NO 。 ascii_download_enable=YES/NO（NO）设置是否启用ASCII?模式下载数据 。 默认值为NO 。 访问控制设置 两种控制方式：一种控制主机访问，另一种控制用户访问 。 控制主机访问： tcp_wrappers=YES/NO（YES）设置vsftpd是否与tcp?wrapper相结合来进行主机的访问控制 。 默认值为YES 。 如果启用，则vsftpd服务器会检查/etc/hosts.allow?和/etc/hosts.deny?中的设置，来决定请求连接的主机，是否允许访问该FTP服务器 。这两个文件可以起到简易的防火墙功能 。比如：若要仅允许192.168.0.1—192.168.0.254的用户可以连接FTP服务器，则在/etc/hosts.allow文件中添加以下内容： vsftpd:192.168.0.?:allow all:all?:deny 控制用户访问： 对于用户的访问控制可以通过/etc目录下的vsftpd.user_list和ftpusers文件来实现 。 userlist_file=/etc/vsftpd.user_list控制用户访问FTP的文件，里面写着用户名称 。 一个用户名称一行 。 userlist_enable=YES/NO（NO）是否启用vsftpd.user_list文件 。 userlist_deny=YES/NO（YES）决定vsftpd.user_list文件中的用户是否能够访问FTP服务器 。若设置为YES，则vsftpd.user_list文件中的用户不允许访问FTP，若设置为NO，则只有vsftpd.user_list文件中的用户才能访问FTP 。 /etc/vsftpd/ftpusers文件专门用于定义不允许访问FTP服务器的用户列表（注意:如果?userlist_enable=YES,userlist_deny=NO,此时如果在vsftpd.user_list和ftpusers中都有某个?用户时，那么这个用户是不能够访问FTP的，即ftpusers的优先级要高） 。 默认情况下vsftpd.user_list和ftpusers，这两个?文件已经预设置了一些不允许访问FTP服务器的系统内部账户 。如果系统没有这两个文件，那么新建这两个文件，将用户添加进去即可 。 访问速率设置 anon_max_rate=0设置匿名登入者使用的最大传输速度，单位为B/s，0?表示不限制速度 。 默认值为0 。 local_max_rate=0本地用户使用的最大传输速度，单位为B/s，0?表示不限制速度 。 预设值为0 。 超时时间设置 accept_timeout=60设置建立FTP连接的超时时间，单位为秒 。 默认值为60 。 connect_timeout=60PORT?方式下建立数据连接的超时时间，单位为秒 。 默认值为60 。 data_connection_timeout=120设置建立FTP数据连接的超时时间，单位为秒 。 默认值为120 。 idle_session_timeout=300设置多长时间不对FTP服务器进行任何操作，则断开该FTP连接，单位为秒 。 默认值为300? 。 日志文件设置 xferlog_enable=?YES/NO（YES）是否启用上传/下载日志记录 。 如果启用，则上传与下载的信息将被完整纪录在xferlog_file?所定义的档案中 。 预设为开启 。 xferlog_file=/var/log/vsftpd.log 设置日志文件名和路径，默认值为/var/log/vsftpd.log 。 xferlog_std_format=YES/NO（NO）如果启用，则日志文件将会写成xferlog的标准格式，如同wu-ftpd?一般 。 默认值为关闭 。 log_ftp_protocol=YES|NO（NO）如果启用此选项，所有的FTP请求和响应都会被记录到日志中，默认日志文件在/var/log/vsftpd.log 。 启用此选项时，xferlog_std_format不能被激活 。 这个选项有助于调试 。 默认值为NO 。 定义用户配置文件 在vsftpd中，可以通过定义用户配置文件来实现不同的用户使用不同的配置 。 user_config_dir=/etc/vsftpd/userconf设置用户配置文件所在的目录 。 当设置了该配置项后，用户登陆服务器后，系统就会到/etc/vsftpd/userconf目录下，读取与当前用户名相同的文件，并根据文件中的配置命令，对当前用户进行更进一步的配置 。例如：定义user_config_dir=/etc/vsftpd/userconf，且主机上有使用者?test1,test2，那么我们就在user_config_dir?的目录新增文件名为test1和test2两个文件 。 若是test1?登入，则会读取user_config_dir?下的test1?这个档案内的设定 。 默认值为无 。 利用用户配置文件，可以实现对不同用户进行访问速度的控制，在各用户配置文件中定义local_max_rate=XX，?即可 。 FTP的工作方式与端口设置 FTP有两种工作方式：PORT?FTP（主动模式）和PASV?FTP（被动模式） listen_port=21设置FTP服务器建立连接所监听的端口，默认值为21 。 connect_from_port_20=YES/NO指定FTP使用20端口进行数据传输，默认值为YES 。 ftp_data_port=20设置在PORT方式下，FTP数据连接使用的端口，默认值为20 。 pasv_enable=YES/NO（YES）若设置为YES，则使用PASV工作模式；若设置为NO，则使用PORT模式 。 默认值为YES，即使用PASV工作模式 。 pasv_max_port=0在PASV工作模式下，数据连接可以使用的端口范围的最大端口，0?表示任意端口 。 默认值为0 。 pasv_min_port=0在PASV工作模式下，数据连接可以使用的端口范围的最小端口，0?表示任意端口 。 默认值为0 。 与连接相关的设置 listen=YES/NO（YES）设置vsftpd服务器是否以standalone模式运行 。 以standalone模式运行是一种较好的方式，此时listen必须设置为YES，此为默?认值 。 建议不要更改，有很多与服务器运行相关的配置命令，需要在此模式下才有效 。 若设置为NO，则vsftpd不是以独立的服务运行，要受到xinetd?服务的管控，功能上会受到限制 。 max_clients=0设置vsftpd允许的最大连接数，默认值为0，表示不受限制 。 若设置为100时，则同时允许有100个连接，超出的将被拒绝 。 只有在standalone模式运行才有效 。 max_per_ip=0设置每个IP允许与FTP服务器同时建立连接的数目 。 默认值为0，表示不受限制 。 只有在standalone模式运行才有效 。 listen_address=IP地址设置FTP服务器在指定的IP地址上侦听用户的FTP请求 。 若不设置，则对服务器绑定的所有IP地址进行侦听 。 只有在standalone模式运行才有效 。 setproctitle_enable=YES/NO（NO）设置每个与FTP服务器的连接，是否以不同的进程表现出来 。 默认值为NO，此时使用ps?aux?|grep?ftp只会有一个vsftpd的进程 。 若设置为YES，则每个连接都会有一个vsftpd的进程 。 虚拟用户设置 虚拟用户使用PAM认证方式 。 pam_service_name=vsftpd设置PAM使用的名称，默认值为/etc/pam.d/vsftpd 。 guest_enable=?YES/NO（NO）启用虚拟用户 。 默认值为NO 。 guest_username=ftp这里用来映射虚拟用户 。 默认值为ftp 。 virtual_use_local_privs=YES/NO（NO）当该参数激活（YES）时，虚拟用户使用与本地用户相同的权限 。 当此参数关闭（NO）时，虚拟用户使用与匿名用户相同的权限 。 默认情况下此参数是关闭的（NO） 。 其他设置 text_userdb_names=?YES/NO（NO）设置在执行ls?–la之类的命令时，是显示UID、GID还是显示出具体的用户名和组名 。 默认值为NO，即以UID和GID方式显示 。 若希望显示用户名和组名，则设置为YES 。 ls_recurse_enable=YES/NO（NO）若是启用此功能，则允许登入者使用ls?–R（可以查看当前目录下子目录中的文件）这个指令 。 默认值为NO 。 hide_ids=YES/NO（NO）如果启用此功能，所有档案的拥有者与群组都为ftp，也就是使用者登入使用ls?-al之类的指令，所看到的档案拥有者跟群组均为ftp 。 默认值为关闭 。 download_enable=YES/NO（YES）如果设置为NO，所有的文件都不能下载到本地，文件夹不受影响 。 默认值为YES 。 常见问题 1：本地用户无法登陆 检查本地用户是否存在，是否有密码 检查登陆用户所配置的家目录local_root 是否具有访问权限访问权限 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"Linux-Tutorial/markdown-file/VPN.html":{"url":"Linux-Tutorial/markdown-file/VPN.html","title":"VPN（Virtual Private Network）介绍","keywords":"","body":"VPN（Virtual Private Network）介绍 VPN 介绍 VPN 分类有很多，这里主要讲 PPTPD 和 L2TPD、OpenVPN PPTPD 安装 查看是否已安装： CentOS：rpm -qa | grep pptpd Ubuntu：dpkg -l | grep pptpd 先安装依赖包： CentOS 6：sudo yum -y install ppp Ubuntu：sudo apt-get install -y ppp 安装： CentOS 6：sudo yum -y install pptpd Ubuntu：sudo apt-get install -y pptpd PPTPD 服务配置 注意：PPTPD 默认端口是 1723，所以防火墙要取消对其限制，查看 1723 端口是否开启：sudo netstat -apnl | grep 1723 编辑 PPTPD 配置文件介绍（记得先备份）：sudo vim /etc/pptpd.conf 修改配置信息： option /etc/ppp/options.pptpd #配置文件中有一行这样的参数，如果没有自己添加上去，正常默认是有的。同时也有可能是这样的一句话：option /etc/ppp/pptpd-options，具体你自己看下你的配置文件里面是什么内容。 logwtmp #默认这个是没有被注释的，这里要进行注释 localip 172.31.0.1 #本机服务器的内网IP地址，建议你的内网IP一般不要用常见网段，默认这个被注释，取消注释 remoteip 192.168.0.10-200 #客户端的IP地址范围，默认这个被注释，取消注释 编辑 PPP 配置文件介绍（记得先备份）：sudo vim /etc/ppp/options.pptpd 或是 sudo vim /etc/ppp/pptpd-options 在文件尾巴添加如下配置信息（默认配置文件应该有，只是被注释了）： ms-dns 8.8.8.8 #配置DNS，如果是境外服务器最好改为google的，国内的看情况 ms-dns 8.8.4.4 #配置DNS，如果是境外服务器最好改为google的，国内的看情况 开启系统转发（记得先备份）：sudo vim /etc/sysctl.conf 修改配置信息该值改为 1： net.ipv4.ip_forward=1 # 默认是注释掉的，要取消注释，也有出现是没有注释，但是默认是0：net.ipv4.ip_forward=0 刷新配置：sudo sysctl -p 设置 iptables 转发： 追加 iptables 规则：sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 设置默认启动使用该规则：sudo vim /etc/rc.local 在配置文件中添加：iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 添加登录账号（记得先备份）：sudo vim /etc/ppp/chap-secrets 在文件尾巴添加如下配置信息： mytest1 pptpd 123456 * #该格式分别表示：登录名、登录协议、密码、该连接上的客户端用什么 IP（* 表示随机一个 IP） 重启服务：sudo service pptpd restart Windows 连接 VPN 方法：http://www.cnblogs.com/yuzeren48/p/4123879.html L2TPD 安装 查看是否已安装： CentOS：rpm -qa | grep xl2tpd Ubuntu：dpkg -l | grep xl2tpd 安装： CentOS 6：XXXXXXXXXXXXXXXXXXXXXXXX Ubuntu：sudo apt-get install -y xl2tpd ppp openswan L2TPD 服务配置 注意：L2TPD 默认端口是 1701，所以防火墙要取消对其限制，查看 1701 端口是否开启：sudo netstat -apnl | grep 1701 编辑 Openswan 配置文件介绍（记得先备份）：sudo vim /etc/ipsec.conf left=172.31.201.255 #其中这里的IP地址改为本机的内网IP，文件中有两处，都进行修改 编辑 IPsec-based 配置文件介绍（记得先备份）：sudo vim /etc/ipsec.secrets 172.31.201.255 %any: PSK\"adc123456\" #在文件最后一行补充：（格式是：本机内网IP，后面是配置密钥。密钥不配置也可以但是建议配上去） 编辑 L2TPD 配置文件介绍（记得先备份）：sudo vim /etc/xl2tpd/xl2tpd.conf 修改配置信息： ipsec saref = yes require chap = yes refuse pap = yes require authentication = yes ppp debug = yes length bit = yes ip range = 192.168.1.10-192.168.1.200 #这是客户端连接本机的IP端限制 local ip = 172.31.201.255 #这是本机服务器端的内网 IP pppoptfile = /etc/ppp/options.xl2tpd #指定本机的 PPP 配置文件地址，如果你的 PPP 配置文件地址不是这里那就改下 编辑 PPP 配置文件介绍（记得先备份）：sudo vim /etc/ppp/options.xl2tpd (如果没有这个文件自己创建) 在文件尾巴添加如下配置信息： refuse-mschap-v2 refuse-mschap ms-dns 8.8.8.8 #配置DNS，如果是境外服务器最好改为google的，国内的看情况 ms-dns 8.8.4.4 #配置DNS，如果是境外服务器最好改为google的，国内的看情况 asyncmap 0 auth lock hide-password local name l2tpd proxyarp lcp-echo-interval 30 lcp-echo-failure 4 refuse refuse refuse 在终端输入命令：sudo sh -c 'for each in /proc/sys/net/ipv4/conf/* do echo 0 > $each/accept_redirects echo 0 > $each/send_redirects done' 开启系统转发（记得先备份）：sudo vim /etc/sysctl.conf 修改配置信息： net.ipv4.ip_forward=1 #默认是注释掉的，要取消注释 刷新配置：sudo sysctl -p 设置 iptables 转发： 追加 iptables 规则：sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 设置默认启动使用该规则：sudo vim /etc/rc.local 在配置文件中添加：iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 添加登录账号（记得先备份）：sudo vim /etc/ppp/chap-secrets 在文件尾巴添加如下配置信息： mytest1 l2tpd 123456 #该格式分别表示：登录名、登录协议、密码、该连接上的客户端用什么 IP（ 表示随机一个 IP） 重启服务：sudo service l2tpd restart Windows 连接 VPN 方法：http://www.cnblogs.com/yuzeren48/p/4123879.html OpenVPN 安装 主要软件版本信息： OpenVPN：2.3.11-1.el6 EasyRsa：2.2.2-1.el6 检查系统环境是否支持：cat /dev/net/tun，如果出现：cat: /dev/net/tun: File descriptor in bad state，则表示支持。如果现实的是：cat: /dev/net/tun: No such device，则不支持 需要先安装 EPEL 源，具体可以看这篇文章：CentOS 源设置 安装： 安装依赖包： yum install -y gcc make rpm-build autoconf.noarch openssl openssl-devel lzo lzo-devel pam pam-devel automake pkgconfig 安装 OpenVPN： yum install -y openvpn easy-rsa OpenVPN 服务配置 禁用 selinux 编辑配置文件：vim /etc/selinux/config 把 SELINUX=enforcing 改为 SELINUX=disabled 生成OpenVPN需要的服务器、客户端证书 使用 easy-rsa 的脚本产生证书 修改vars文件 cd /usr/share/easy-rsa/2.0，后面关于证书的操作都是在这个目录下 vim vars 需要修改的内容主要有下面这些信息（在文件 64 行）：注册信息，比如公司地址、公司名称、部门名称等。 export KEY_COUNTRY=\"CN\" export KEY_PROVINCE=\"GuangDong\" export KEY_CITY=\"GuangZhou\" export KEY_ORG=\"YouMeekOrganization\" export KEY_EMAIL=\"admin@youmeek.com\" export KEY_OU=\"YouMeekOrganizationalUnit\" 初始化环境变量 source vars 清除keys目录下所有与证书相关的文件，下面步骤生成的证书和密钥都在 /usr/share/easy-rsa/2.0/keys 目录里 ./clean-all 生成根证书 ca.crt 和根密钥 ca.key（会有好几个提示，你都不需要输入什么内容，一路按回车即可，除非你懂原理） ./build-ca 为服务端生成证书和密钥（也会有好几个提示，你都不需要输入什么内容，一路按回车即可，直到提示需要输入y/n时，输入y再按回车继续走） 其中：server，这个名字别改，照着来，等下那些就可以直接使用。 ./build-key-server server 每一个登陆的VPN客户端需要有一个证书，每个证书在同一时刻只能供一个客户端连接，下面语句是建立2份demo，你可以只输入第一个即可。 为客户端生成证书和密钥（一路按回车，直到提示需要输入y/n时，输入y再按回车，一共两次） 其中：client1，这个名字别改，照着来，等下那些就可以直接使用，除非你会。 ./build-key client1 ./build-key client2 创建迪菲·赫尔曼密钥，会在 keys 目录里面生成dh2048.pem文件（生成过程比较慢，在此期间不要去中断它，我这边花的时间是：2分钟） ./build-dh 设置OpenVPN服务端配置文件 复制一份服务器端配置文件模板server.conf到/etc/openvpn/ 其中，我这边现在的版本是：2.3.11，所以我这边是填这个，你的不一定就跟我一样，所以你先到 doc 目录下，看下你具体是哪个版本，改下下面这句命令即可。 cp /usr/share/doc/openvpn-2.3.11/sample/sample-config-files/server.conf /etc/openvpn/ 编辑 server.conf，把下面的内容替换到已有的文件中，记得去掉后面的注释，我这样写只是为了方便解释对应的内容含义： vim /etc/openvpn/server.conf port 1194 # 端口一般我们不改 proto udp # 默认使用udp，如果使用HTTP Proxy，改成tcp，必须使用tcp协议，还需要注意的是：需要与客户端配置保持一致，等下客户端配置会说道 dev tun ca /usr/share/easy-rsa/2.0/keys/ca.crt # 这一行默认值为：ca ca.crt cert /usr/share/easy-rsa/2.0/keys/server.crt # 这一行默认值为：cert server.crt key /usr/share/easy-rsa/2.0/keys/server.key # 这一行默认值为：key server.key dh /usr/share/easy-rsa/2.0/keys/dh2048.pem # 这一行默认值为：dh dh2048.pem server 10.192.170.0 255.255.255.0 # 给客户的分配的局域网IP段，默认虚拟局域网网段，不要和实际的局域网冲突即可，这个10.192.170.0跟下面还有一个地方配置也有关联，需要注意 ifconfig-pool-persist ipp.txt # 启用了ipp.txt作为客户端和virtual IP的对应表，以方便客户端重新连接可以获得同样的IP； keepalive 10 120 comp-lzo persist-key persist-tun status openvpn-status.log # OpenVPN的状态日志，默认为/etc/openvpn/openvpn-status.log log openvpn.log # OpenVPN的运行日志，默认为/etc/openvpn/openvpn.log log-append openvpn.log verb 3 push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 8.8.8.8\" push \"dhcp-option DOMAIN-SEARCH ap-northeast-1.compute.internal\" push \"dhcp-option DOMAIN-SEARCH ec2.drawbrid.ge\" 配置内核和防火墙，开启nat功能，启动服务 开启路由转发功能 sed -i '/net.ipv4.ip_forward/s/0/1/' /etc/sysctl.conf 刷新配置：sudo sysctl -p 辨别你的 VPS 是属于那种虚拟方式，主流有：Xen KVM OpenVZ，方法： sudo yum install -y virt-what，virt-what是一个判断当前环境所使用的虚拟技术的脚本，常见的虚拟技术基本上都能正常识别出来 sudo virt-what，我这边 Vultr 输出的结果是：KVM 配置防火墙 iptables -A INPUT -p udp --dport 1194 -j ACCEPT iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 设置IP转发，若你的 VPS 虚拟方案是：Xen 或 KVM的请输入：（eth0要根据具体的网卡标示来，可以通过ifconfig查看），其中：10.192.170.0/16 表示客户端连接上去后从这个区间中分配给客户端的IP地址 iptables -t nat -A POSTROUTING -s 10.192.170.0/16 -o eth0 -j MASQUERADE 若你的 VPS 虚拟方案是：OpenVZ 的请输入：（45.32.90.22是你VPS的IP） iptables -t nat -A POSTROUTING -s 10.192.170.0/16 -j SNAT --to-source 45.32.90.22 保存防火墙配置 service iptables save service iptables restart 启动openvpn并设置为开机启动 service openvpn start chkconfig openvpn on OpenVPN客户端配置 下载 Windows 客户端： https://openvpn.net/index.php/download/community-downloads.html Mac 用户下载这个 tunnelblick（该地址需要开穿越）： http://code.google.com/p/tunnelblick/ 从服务器上下载证书文件到本地： /usr/share/easy-rsa/2.0/keys/ca.crt /usr/share/easy-rsa/2.0/keys/client1.crt /usr/share/easy-rsa/2.0/keys/client1.key 到你本地的电脑上（window电脑在安装好OpenVPN软件后可以把如上证书拷贝到如下文件夹里：C:\\Program Files\\OpenVPN\\config ） 从服务器找到这个文件：/usr/share/doc/openvpn-2.3.11/sample/sample-windows/sample.ovpn，下载到本地电脑这个目录下：C:\\Program Files\\OpenVPN\\config 然后把这个文件改名字为：client1.ovpn 文件内如如下, 其他不用改只要把SERVER-IP 改成你服务器的 IP client #这个client不是自定义名称 不能更改 dev tun #要与前面server.conf中的配置一致。 proto udp #要与前面server.conf中的配置一致。 remote 45.32.90.22 1194 #将45.32.90.22替换为你VPS的IP，端口也与前面的server.conf中配置一致。 resolv-retry infinite nobind persist-key persist-tun ca ca.crt #具体名称以刚下载的为准 cert client1.crt #具体名称以刚下载的为准 key client1.key #具体名称以刚下载的为准 comp-lzo verb 3 打开 OpenVPN 客户端，连接试试看。 VPN 资料 http://www.jikexueyuan.com/course/1692_2.html?ss=1 http://gnailuy.com/linux/2011/07/04/pptp-vpn/ http://www.centoscn.com/CentosServer/test/2014/1120/4153.html https://linux.cn/article-3706-1.html http://www.bkjia.com/yjs/1041400.html http://blog.liujason.com/1663.html http://shit.name/openvpn-on-centos/ http://kunsland.github.io/blogs/2015/03/22/vps-openvpn/ http://neolee.com/web/centos-openvz-vps-configuration-openvpn/ Freeradius 服务（用于账号认证管理的工具，可以扩展到VPN的账号管理） Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/NFS.html":{"url":"Linux-Tutorial/markdown-file/NFS.html","title":"NFS（Network FileSystem）介绍","keywords":"","body":"NFS（Network FileSystem）介绍 NFS 安装 查看是否已安装： CentOS：rpm -qa | grep nfs-* Ubuntu：dpkg -l | grep nfs-* 安装： CentOS 5：sudo yum install -y nfs-utils portmap CentOS 6：sudo yum install -y nfs-utils rpcbind Ubuntu：sudo apt-get install -y nfs-common nfs-kernel-server NFS 服务器配置文件常用参数 配置文件介绍（记得先备份）：sudo vim /etc/exports 默认配置文件里面是没啥内容的，我们需要自己加上配置内容，一行表示共享一个目录。为了方便使用，共享的目录最好将权限设置为 777（chmod 777 folderName）。 假设在配置文件里面加上：/opt/mytest 192.168.0.0/55(rw,sync,all_squash,anonuid=501,anongid=501,no_subtree_check) 该配置解释： /opt/mytest 表示我们要共享的目录 192.168.0.0/55 表示内网中这个网段区间的IP是可以进行访问的，如果要任意网段都可以访问，可以用 * 号表示 (rw,sync,all_squash,anonuid=501,anongid=501,no_subtree_check) 表示权限 rw：是可读写（ro是只读） sync：同步模式，表示内存中的数据时时刻刻写入磁盘（async：非同步模式，内存中数据定期存入磁盘） all_squash：表示不管使用NFS的用户是谁，其身份都会被限定为一个指定的普通用户身份。（no_root_squash：其他客户端主机的root用户对该目录有至高权限控制。root_squash：表示其他客户端主机的root用户对该目录有普通用户权限控制） anonuid/anongid：要和root_squash或all_squash选项一同使用，表示指定使用NFS的用户被限定后的uid和gid，前提是本图片服务器的/etc/passwd中存在这一的uid和gid no_subtree_check：不检查父目录的权限 启动服务： /etc/init.d/rpcbind restart /etc/init.d/nfs-kernel-server restart NFS 客户端访问 客户端要访问服务端的共享目录需要对其共享的目录进行挂载，在挂载之前先检查下：showmount -e 192.168.1.25（这个 IP 是 NFS 的服务器端 IP） 如果显示：/opt/mytest 相关信息表示成功了。 现在开始对其进行挂载：mount -t nfs 192.168.1.25:/opt/mytest/ /mytest/ 在客户端机器上输入命令：df -h 可以看到多了一个 mytest 分区。然后我们可以再创建一个软链接，把软链接放在 war 包的目录下，这样上传的图片都会跑到另外一台服务器上了。软链接相关内容请自行搜索。 NFS 资料 http://wiki.jikexueyuan.com/project/linux/nfs.html http://www.jb51.net/os/RedHat/77993.html http://www.cnblogs.com/Charles-Zhang-Blog/archive/2013/02/05/2892879.html http://www.linuxidc.com/Linux/2013-08/89154.htm http://www.centoscn.com/image-text/config/2015/0111/4475.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/NTP.html":{"url":"Linux-Tutorial/markdown-file/NTP.html","title":"NTP（Network Time Protocol）介绍","keywords":"","body":"NTP（Network Time Protocol）介绍 NTP 安装 查看是否已安装： CentOS：rpm -qa | grep ntp-* Ubuntu：dpkg -l | grep ntp-* 安装： CentOS 6/7：sudo yum install -y ntp Ubuntu：sudo apt-get install -y ntp 配置阿里云 NTP（推荐） 官网介绍：https://help.aliyun.com/knowledge_detail/40583.html 配置文件介绍（记得先备份）：sudo vim /etc/ntp.conf 注释掉以下默认的配置内容： server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 新增配置： ntp1.aliyun.com ntp2.aliyun.com ntp3.aliyun.com ntp4.aliyun.com ntp5.aliyun.com ntp6.aliyun.com ntp7.aliyun.com CentOS 6 重启 NTP 服务：sudo service ntpd start 添加 NTP 自启动：sudo chkconfig ntpd on CentOS 7 重启 NTP 服务：sudo systemctl start ntpd.service 添加 NTP 自启动：sudo systemctl enable ntpd.service NTP 服务器配置文件常用参数 世界上可以校对时间节点：http://www.pool.ntp.org/zh 中国时间校对服务器节点：http://www.pool.ntp.org/zone/cn 配置文件介绍（记得先备份）：sudo vim /etc/ntp.conf 该配置解释： 标注 1 是默认内容，我们这里进行了注释。 标注 2 是新增内容，表示使用中国时间校对服务器节点地址。 server 0.cn.pool.ntp.org server 1.cn.pool.ntp.org server 2.cn.pool.ntp.org server 3.cn.pool.ntp.org 我的配置如下 ``` driftfile /var/lib/ntp/drift pidfile /var/run/ntpd.pid logfile /var/log/ntp.log Access Control Support restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery restrict 127.0.0.1 restrict 192.168.0.0 mask 255.255.0.0 nomodify notrap nopeer noquery restrict 172.16.0.0 mask 255.240.0.0 nomodify notrap nopeer noquery restrict 100.64.0.0 mask 255.192.0.0 nomodify notrap nopeer noquery restrict 10.0.0.0 mask 255.0.0.0 nomodify notrap nopeer noquery local clock server 127.127.1.0 fudge 127.127.1.0 stratum 10 restrict ntp1.aliyun.com nomodify notrap nopeer noquery restrict ntp1.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp10.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp11.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp12.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp2.aliyun.com nomodify notrap nopeer noquery restrict ntp2.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp3.aliyun.com nomodify notrap nopeer noquery restrict ntp3.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp4.aliyun.com nomodify notrap nopeer noquery restrict ntp4.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp5.aliyun.com nomodify notrap nopeer noquery restrict ntp5.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp6.aliyun.com nomodify notrap nopeer noquery restrict ntp6.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp7.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp8.cloud.aliyuncs.com nomodify notrap nopeer noquery restrict ntp9.cloud.aliyuncs.com nomodify notrap nopeer noquery server ntp1.aliyun.com iburst minpoll 4 maxpoll 10 server ntp1.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp10.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp11.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp12.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp2.aliyun.com iburst minpoll 4 maxpoll 10 server ntp2.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp3.aliyun.com iburst minpoll 4 maxpoll 10 server ntp3.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp4.aliyun.com iburst minpoll 4 maxpoll 10 server ntp4.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp5.aliyun.com iburst minpoll 4 maxpoll 10 server ntp5.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp6.aliyun.com iburst minpoll 4 maxpoll 10 server ntp6.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp7.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp8.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 server ntp9.cloud.aliyuncs.com iburst minpoll 4 maxpoll 10 ``` 启动服务： sudo service ntpd stop（改配置后，先停再启动） sudo service ntpd start 手动更新时间： sudo ntpdate ntp1.aliyun.com 服务加到启动项 CentOS 系统：sudo chkconfig ntpd on Ubuntu 系统 sudo apt-get install -y sysv-rc-conf sudo sysv-rc-conf ntpd on NTP 资料 http://www.jikexueyuan.com/course/1710.html http://www.pool.ntp.org/zh http://blog.kissdata.com/2014/10/28/ubuntu-ntp.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Samba.html":{"url":"Linux-Tutorial/markdown-file/Samba.html","title":"Samba 介绍","keywords":"","body":"Samba 介绍 Samba 安装 查看是否已安装： CentOS：rpm -qa | grep samba Ubuntu：dpkg -l | grep samba 安装： CentOS 6：yum install samba samba-client samba-common Ubuntu：sudo apt-get install -y samba samba-client Samba 服务器配置文件常用参数 配置文件介绍（记得先备份 cp /etc/samba/smb.conf /etc/samba/smb.conf.backup）：sudo vim /etc/samba/smb.conf 该配置解释： 在 [global] 区域 workgroup = WORKGROUP #WORKGROUP表示Windows默认的工作组名称，一般共享给windows是设置为WORKGROUP，此字段不重要，无需与 Windows 的域保持一致 security = user #ubuntu下配置文件默认没有这句,这个是自己填上去的。表示指定samba的安全等级，安全等级分别有四种：share（其他人不需要账号密码即可访问共享目录）、user（检查账号密码）、server（表示检查密码由另外一台服务器负责）、domain（指定Windows域控制服务器来验证用户的账号和密码） 注: samba 4 不再支持 security = share (查看版本 smbd --version) 在新区域区域 当 security = share 使用下面这段，这段自己添加的，其中myshare这个名字表示其他机子访问该分享地址时用：file://该服务机IP/myshare[myshare] comment = share all path = /opt/mysamba #分享的目录，其中这个目录需要chmod 777 /opt/mysamba权限 browseable = yes writable = yes public =yes 当 security = user 使用下面这段，这段自己添加的，其中 myshare2 这个名字表示其他机子访问该分享地址时用：file://该服务机IP/myshare2 可以返回的账号必须是系统已经存在的账号。先给系统添加账号：useradd user1，再用samba的设置添加账号：pdbedit -a user1，会让你设立该samba账号密码。列出账号：pdbedit -L[myshare2] comment = share for users path = /opt/mysamba2 #分享的目录，其中这个目录需要chmod 777 /opt/mysamba权限 # (不一定要 777 权限，只要登录 samba 的用户是这个目录的用户即可，那么在 Windows 中的文件创建和写入都等同于 linux 的等价账户) browseable = yes writable = yes public = no read only = no guest ok = no # samba 4 拥有的 create mask = 0646 force create mode = 0646 directory mask = 0747 force directory mode = 0747 一份成功的 samba 4 配置 [global] workgroup = WORKGROUP passdb backend = tdbsam printing = cups printcap name = cups printcap cache time = 750 cups options = raw map to guest = Bad User include = /etc/samba/dhcp.conf logon path = \\\\%L\\profiles\\.msprofile logon home = \\\\%L\\%U\\.9xprofile logon drive = P: max connections = 0 deadtime = 0 max log size = 500 [share1] path = /home/ browsable =yes writable = yes read only = no guest ok=no create mask = 0646 force create mode = 0646 directory mask = 0747 force directory mode = 0747 启动服务（CentOS 6）： sudo service samba restart service smb restart # 启动 samba 启动服务（CentOS 7）： systemctl start smb.service # 启动 samba systemctl enable smb.service # 激活 systemctl status smb.service # 查询 samba 状态（启动 samba 前后可以用查询验证） 启动服务（Ubuntu 16.04.3 -- ljoaquin提供）： sudo service smbd restart Samba 登录及验证 在 Windows 连接 Samba 之前，可在本地（linux）使用命令验证 smbclient –L //localhost/ -U 接下来输入的 password 来自于 pdbedit -a user1 命令为该用户设置的密码，不一定是 linux 用户密码 来自 /etc/samba/smb.conf 文件中的标签，如上面的例子中有 //localhost/myshare2 提示如下面，表示 Samba 服务启动成功 Domain=[xxx1] OS=[Windows 6.1] Server=[Samba 4.6.2] Sharename Type Comment --------- ---- ------- share1 Disk IPC$ IPC IPC Service (Samba 4.6.2) Domain=[xxx1] OS=[Windows 6.1] Server=[Samba 4.6.2] Server Comment --------- ------- Workgroup Master --------- ------- xxx2 xxx1 WORKGROUP xxx3 Windows 登录 打开资源管理器 -> 映射网络驱动器 -> 文件夹 填写上述 smbclient –L 命令后面加的路径 -> 弹出用户名密码对话框 -> 登录成功 Samba 登录失败 linux 防火墙 Windows 用户密码都正确，错误提示‘未知的用户名和密码。’ regedit 打开注册表，删除键值 HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Lsa 中的 LMCompatibilityLevel，无需重启计算机 Samba 资料 http://www.lvtao.net/linux/555.html https://www.centos.bz/2011/07/centos5-install-samba-windows-linux-fileshare/ https://wsgzao.github.io/post/samba/ http://linux.vbird.org/linux_server/0370samba.php https://www.liberiangeek.net/2014/07/create-configure-samba-shares-centos-7/ https://superuser.com/questions/1125438/windows-10-password-error-with-samba-share https://github.com/SeanXP/README.md/tree/master/samba http://www.apelearn.com/bbs/study/23.htm Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Crontab.html":{"url":"Linux-Tutorial/markdown-file/Crontab.html","title":"Crontab 介绍","keywords":"","body":"Crontab 介绍 Crontab 安装 查看是否已安装： CentOS：rpm -qa | grep cron Ubuntu：dpkg -l | grep cron 安装（一般系统是集成的）： CentOS 6 / 7：sudo yum install -y vixie-cron crontabs Ubuntu：sudo apt-get install -y cron 服务常用命令 CentOS 6 service crond start 启动服务 service crond stop 关闭服务 service crond restart 重启服务 CentOS 7 systemctl start crond 启动服务 systemctl restart crond 重新启动服务 systemctl status crond 加入自启动 systemctl stop crond 关闭服务 Crontab 服务器配置文件常用参数 配置文件介绍（记得先备份）：sudo vim /etc/crontab 注意：不要在配置文件里面写相对路径 该配置格式解释： 常用例子介绍： 30 21 * * * service httpd restart #每晚的 21:30 重启 apache 30 21 * * 6,0 service httpd restart #每周六、周日的 21:30 重启 apache 45 4 1,10,22 * * service httpd restart #每月的 1、10、22 日的 4:45 重启 apache 45 4 1-10 * * service httpd restart #每月的 1 到 10 日的 4:45 重启 apache */2 * * * * service httpd restart #每隔两分钟重启 apache 1-59/2 * * * * service httpd restart #每隔两分钟重启 apache（这个比较特殊：1-59/2 这个表示过掉0分，从 1 分开始算，每隔两分执行，所以 1 分执行了，3 分执行了，5 分执行了....都是奇数进行执行。默认的 */2 都是偶数执行。） * */2 * * * service httpd restart #每隔两小时重启 apache 0 23-7/2 * * * service httpd restart #晚上 11 点到早上 7 点之间，每隔 2 个小时重启 apache 0-59/30 18-23 * * * service httpd restart #每天 18:00 到 23：00 之间，每隔 30 分钟重启 apache（方法一） 0,30 18-23 * * * service httpd restart #每天 18:00 到 23：00 之间，每隔 30 分钟重启 apache（方法二） 0 4 * * sun root /opt/shell/crontab-redis-restart.sh #每周日 4:00 执行一个脚本（root 用户运行，有些脚本不指定用户会报：ERROR (getpwnam() failed） 更多例子可以看：http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html 执行记录日志：tail -f /var/log/cron（如果发现任务不执行，可以来这里盯着日志看） Crontab 权限问题 一般默认只有 root 用户可以使用 如果要指定某个用户可以使用，可以在 /etc/cron.allow 添加（不存在文件就创建一个） 如果要指定某个用户不可以使用，可以在 /etc/cron.deny 添加（不存在文件就创建一个） 如果一个用户同时在两个文件都存在，那则以 allow 为准 Crontab 不执行 Crontab 不执行原因有很多，可以 Google 搜索：Crontab 不执行，这里不多说。 Crontab 资料 http://www.imooc.com/video/4498 http://www.centoscn.com/image-text/config/2015/0901/6096.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Iptables.html":{"url":"Linux-Tutorial/markdown-file/Iptables.html","title":"Iptables 介绍","keywords":"","body":"Iptables 介绍 前提说明 iptables 的设置在 CentOS 和 Ubuntu 下有些细节不一样，Ubuntu 这里不讲，文章底下贴的资料有部分关于 Ubuntu 的，有需要的可以自己看。一般大家会用到 iptables 都是服务器，而一般服务器大家普遍是用 CentOS） Iptables 安装 查看是否已安装： CentOS：rpm -qa | grep iptables 安装（一般系统是集成的）： CentOS 6：yum install -y iptables Iptables 的配置文件 路径：vim /etc/sysconfig/iptables Iptables 服务器配置文件常用参数 常用命令： 查看已有规则列表，并且显示编号：iptables -L -n --line-numbers 要删除 INPUT 里序号为 8 的规则，执行：iptables -D INPUT 8 保存配置命令：service iptables save 或者 /etc/rc.d/init.d/iptables save 重启服务命令 ：service iptables restart 查看服务状态： service iptables status 设置开启默认启动： chkconfig --level 345 iptables on 清除所有规则(慎用) iptables -F iptables -X iptables -Z 添加规则：格式 iptables [-AI 链名] [-io 网络接口] [-p 协议] [-s 来源IP/网域] [-d 目标IP/网域] -j [ACCEPT|DROP|REJECT|LOG] 选项与参数： -AI 链名：针对某的链进行规则的 \"插入\" 或 \"累加\" -A ：新增加一条规则，该规则增加在原本规则的最后面。例如原本已经有四条规则，使用 -A 就可以加上第五条规则！ -I ：插入一条规则。如果没有指定此规则的顺序，默认是插入变成第一条规则。例如原本有四条规则，使用 -I 则该规则变成第一条，而原本四条变成 2~5 号链 ：有 INPUT, OUTPUT, FORWARD 等，此链名称又与 -io 有关，请看底下。 -io 网络接口：设定封包进出的接口规范 -i ：封包所进入的那个网络接口，例如 eth0, lo 等接口。需与 INPUT 链配合； -o ：封包所传出的那个网络接口，需与 OUTPUT 链配合； -p 协定：设定此规则适用于哪种封包格式。主要的封包格式有： tcp, udp, icmp 及 all 。 -s 来源 IP/网域：设定此规则之封包的来源项目，可指定单纯的 IP 或包括网域，例如：IP：192.168.0.100，网域：192.168.0.0/24, 192.168.0.0/255.255.255.0 均可。若规范为『不许』时，则加上 ! 即可，例如：-s ! 192.168.100.0/24 表示不许 192.168.100.0/24 之封包来源。 -d 目标 IP/网域：同 -s ，只不过这里指的是目标的 IP 或网域。 -j ：后面接动作，主要的动作有接受(ACCEPT)、丢弃(DROP)、拒绝(REJECT)及记录(LOG) Iptables 例子 开放指定端口 iptables -I INPUT -i lo -j ACCEPT #允许本地回环接口(即运行本机访问本机) iptables -I INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 允许已建立的或相关连的通行 iptables -I OUTPUT -j ACCEPT #允许所有本机向外的访问 iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT # 允许访问 22 端口 iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT #允许访问 80 端口 iptables -A INPUT -p tcp -m tcp --dport 8080 -j ACCEPT #允许访问 8080 端口 iptables -A INPUT -p tcp -m tcp --dport 21 -j ACCEPT #允许 FTP 服务的 21 端口 iptables -A INPUT -p tcp -m tcp --dport 20 -j ACCEPT #允许 FTP 服务的 20 端口 iptables -I INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT #允许 ping iptables -I INPUT -j REJECT #禁止其他未允许的规则访问（使用该规则前一定要保证 22 端口是开着，不然就连 SSH 都会连不上） iptables -I FORWARD -j REJECT Iptables 资料 https://wsgzao.github.io/post/iptables/ http://www.vpser.net/security/linux-iptables.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Hsk-Install.html":{"url":"Linux-Tutorial/markdown-file/Hsk-Install.html","title":"花生壳-安装介绍","keywords":"","body":"花生壳 安装 CentOS 下过程 官网：http://hsk.oray.com/ 官网下载：http://hsk.oray.com/download/#type=linux 官网安装说明：http://service.oray.com/question/1890.html 软件包下载：wget http://download.oray.com/peanuthull/linux/phddns-2.0.6.el6.x86_64.rpm 安装：sudo yum localinstall -y phddns-2.0.6.el6.x86_64.rpm 配置： 安装完毕后，在终端下运行：phddns 第一步：：Enter server address(press ENTER use phddns60.oray.net)，这是提示您输入花生壳服务器的域名，如果网站上没有更新域名的公告说明，这一步直接回车即可，会使用默认的 phddns60.oray.net 域名。 第二步：Enter your Oray account:这是提示您输入在花生壳官网注册的用户名，请根据实际情况输入。 第三步：Password：这是提示您输入在花生壳官网注册的用户名所对应的密码，请根据实际情况输入。 第四步：Network interface(s): 这是要配置您这台服务器的网络参数，花生壳（公网版）软件会自动检查，并输出您的网络情况。eth0部分可能和上面的不一样，是您的实际网络设置。如果您有两块网卡，eth0 和eth1 ,而您希望用eth1来绑定花生壳，请在这里输入 eth1 ，然后回车。如果您只有一块网卡，或者您希望使用 eth0来绑定花生壳，在这里直接回车即可。 第五步：Log to use(default /var/log/phddns.log):这是提示您输入花生壳（公网版）软件日志的保存位置，请使用绝对路径指定日志文件名。如果直接回车，会使用 /var/log/phddns.log 来保存日志。 第六步： Save to configuration file (/etc/phlinux.conf)?(yes/no/other): 这是提示您输入上述配置的保存文件名。如果输入yes 或直接回车，将会使用/etc/phlinux.conf 来作为配置的保存文件名。如果输入other ，将会提示您自行指定文件名，请使用绝对路径来指定这个配置文件名。如果输入no ,不对上述配置进行保存,下次重新使用花生壳（公网版）时,需要手动指定配置文件或再次通过交互模式进行设置. 第六步执行完毕后，屏幕上会依次出现：defOnStatusChanged ok，DomainsRegistered，UserType，表示安装、配置完成了 启动服务：/usr/bin/phddns -c /etc/phlinux.conf -d 设置开机启动：echo \"/usr/bin/phddns -c /etc/phlinux.conf -d\" >> /etc/rc.local 查看进程：ps -ef | grep phddns 卸载： rpm -qa|grep phddns sudo rpm -e phddns-2.0.6-1.el6.x86_64 资料 http://service.oray.com/question/1890.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/JDK-Install.html":{"url":"Linux-Tutorial/markdown-file/JDK-Install.html","title":"JDK 安装","keywords":"","body":"JDK 安装 CentOS 下过程 JDK 在 CentOS 和 Ubuntu 下安装过程是一样的，所以这里不再讲 Ubuntu 系统下的安装 JDK 1.8 下载 此时（20160205）最新版本：jdk-8u72-linux-x64.tar.gz 官网：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 百度云下载（64 位）：http://pan.baidu.com/s/1eQZffbW 官网压缩包地址：http://211.138.156.198:82/1Q2W3E4R5T6Y7U8I9O0P1Z2X3C4V5B/download.oracle.com/otn-pub/java/jdk/8u72-b15/jdk-8u72-linux-x64.tar.gz 在命令行模式下下载上面压缩包： cd /opt sudo wget http://211.138.156.198:82/1Q2W3E4R5T6Y7U8I9O0P1Z2X3C4V5B/download.oracle.com/otn-pub/java/jdk/8u72-b15/jdk-8u72-linux-x64.tar.gz 默认 CentOS 有安装 openJDK，建议先卸载掉 检查 JDK 命令：java -version 查询本地 JDK 安装程序情况； rpm -qa|grep java 我查询出来的结果如下： java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el6_7.x86_64 java-1.7.0-openjdk-1.7.0.95-2.6.4.0.el6_7.x86_64 tzdata-java-2015g-2.el6.noarch 卸载上面三个文件（--nodeps 的作用：忽略依赖的检查）： sudo rpm -e --nodeps java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el6_7.x86_64 sudo rpm -e --nodeps java-1.7.0-openjdk-1.7.0.95-2.6.4.0.el6_7.x86_64 sudo rpm -e --nodeps tzdata-java-2015g-2.el6.noarch 也可以一起卸载：sudo rpm -e --nodeps java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el6_7.x86_64 java-1.7.0-openjdk-1.7.0.95-2.6.4.0.el6_7.x86_64 tzdata-java-2015g-2.el6.noarch 如果是 CentOS 7 的话：sudo rpm -e --nodeps javapackages-tools-3.4.1-11.el7.noarch java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64 java-1.7.0-openjdk-headless-1.7.0.131-2.6.9.0.el7_3.x86_64 python-javapackages-3.4.1-11.el7.noarch java-1.7.0-openjdk-1.7.0.131-2.6.9.0.el7_3.x86_64 java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64 tzdata-java-2017a-1.el7.noarch JDK 1.8 安装 我们以安装 jdk-8u72-linux-x64.tar.gz 为例 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 解压安装包：sudo tar -zxvf jdk-8u72-linux-x64.tar.gz 移到解压包到我个人习惯的安装目录下：mv jdk1.8.0_72/ /usr/program/ 配置环境变量： 编辑配置文件：sudo vim /etc/profile 在该文件的最尾巴，添加下面内容：# JDK JAVA_HOME=/usr/program/jdk1.8.0_72 JRE_HOME=$JAVA_HOME/jre PATH=$PATH:$JAVA_HOME/bin CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JAVA_HOME export JRE_HOME export PATH export CLASSPATH 执行命令，刷新该配置（必备操作）：source /etc/profile 检查是否使用了最新的 JDK：java -version 其他 JDK 历史版本下载地址整理（不间断更新）： JDK 9：https://jdk9.java.net/download/ JDK 8：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html JDK 7：http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html JDK 7 - 64 位：wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz JDK 7 - 32 位：wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-i586.tar.gz JDK 6：http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-archive-downloads-javase6-419409.html 资料 http://www.jikexueyuan.com/course/480_1.html?ss=1 <> <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Java-bin.html":{"url":"Linux-Tutorial/markdown-file/Java-bin.html","title":"Java bin 目录下的工具","keywords":"","body":"Java bin 目录下的工具 JVM 内存结构 参考资料：JVM内存结构（基于JDK8） 运行时数据区（JVM 规范） VM 栈（JVM 虚拟机栈） 是线程私有的，它的生命周期和线程相同。它描述的是 Java 方法执行的内存模式。 Java 堆区（Heap） 是 Java 虚拟机所管理的内存中最大的一块。是被所有线程共享的一块内存区域，在虚拟机启动时候创建。用于存放对象实例。 方法区（Method Area） 也是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 虽然在 JVM 规范上是描述为堆的一个逻辑部分，但是它有一个别名：Non-Heap（非堆），独立于堆区之外的。JDK8 它是：Metaspace 区 Metaspace：主要存放：Class、Package、Method、Field、字节码、常量池、符号引用等等 方法区里面有一个：运行时常量池（Run-Time Constant Pool），用于存放编译期生成的各种字面量和符号应用，在类加载后进入该池存放。 本地方法栈（Native Method Stacks） 与虚拟机栈所发挥的作用类似，之间的区别： 虚拟机栈是为虚拟机执行 Java 方法（也就是字节码）服务 本地方法栈是为了虚拟机使用到 Native 方法服务。 JDK8 真实内存结构（HotSpot） HotSpot--Java HotSpot Performance Engine，是 Java 虚拟机的一个实现，目前是 Oracle 在维护和发布。 JDK8 HotSpot 的堆内存区域结构 组成：Eden + Surviver（S0 + S1） + Old 对象生命周期：Eden > Surviver（S0 + S1） > Old Eden:该区域是最主要的刚创建的对象的内存分配区域，绝大多数对象都会被创建到这里（除了部分大对象通过内存担保机制创建到Old区域，默认大对象都是能够存活较长时间的），该区域的对象大部分都是短时间都会死亡的，故垃圾回收器针对该部分主要采用标记整理算法了回收该区域。 Surviver:该区域也是属于新生代的区域，该区域是将在Eden中未被清理的对象存放到该区域中，该区域分为两块区域，采用的是复制算法，每次只使用一块，Eden与Surviver区域的比例是8:1，是根据大量的业务运行总结出来的规律。 Old:该区域是属于老年代，一般能够在Surviver中没有被清除出去的对象才会进入到这块区域，该区域主要是采用标记清除算法。 总结：java堆的垃圾回收是垃圾回收器最主要的光顾对象，整体采用分代收集的策略，对不同区域结合其特点采用不同的垃圾收集算法。我们在编程中也应该关注这一块区域，尽量不适用大对象，尽可能的创建局部对象，使用过后确定废弃不用的对象及时断开引用，尽量避免使用循环的对象引用（可达性分析也是比较消耗资源的）等等。 JVM内存区域的详解图 更多这类文章 从实际案例聊聊Java应用的GC优化 频繁GC问题或内存溢出排查流程 使用 jps，查看线程ID，假设 PID 为 12011 使用 jstat -gc PID 250 20，查看gc情况，一般比较关注PERM区的情况，查看GC的增长情况。 使用 jstat -gccause PID：额外输出上次GC原因 使用 jmap -dump:format=b,file=/opt/myHeapDumpFileName 12011，生成堆转储文件 使用 jhat 或者可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）分析堆情况。 结合代码解决内存溢出或泄露问题。 死锁问题 使用 jps查看线程ID，假设 PID 为 12011 使用 jstack 12011 查看线程情况 jps 显示当前所有 java 进程 pid 的命令 16470 Jps 12011 Bootstrap jps -v 跟：ps -ef|grep java 主要输出内容一样 12011 是我这边的一个 java 应用的 pid，下面的其他命令都是自己与此应用进行分析的 jstat（重要） 显示进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 查看类加载信息：jstat -class PID 垃圾回收统计 jstat -gc PID 250 10，每250毫秒查询一次，一共查询10次。 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 34944.0 34944.0 1006.5 0.0 279616.0 235729.8 699072.0 12407.5 20736.0 20145.5 2560.0 2411.8 6 0.392 0 0.000 0.392 列含义说明： 34944.0 表示 34M 大小，235729.8 表示 235M SO + S1 + Eden = young 区 -S0C 年轻代中第一个survivor（幸存区）的容量 (字节) -S1C 年轻代中第二个survivor（幸存区）的容量 (字节) -S0U 年轻代中第一个survivor（幸存区）目前已使用空间 (字节) （字母 U 表示 used） -S1U 年轻代中第二个survivor（幸存区）目前已使用空间 (字节) （字母 U 表示 used） -EC 年轻代中Eden（伊甸园）的容量 (字节) -EU 年轻代中Eden（伊甸园）目前已使用空间 (字节) OC + OU = old 区 -OC Old代的容量 (字节) -OU Old代目前已使用空间 (字节) MC + MU = Metaspace 区 MC 方法区大小 MU 方法区使用大小 其他 CCSC 压缩类空间大小 CCSU 压缩类空间使用大小 YGC 年轻代垃圾回收次数 YGCT 年轻代垃圾回收消耗时间 FGC 老年代垃圾回收次数 FGCT 老年代垃圾回收消耗时间 GCT 垃圾回收消耗总时间 堆内存统计 jstat -gccapacity 12011 250 10，查询进程 12011 VM内存中三代（young,old,perm）对象的使用和占用大小，每250毫秒查询一次，一共查询10次。 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 349504.0 1398080.0 349504.0 34944.0 34944.0 279616.0 699072.0 2796224.0 699072.0 699072.0 0.0 1067008.0 20736.0 0.0 1048576.0 2560.0 6 0 列含义说明： NGCMN 年轻代(young)中初始化(最小)的大小(字节) NGCMX 年轻代(young)的最大容量 (字节) NGC 年轻代(young)中当前的容量 (字节) S0C 年轻代中第一个survivor（幸存区）的容量 (字节) S1C 年轻代中第二个survivor（幸存区）的容量 (字节) EC 年轻代中Eden（伊甸园）的容量 (字节) OGCMN old代中初始化(最小)的大小 (字节) OGCMX old代的最大容量(字节) OGC old代当前新生成的容量 (字节) OC Old代的容量 (字节) MCMN 最小元数据容量 MCMX 最大元数据容量 MC 当前元数据空间大小 CCSMN 最小压缩类空间大小 CCSMX 最大压缩类空间大小 CCSC 当前压缩类空间大小 YGC 年轻代gc次数，从应用程序启动到采样时年轻代中gc次数 FGC 老年代GC次数，从应用程序启动到采样时old代(全gc = Full gc次数)gc次数 更多其他参数的使用可以看： Java命令学习系列（四）——jstat java高分局之jstat命令使用 jstat命令查看jvm的GC情况 （以Linux为例） gcutil 使用：jstat -gcutil PID 3000 10： 正常情况结果应该是这样的： S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 0.00 67.63 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.68 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.71 38.09 78.03 68.82 124 0.966 5 0.778 1.744 0.00 0.00 67.71 38.09 78.03 68.82 124 0.966 5 0.778 1.744 S0：SO 当前使用比例 S1：S1 当前使用比例 E：Eden 区使用比例（百分比）（异常的时候，这里可能会接近 100%） O：old 区使用比例（百分比）（异常的时候，这里可能会接近 100%） M：Metaspace 区使用比例（百分比）（异常的时候，这里可能会接近 100%） CCS：压缩使用比例 YGC：年轻代垃圾回收次数 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间（Full gc耗时）（单位秒） GCT：垃圾回收消耗总时间（单位秒） 异常的时候每次 Full GC 时间也可能非常长，每次时间计算公式=FGCT值/FGC指） 在 YGC 之前 年轻代 = eden + S1；YGC 之后，年轻代 = eden + S0。 如果看到 YGC 之后 old 区空间没变，表示此次 YGC，没有对象晋升到 old 区 jmap 生成堆转储快照（heap dump） heap dump 主要记录了在某一时刻JVM堆中对象使用的情况，即某个时刻JVM堆的快照，是一个二进制文件，主要用于分析哪些对象占用了太对的堆空间，从而发现导致内存泄漏的对象。 堆Dump是反应Java堆使用情况的内存镜像，其中主要包括系统信息、虚拟机属性、完整的线程Dump、所有类和对象的状态等。 一般，在内存不足、GC异常等情况下，我们就会怀疑有内存泄露。这个时候我们就可以制作堆Dump来查看具体情况，分析原因。 常见内存错误： outOfMemoryError 年老代内存不足。 outOfMemoryError:PermGen Space 永久代内存不足。 outOfMemoryError:GC overhead limit exceed 垃圾回收时间占用系统运行时间的98%或以上。 jmap -heap 12011，查看指定进程堆（heap）使用情况 Attaching to process ID 12011, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.151-b12 using thread-local object allocation. Mark Sweep Compact GC Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 4294967296 (4096.0MB) NewSize = 357892096 (341.3125MB) MaxNewSize = 1431633920 (1365.3125MB) OldSize = 715849728 (682.6875MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: New Generation (Eden + 1 Survivor Space): capacity = 322109440 (307.1875MB) used = 242418024 (231.1878433227539MB) free = 79691416 (75.9996566772461MB) 75.2595217327378% used Eden Space: capacity = 286326784 (273.0625MB) used = 241387328 (230.20489501953125MB) free = 44939456 (42.85760498046875MB) 84.30483681191348% used From Space: capacity = 35782656 (34.125MB) used = 1030696 (0.9829483032226562MB) free = 34751960 (33.142051696777344MB) 2.88043458819826% used To Space: capacity = 35782656 (34.125MB) used = 0 (0.0MB) free = 35782656 (34.125MB) 0.0% used tenured generation: capacity = 715849728 (682.6875MB) used = 12705280 (12.11669921875MB) free = 703144448 (670.57080078125MB) 1.774852947908084% used 7067 interned Strings occupying 596016 bytes. jmap -histo 12011，查看堆内存(histogram)中的对象数量及大小（下面 demo 内容太多，所以选取其中一部分） jmap -histo:live 12011，查看堆内存(histogram)中的对象数量及大小，但是JVM会先触发gc，然后再统计信息 jmap -dump:format=b,file=/opt/myHeapDumpFileName 12011，将内存使用的详细情况输出到文件，之后一般使用其他工具进行分析。 生成的文件可以用一些可视化工具（Eclipse Memory Analyzer 、IBM HeapAnalyzer）来查看 编号 个数 字节 类名 508: 6 192 java.lang.invoke.LambdaForm$BasicType 509: 8 192 java.lang.invoke.MethodHandleImpl$Intrinsic 510: 8 192 java.math.RoundingMode 511: 6 192 java.net.NetworkInterface$1checkedAddresses 512: 6 192 java.rmi.server.UID 513: 3 192 java.text.DateFormatSymbols 514: 8 192 java.util.Formatter$FixedString 515: 6 192 java.util.TreeMap$KeyIterator 516: 8 192 java.util.regex.Pattern$Slice 517: 8 192 jdk.net.SocketFlow$Status 518: 6 192 net.sf.ehcache.DefaultElementEvictionData 519: 3 192 net.sf.ehcache.store.chm.SelectableConcurrentHashMap 520: 8 192 org.apache.logging.log4j.Level 521: 8 192 org.apache.logging.log4j.core.appender.rolling.RolloverFrequency 522: 4 192 org.apache.logging.log4j.core.impl.ThrowableProxy 523: 3 192 org.apache.logging.log4j.core.layout.PatternLayout 524: 12 192 org.apache.logging.log4j.core.util.datetime.FastDateParser$NumberStrategy 525: 3 192 org.apache.logging.log4j.core.util.datetime.FixedDateFormat 526: 8 192 org.apache.logging.log4j.spi.StandardLevel 527: 2 192 sun.nio.ch.ServerSocketChannelImpl 528: 4 192 sun.nio.cs.StreamEncoder 529: 6 192 sun.reflect.generics.reflectiveObjects.TypeVariableImpl 530: 11 176 java.text.NumberFormat$Field 531: 11 176 java.util.concurrent.ConcurrentSkipListSet 532: 2 176 javax.management.remote.rmi.NoCallStackClassLoader 533: 11 176 org.apache.logging.log4j.core.lookup.MapLookup 534: 8 168 [Ljava.lang.reflect.TypeVariable; 535: 1 168 [[Ljava.math.BigInteger; jstack（线程快照 -- CPU 负载高） jstack命令主要用来查看Java线程的调用堆栈的，可以用来分析线程问题（如死锁） jstack用于生成java虚拟机当前时刻的 线程快照（thread dump）。主要记录JVM在某一时刻各个线程执行的情况，以栈的形式显示，是一个文本文件。 线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。 另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack 12011，查看线程情况 jstack -l 12011，除堆栈外，显示关于锁的附件信息 导出文件：jstack -l PID >> /opt/jstack-tomcat1-20180917.log 把占用 CPU 资源高的线程十进制的 PID 转换成 16 进制：printf \"%x\\n\" PID，比如：printf \"%x\\n\" 12401 得到结果是：3071 在刚刚输出的那个 log 文件中搜索：3071，可以找到：nid=0x3071 在线看某个线程 PID 的情况：jstack 进程ID | grep 十六进制线程ID -A 10 -A 10 参数用来指定显示行数，否则只会显示一行信息 下面 demo 内容太多，所以选取其中一部分结构： 常见线程状态 Runnable：正在运行的线程 Sleeping：休眠的线程 Waiting：等待的线程 2018-03-08 14:28:13 Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode): \"Attach Listener\" #53 daemon prio=9 os_prio=0 tid=0x00007f8a34009000 nid=0x865 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Log4j2-AsyncLoggerConfig-1\" #16 daemon prio=5 os_prio=0 tid=0x00007f8a5c48d800 nid=0x2f0c waiting on condition [0x00007f8a4cbfe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at com.lmax.disruptor.BlockingWaitStrategy.waitFor(BlockingWaitStrategy.java:45) at com.lmax.disruptor.ProcessingSequenceBarrier.waitFor(ProcessingSequenceBarrier.java:56) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:124) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) \"Wrapper-Control-Event-Monitor\" #13 daemon prio=5 os_prio=0 tid=0x00007f8a5c34e000 nid=0x2efc waiting on condition [0x00007f8a60314000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.tanukisoftware.wrapper.WrapperManager$3.run(WrapperManager.java:731) \"RMI TCP Accept-0\" #11 daemon prio=5 os_prio=0 tid=0x00007f8a5c32f800 nid=0x2efa runnable [0x00007f8a60619000] java.lang.Thread.State: RUNNABLE at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:52) at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400) at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372) at java.lang.Thread.run(Thread.java:748) \"Service Thread\" #7 daemon prio=9 os_prio=0 tid=0x00007f8a5c0b4800 nid=0x2ef3 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"C1 CompilerThread1\" #6 daemon prio=9 os_prio=0 tid=0x00007f8a5c0b1800 nid=0x2ef2 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"C2 CompilerThread0\" #5 daemon prio=9 os_prio=0 tid=0x00007f8a5c0af800 nid=0x2ef1 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Signal Dispatcher\" #4 daemon prio=9 os_prio=0 tid=0x00007f8a5c0aa800 nid=0x2ef0 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \"Finalizer\" #3 daemon prio=8 os_prio=0 tid=0x00007f8a5c07b000 nid=0x2eef in Object.wait() [0x00007f8a614f4000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) - locked (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209) \"VM Thread\" os_prio=0 tid=0x00007f8a5c06e800 nid=0x2eed runnable \"VM Periodic Task Thread\" os_prio=0 tid=0x00007f8a5c332000 nid=0x2efb waiting on condition JNI global references: 281 资料 https://juejin.im/entry/5a9220f85188257a856f5d6e https://www.javatang.com/archives/2017/10/19/33151873.html https://juejin.im/post/5a9b811a6fb9a028e46e1c88 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/SVN-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/SVN-Install-And-Settings.html","title":"SVN 安装和配置","keywords":"","body":"Subversion 1.8 安装 RPM 安装（推荐） wandisco 整理的 RPM 文件官网：http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/ 下载下面几个 RPM 文件： 创建目录来保存下载的 RPM：sudo mkdir -p /opt/setups/subversion/ ; cd /opt/setups/subversion/ wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/mod_dav_svn-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/serf-1.3.7-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-gnome-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-javahl-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-perl-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-python-1.8.15-1.x86_64.rpm wget http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/x86_64/subversion-tools-1.8.15-1.x86_64.rpm 如果上面的 RPM 链接失效，你也可以考虑下载我提供的百度云盘地址：http://pan.baidu.com/s/1pKnGia3 安装下载的 RPM 文件： sudo rpm -ivh *.rpm 检查安装后的版本： svn --version 编译安装（不推荐） subversion 1.8 编译安装（本人没有尝试成功，所以不推荐，下面内容只供参考） 官网安装说明（查找关键字 Dependency Overview）：http://svn.apache.org/repos/asf/subversion/trunk/INSTALL 此时 1.8 最新版本为：subversion-1.8.15.tar.gz 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 安装编译所需工具： sudo yum install -y gcc gcc-c++ autoconf libtool 所需依赖包说明： （必要包）apr 和 apr-util 官网地址：http://archive.apache.org/dist/apr/ （必要包）zlib 官网地址：ttp://www.zlib.net/ （必要包）SQLite 官网地址：http://www.sqlite.org/download.html （必要包）Subversion 官网地址：https://subversion.apache.org/download.cgi 所需依赖包下载： apr 下载：wget http://archive.apache.org/dist/apr/apr-1.5.2.tar.gz apr-util 下载：wget http://archive.apache.org/dist/apr/apr-util-1.5.4.tar.gz zlib 下载：wget http://zlib.net/zlib-1.2.8.tar.gz SQLite 下载：wget http://www.sqlite.org/2016/sqlite-amalgamation-3100200.zip Subversion 下载：wget http://apache.fayea.com/subversion/subversion-1.8.15.tar.gz 安装依赖包： apr 安装： 解压：tar -zxvf apr-1.5.2.tar.gz 移动到我个人习惯的安装目录下：mv apr-1.5.2/ /usr/program/ 标准的 GNU 源码安装方式： cd /usr/program/apr-1.5.2 ./configure make make install 安装完得到安装的配置路径：/usr/local/apr/bin/apr-1-config，这个需要记下来，下面会用到 apr-util 安装： 解压：tar -zxvf apr-util-1.5.4.tar.gz 移动到我个人习惯的安装目录下：mv apr-util-1.5.4/ /usr/program/ 标准的 GNU 源码安装方式： cd /usr/program/apr-util-1.5.4/ ./configure --with-apr=/usr/local/apr/bin/apr-1-config make make install 安装完得到安装的配置路径：/usr/local/apr/bin/apu-1-config，这个需要记下来，下面会用到 zlib 安装： 解压：tar -zxvf zlib-1.2.8.tar.gz 移动到我个人习惯的安装目录下：mv zlib-1.2.8/ /usr/program/ 标准的 GNU 源码安装方式： cd /usr/program/zlib-1.2.8/ ./configure make make install Subversion 解压： 解压：tar -zxvf subversion-1.8.15.tar.gz 移动到我个人习惯的安装目录下：mv subversion-1.8.15/ /usr/program/ SQLite 安装： 解压：unzip sqlite-amalgamation-3100200.zip 移动到 subversion 目录下：mv sqlite-amalgamation-3100200/ /usr/program/subversion-1.8.15/ Subversion 安装： 标准的 GNU 源码安装方式： cd /usr/program/subversion-1.8.15/ ./configure --prefix=/usr/local/subversion --with-apr=/usr/local/apr/bin/apr-1-config --with-apr-util=/usr/local/apr/bin/apu-1-config make make install SVN 配置 在系统上创建一个目录用来存储所有的 SVN 文件：mkdir -p /opt/svn/repo/ 新建一个版本仓库：svnadmin create /opt/svn/repo/ 生成如下目录和文件： 目录：locks 目录：hooks 目录：db 目录：conf 文件：format 文件：README.txt 其中，目录 conf 最为重要，常用的配置文件都在里面 svnserve.conf 是 svn 服务综合配置文件 passwd 是用户名和密码配置文件 authz 是权限配置文件 设置配置文件 编辑配置文件：vim /opt/svn/repo/conf/svnserve.conf 配置文件中下面几个参数（默认是注释的）： anon-access： 对不在授权名单中的用户访问仓库的权限控制，有三个可选性：write、read、none none 表示没有任何权限 read 表示只有只读权限 write 表示有读写权限 auth-access：对在授权名单中的用户访问仓库的权限控制，有三个可选性：write、read、none none 表示没有任何权限 read 表示只有只读权限 write 表示有读写权限 password-db：指定用户数据配置文件 authz-db：指定用户权限配置文件 realm：指定版本库的认证域，即在登录时提示的认证域名称。若两个版本库的认证域相同，建议使用相同的用户名口令数据文件 当前实例的配置内容：realm = myrepo 添加用户 编辑配置文件：vim /opt/svn/repo/conf/passwd 添加用户很简答，如上图所示在配置文中添加一个格式为：用户名 = 密码 的即可 设置用户权限 编辑配置文件：vim /opt/svn/repo/conf/authz 配置文件中几个参数解释： r 表示可写 w 表示可读 rw 表示可读可写 * = 表示除了上面设置的权限用户组以外，其他所有用户都设置空权限，空权限表示禁止访问本目录，这很重要一定要加上 [groups] 表示下面创建的是用户组，实际应用中一般我们对使用者都是进行分组的，然后把权限控制在组上，这样比较方便。使用组权限方式：@组名 = rw 启动服务 svnserve -d -r /opt/svn/repo/ --listen-port 3690 -d 表示后台运行 -r /opt/svn/repo/ 表示指定根目录 --listen-port 3690 表示指定端口，默认就是 3690，所以如果要用默认端口这个也是可以省略掉的 停止服务 killall svnserve 测试 iptables 处理 一种方式：先关闭 iptables，防止出现拦截问题而测试不了：service iptables stop 一种方式：在 iptables 中添加允许规则（svn 默认端口是 3690）： 添加规则：sudo iptables -I INPUT -p tcp -m tcp --dport 3690 -j ACCEPT 保存规则：sudo /etc/rc.d/init.d/iptables save 重启 iptables：sudo service iptables restart 在 Windows 的 svn 客户端上访问:svn://192.168.0.110 SVN 设置提交之后可修改提交的 Message 信息 默认的 SVN 是无法修改提交后的 Message 信息的，修改会报如下错误： 解决办法： 下载我 hooks 文件：http://pan.baidu.com/s/1c1jtlmw 把 pre-revprop-change 文件放在你的仓库下，比如我仓库地址是：/opt/svn/repo/hooks 编辑该文件：vim /opt/svn/repo/hooks/pre-revprop-change 把文件尾巴的这句脚本：echo \"$1 $2 $3 $4 $5\" >> /opt/svn/repo/logchanges.log，改为：echo \"$1 $2 $3 $4 $5\" >> /你的仓库地址/logchanges.log 你在该目录下也可以看到一个文件 pre-revprop-change.tmpl，这个其实就是 svn 提供给你模板，其他的那些你有兴趣也可以研究下 资料 http://tecadmin.net/install-subversion-1-8-on-centos-rhel/ http://svn.apache.org/repos/asf/subversion/trunk/INSTALL http://chenpipi.blog.51cto.com/8563610/1613007 https://blog.linuxeye.com/348.html http://jingyan.baidu.com/article/046a7b3efb6a5df9c27fa991.html http://www.ha97.com/4467.html http://blog.feehi.com/linux/7.html http://my.oschina.net/lionel45/blog/298305?fromerr=1NdIndN0 http://www.centoscn.com/CentosServer/ftp/2015/0622/5708.html http://blog.csdn.net/tianlesoftware/article/details/6119231 http://www.scmeye.com/thread-419-1-1.html http://m.blog.csdn.net/article/details?id=7908907 设置可编辑提交信息：http://stackoverflow.com/questions/692851/can-i-go-back-and-edit-comments-on-an-svn-checkin Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Tomcat-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Tomcat-Install-And-Settings.html","title":"Tomcat 安装和配置、优化","keywords":"","body":"Tomcat 8 安装和配置、优化 Tomcat 8 安装 Tomcat 8 安装 官网：http://tomcat.apache.org/ Tomcat 8 官网下载：http://tomcat.apache.org/download-80.cgi 此时（20160207） Tomcat 8 最新版本为：apache-tomcat-8.0.32.tar.gz 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 Tomcat 8 下载（201706 更新：旧版本失效）：wget http://apache.fayea.com/tomcat/tomcat-8/v8.0.44/bin/apache-tomcat-8.0.44.tar.gz 压缩包解压：tar -zxvf apache-tomcat-8.0.32.tar.gz 移到解压出来文件夹到 /usr 下：mv apache-tomcat-8.0.32/ /usr/program/ 为了方便，修改解压目录的名字：mv /usr/program/apache-tomcat-8.0.32/ /usr/program/tomcat8/ 设置 Iptables 规则（这一步是必须设置的）： 一种方式：先关闭 iptables，防止出现拦截问题而测试不了：service iptables stop 一种方式：在 iptables 中添加允许规则（Tomcat 默认端口是 8080）： 添加规则：iptables -I INPUT -p tcp -m tcp --dport 8080 -j ACCEPT 保存规则：service iptables save 重启 iptables：service iptables restart 测试安装好后的 Tomcat： 启动 Tomcat：sh /usr/program/tomcat8/bin/startup.sh ; tail -200f /usr/program/tomcat8/logs/catalina.out 访问：http://服务器 IP 地址:8080/ 停止 Tomcat：sh /usr/program/tomcat8/bin/shutdown.sh 如果启动报：java.net.UnknownHostException 或 localhost. This prevents creation of a GUID 解决办法： 修改配置文件：vim /etc/sysconfig/network，把里面的 HOSTNAME 改为你自己设置的一个名字，比如我这边改为：youmeek 修改配置文件：vim /etc/hosts，把第一行的 127.0.0.1 这一行删掉，然后再最后一行增加这句：127.0.0.1 localhost youmeek，这里最后的 youmeek 就是主机名 然后重启计算机：reboot 如果启动报：java.net.BindException: 地址已在使用 / java.net.BindException: Address already in use 解决办法：这表示端口被占用，修改 Tomcat 的 server.xml 配置文件，把端口改了即可。 Tomcat 8 配置 设置 Tomcat 相关变量： vim /usr/program/tomcat8/bin/catalina.sh 在配置文件的可编辑内容最上面（98 行开始），加上如下内容（具体参数根据你服务器情况自行修改）：JAVA_HOME=/usr/program/jdk1.8.0_72 CATALINA_HOME=/usr/program/tomcat8 CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms2048m -Xmx2048m -Xmn1024m -XX:PermSize=256m -XX:MaxPermSize=512m -XX:SurvivorRatio=10 -XX:MaxTenuringThreshold=15 -XX:NewRatio=2 -XX:+DisableExplicitGC\" CATALINA_PID=$CATALINA_HOME/catalina.pid 如果使用 shutdown.sh 还无法停止 tomcat，可以修改其配置：vim /usr/program/tomcat8/bin/shutdown.sh 把最尾巴这一行：exec \"$PRGDIR\"/\"$EXECUTABLE\" stop \"$@\" 改为：exec \"$PRGDIR\"/\"$EXECUTABLE\" stop 10 -force Tomcat 8 优化 Tomcat 6/7/8 的优化参数有点不一样，最好按下面的方式看一下官网这个文档是否还保留着这个参数 启动tomcat，访问该地址，下面要讲解的一些配置信息，在该文档下都有说明的： 文档：http://127.0.0.1:8080/docs/config 你也可以直接看网络版本： Tomcat 6 文档：https://tomcat.apache.org/tomcat-6.0-doc/config Tomcat 7 文档：https://tomcat.apache.org/tomcat-7.0-doc/config/ Tomcat 8 文档：https://tomcat.apache.org/tomcat-8.0-doc/config/ 如果你需要查看 Tomcat 的运行状态可以配置tomcat管理员账户，然后登陆 Tomcat 后台进行查看 编辑 /opt/tomcat8/bin/conf/tomcat-users.xml 文件，在里面添加下面信息： 编辑配置文件：vim /usr/program/tomcat8/conf/server.xml 打开默认被注释的连接池配置 默认值： --> 修改为： 重点参数解释： maxThreads，最大并发数，默认设置 200，一般建议在 500 ~ 800，根据硬件设施和业务来判断 minSpareThreads，Tomcat 初始化时创建的线程数，默认设置 25 prestartminSpareThreads，在 Tomcat 初始化的时候就初始化 minSpareThreads 的参数值，如果不等于 true，minSpareThreads 的值就没啥效果了 maxQueueSize，最大的等待队列数，超过则拒绝请求 maxIdleTime，如果当前线程大于初始化线程，那空闲线程存活的时间，单位毫秒，默认60000=60秒=1分钟。 修改默认的链接参数配置 默认值： 修改为： 重点参数解释： protocol，Tomcat 8 设置 nio2 更好：org.apache.coyote.http11.Http11Nio2Protocol（如果这个用不了，就用下面那个） protocol，Tomcat 6、7 设置 nio 更好：org.apache.coyote.http11.Http11NioProtocol enableLookups，禁用DNS查询，tomcat 8 默认已经是禁用了。 maxConnections，最大连接数，tomcat 8 默认设置 10000 acceptCount，指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理，默认设置 100 maxPostSize，以 FORM URL 参数方式的 POST 提交方式，限制提交最大的大小，默认是 2097152(2兆)，它使用的单位是字节。10485760 为 10M。如果要禁用限制，则可以设置为 -1。 maxHttpHeaderSize，http请求头信息的最大程度，超过此长度的部分不予处理。一般8K。 禁用 AJP（如果你服务器没有使用 Apache） 把下面这一行注释掉，默认 Tomcat 是开启的。 --> 关闭自动部署功能： 旧值： 新值： JVM 优化（JDK 8） 模型资料来源：http://xmuzyq.iteye.com/blog/599750 配比资料：http://www.jianshu.com/p/d45e12241af4 JDK8 配比：关键系统的JVM参数推荐(2018仲夏版) JDK8 常用配比总结 8G 内存：CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -Xms4g -Xmx4g\" Java 的内存模型看：这篇文章 Linux 修改 /usr/program/tomcat8/bin/catalina.sh 文件，把下面信息添加到文件第一行。 如果服务器只运行一个 Tomcat，堆栈信息可以这样配置： 机子内存如果是 4G： CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms2g -Xmx2g\" 机子内存如果是 8G： CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms4g -Xmx4g\" 机子内存如果是 16G： CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms8g -Xmx8g\" 机子内存如果是 32G： CATALINA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms16g -Xmx16g\" 如果是 8G 开发机 -Xms2g -Xmx2g 如果是 16G 开发机 -Xms4g -Xmx4g 还有一个参数：-XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M 这个可以通过调试来确认什么值合适，一般通过使用 jstat -gc PID 250 20，查看 gc 情况下的 MC、MU 情况。 默认 MaxMetaspaceSize 是 -1，无上限，所以如果硬件还行，不配置也没啥问题。 自己也了解 JVM 实际情况，那就根据实际情况调整。一般项目可以推荐：-XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M Windows 修改 /tomcat7/bin/catalina.bat 文件，找到这一行：echo Using CATALINA_BASE: \"%CATALINA_BASE%\"，然后在其上面添加如下内容，此方法只对解压版的 Tomcat 有效果，对于安装版本的需要点击安装后任务栏上的那个 Tomcat 图标，打开配置中有一个 Java Tab 的进行编辑。set JAVA_OPTS=%JAVA_OPTS% -Dfile.encoding=\"UTF-8\" -Dsun.jnu.encoding=\"UTF8\" -Ddefault.client.encoding=\"UTF-8\" -Duser.language=Zh set JAVA_OPTS=%JAVA_OPTS% -server -Xms4g -Xmx4g tomcat-manager 监控配置（tomcat 8.0.53） 开启步骤 不同的 Tomcat 版本会有差异。 官网文档：https://tomcat.apache.org/tomcat-8.0-doc/manager-howto.html 先确保解压的 tomcat/webapps 下有 manager 项目 在配置文件里面添加可访问用户：vim /usr/local/tomcat8/conf/tomcat-users.xml，比如： 正常情况下，manager ui 界面只运行内网：127.0.0.1 访问，这里我们要关闭这个限制。 修改 webapps 下 manager 项目下的配置：vim /usr/local/tomcat8/webapps/manager/META-INF/context.xml 旧值： 新值： --> 浏览器访问：http://120.78.72.28:8080/manager/status 可以看到 JVM 堆栈信息 可以看到 HTTP 连接数情况 配置文件里面限制的最大线程数：Max threads: 200 当前线程数：Current thread count: 10 当前繁忙的线程数：Current thread busy: 1 如果当前繁忙线程已经是接近最大线程数，那基本可以表示负载到了 保持连接数：Keep alive sockets count: 1 Tomcat 8 的 Log 分割 修改前提：本人为 Tomcat8.5，安装目录为：/usr/program/tomcat8 网络上的官网地址现在打不开：http://cronolog.org/download/index.html 阿里云的 CentOS 有 epel 源所以可以直接下载：yum install cronolog，此时：2017-02，最新版本为：1.6.2-10.el6 安装完后，查看下安装后的目录位置：which cronolog，我这边得到的结果是：/usr/sbin/cronolog，记下这个结果，后面有用到。 修改 catalina.sh 中的内容：vim /usr/program/tomcat8/bin/catalina.sh 找到这段代码（预计在 416 行前后）： shift touch \"$CATALINA_OUT\" if [ \"$1\" = \"-security\" ] ; then if [ $have_tty -eq 1 ]; then echo \"Using Security Manager\" fi shift eval $_NOHUP \"\\\"$_RUNJAVA\\\"\" \"\\\"$LOGGING_CONFIG\\\"\" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \\ -classpath \"\\\"$CLASSPATH\\\"\" \\ -Djava.security.manager \\ -Djava.security.policy==\"\\\"$CATALINA_BASE/conf/catalina.policy\\\"\" \\ -Dcatalina.base=\"\\\"$CATALINA_BASE\\\"\" \\ -Dcatalina.home=\"\\\"$CATALINA_HOME\\\"\" \\ -Djava.io.tmpdir=\"\\\"$CATALINA_TMPDIR\\\"\" \\ org.apache.catalina.startup.Bootstrap \"$@\" start \\ >> \"$CATALINA_OUT\" 2>&1 \"&\" else eval $_NOHUP \"\\\"$_RUNJAVA\\\"\" \"\\\"$LOGGING_CONFIG\\\"\" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \\ -classpath \"\\\"$CLASSPATH\\\"\" \\ -Dcatalina.base=\"\\\"$CATALINA_BASE\\\"\" \\ -Dcatalina.home=\"\\\"$CATALINA_HOME\\\"\" \\ -Djava.io.tmpdir=\"\\\"$CATALINA_TMPDIR\\\"\" \\ org.apache.catalina.startup.Bootstrap \"$@\" start \\ >> \"$CATALINA_OUT\" 2>&1 \"&\" fi 将上面代码改为如下，其中请注意这个关键字：/usr/sbin/cronolog，这个是我上面提到的安装路径，你如果跟我不一样，需要自己修改该相关。 shift # touch \"$CATALINA_OUT\" if [ \"$1\" = \"-security\" ] ; then if [ $have_tty -eq 1 ]; then echo \"Using Security Manager\" fi shift eval $_NOHUP \"\\\"$_RUNJAVA\\\"\" \"\\\"$LOGGING_CONFIG\\\"\" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \\ -classpath \"\\\"$CLASSPATH\\\"\" \\ -Djava.security.manager \\ -Djava.security.policy==\"\\\"$CATALINA_BASE/conf/catalina.policy\\\"\" \\ -Dcatalina.base=\"\\\"$CATALINA_BASE\\\"\" \\ -Dcatalina.home=\"\\\"$CATALINA_HOME\\\"\" \\ -Djava.io.tmpdir=\"\\\"$CATALINA_TMPDIR\\\"\" \\ org.apache.catalina.startup.Bootstrap \"$@\" start 2>&1 | /usr/sbin/cronolog \"$CATALINA_BASE\"/logs/catalina.%Y-%m-%d.out >> /dev/null & else eval $_NOHUP \"\\\"$_RUNJAVA\\\"\" \"\\\"$LOGGING_CONFIG\\\"\" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \\ -classpath \"\\\"$CLASSPATH\\\"\" \\ -Dcatalina.base=\"\\\"$CATALINA_BASE\\\"\" \\ -Dcatalina.home=\"\\\"$CATALINA_HOME\\\"\" \\ -Djava.io.tmpdir=\"\\\"$CATALINA_TMPDIR\\\"\" \\ org.apache.catalina.startup.Bootstrap \"$@\" start 2>&1 | /usr/sbin/cronolog \"$CATALINA_BASE\"/logs/catalina.%Y-%m-%d.out >> /dev/null & fi 禁止外网通过 8080 端口访问 Tomcat 添加 iptables 规则： iptables -t filter -A INPUT -p tcp -m tcp --dport 8080 -s localhost -j ACCEPT iptables -t filter -A INPUT -p tcp -m tcp --dport 8080 -j REJECT service iptables save service iptables restart Dockerfile 构建 Tomcat 镜像并部署 war 包 因为我自己改了 Tomcat 的几个配置文件，所以要把那几个文件和 Dockerfile 放一起进行构建。 在宿主机上创建 dockerfile 存放目录和 logs 目录：mkdir -p /opt/cas-dockerfile/ /data/logs/tomcat/ FROM tomcat:8.0.46-jre8 MAINTAINER GitNavi RUN rm -rf /usr/local/tomcat/webapps/* ADD server.xml /usr/local/tomcat/conf/ ADD cas.war /usr/local/tomcat/webapps/ CMD [\"catalina.sh\", \"run\"] EXPOSE 8081 须知：容器中的 Tomcat 日志我是输出在容器的目录下：/data/logs/，所以我挂载中会有这个挂载选项 开始构建： cd /opt/cas-dockerfile docker build . --tag=\"sso/cas-tomcat8:v1.0.9\" docker run -d -p 8111:8081 -v /data/logs/tomcat/:/data/logs/ --name=\"cas-tomcat-1.0.9\" sso/cas-tomcat8:v1.0.9 查看启动后容器列表：docker ps 进入 tomcat 容器终端查看一些情况：docker exec -it 57a682478233 /bin/bash jar 应用的日志是输出在容器的 /opt 目录下，因为我们上面用了挂载，所在在我们宿主机的 /usr/local/logs 目录下可以看到输出的日志 CentOS 7 防火墙开放端口： firewall-cmd --zone=public --add-port=8111/tcp --permanent firewall-cmd --reload 其他 Tomcat 历史版本下载地址整理（不间断更新）： Tomcat 9.0.0.M4：wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-9/v9.0.0.M4/bin/apache-tomcat-9.0.0.M4.tar.gz Tomcat 8.0.32：wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.0.32/bin/apache-tomcat-8.0.32.tar.gz Tomcat 7.0.68：wget http://apache.fayea.com/tomcat/tomcat-7/v7.0.68/bin/apache-tomcat-7.0.68.tar.gz Tomcat 6.0.45：wget http://mirrors.cnnic.cn/apache/tomcat/tomcat-6/v6.0.45/bin/apache-tomcat-6.0.45.tar.gz 其他问题 log4j2 输出的时间与北京时间相差 8 小时 原因是系统时区不对。 设置时区： timedatectl set-timezone Asia/Shanghai timedatectl status 资料 http://www.jikexueyuan.com/course/2064_3.html?ss=1 http://www.wellho.net/mouth/2163_CATALINA-OPTS-v-JAVA-OPTS-What-is-the-difference-.html http://blog.csdn.net/sunlovefly2012/article/details/47395165 http://blog.csdn.net/lifetragedy/article/details/7708724 http://ihuangweiwei.iteye.com/blog/1233941 http://www.cnblogs.com/ggjucheng/archive/2013/04/16/3024731.html https://tomcat.apache.org/tomcat-8.0-doc/config/http.html#Connector_Comparison http://www.apelearn.com/study_v2/chapter23.html http://blog.csdn.net/hanzheng260561728/article/details/51236131 http://blog.csdn.net/attagain/article/details/38639007 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Jenkins-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Jenkins-Install-And-Settings.html","title":"Jenkins 安装和配置","keywords":"","body":"Jenkins 安装和配置 Jenkins 介绍 官网：http://jenkins-ci.org/ 官网插件库：https://plugins.jenkins.io/ 官网下载：https://jenkins.io/download/ 官网帮助中心：https://wiki.jenkins-ci.org/display/JENKINS/Use+Jenkins Docker 下安装 Jenkins 配置：至少需要 2G 内存 先禁用 selinux 编辑配置文件：vim /etc/selinux/config 把 SELINUX=enforcing 改为 SELINUX=disabled 重启服务器 官网下载中有介绍其版本标识：https://jenkins.io/download/ 我们就选用：Long-term Support (LTS) 官网关于 Docker 部署也有专门文档：https://github.com/jenkinsci/docker/blob/master/README.md 先创建一个宿主机以后用来存放数据的目录：mkdir -p /data/jenkins/jenkins_home && chmod 777 -R /data/jenkins/jenkins_home 安装镜像（813MB，有点大）：docker pull jenkins/jenkins:lts 查看下载下来的镜像：docker images 首次运行镜像：docker run --name jenkins-master -p 8123:18080 -p 50000:50000 -v /etc/localtime:/etc/localtime -v /data/jenkins/jenkins_home:/var/jenkins_home -e JAVA_OPTS=\"-Duser.timezone=Asia/Shanghai\" -d --restart always jenkins/jenkins:lts 这里的 8080 端口是 jenkins 运行程序的端口，必须要有映射的。50000 端口是非必须映射的，但是如果你要用 Jenkins 分布式构建这个就必须开放 如果报下面的错误： touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? 解决办法：chown -R 1000:1000 /data/jenkins/jenkins_home，具体原因：点击查看 问题的本质就是，jenkins 镜像中的系统用户是：jenkins，当你进入容器 bash 内，输入：whoami && id，你就可以看到他的 uid 是 1000，所以这里才 chown 1000 查看容器运行情况：docker ps 进入容器中 Jenkins shell 交互界面：docker exec -it bd290d5eb0d /bin/bash 首次使用 Jenkins / Jenkins 插件推荐 我这里就不截图了，有需要截图可以看这博文，差不多就是这样的：点击我o(∩_∩)o 首次进入 Jenkins 的 Web UI 界面是一个解锁页面 Unlock Jenkins，需要让你输入：Administrator password 这个密码放在：/var/jenkins_home/secrets/initialAdminPassword，你需要先：docker exec -it ci_jenkins_1 /bin/bash 然后：cat /data/jenkins/jenkins_home/secrets/initialAdminPassword 也有可能是这个目录：cat /var/jenkins_home/secrets/initialAdminPassword 然后再接下来就是插件的安装，我推荐直接用它推荐给我们的插件直接安装，稍后再安装自己需要定制的。 插件安装完会进入：Create First Admin User 页面，填写一个用户信息即可。 我的这里的代码仓库是：Gitlab 推荐插件 Publish Over SSH（具体名字要看下） Dashbroad View Folders View OWASP Markup Formatter Plugin Build Name Setter Plugin build timeout plugin Credentials Binding Plugin Embeddable Build Status Plugin Pipeline Build Pipeline Plugin Docker Pipeline Plugin Git plugin GitLab Plugin SSH Slaves plugin Maven Integration plugin Matrix Authorization Strategy Plugin PAM Authentication plugin LDAP Plugin Role-based Authorization Strategy Email Extension Plugin Email Extension Template Plugin Mailer Plugin NotifyQQ（QQ 消息通知） 钉钉通知（钉钉 消息通知） 360 FireLine：代码规范检查，已经集成了阿里巴巴的代码规约（P3C）检查 AnsiColor（可选）：这个插件可以让Jenkins的控制台输出的log带有颜色 oauth（具体名字要看下） Build Failure Analyzer 分析构建错误日志并在构建页面显示错误 SSH plugin 支持通过SSH执行脚本 Pre SCM BuildStep Plugin 在拉代码之前插入一些步骤 GitHub API Plugin Github API插件 GitHub Pull Request Builder Github Pull Request时自动构建 GitHub plugin Github与Jenkins集成 GIT client plugin Git客户端插件 Maven Integration plugin：用于构建 Maven 项目 Gradle Plugin：用于构建 Gradle 项目 Gitlab Plugin：可能会直接安装不成功，如果不成功根据报错的详细信息可以看到 hpi 文件的下载地址，挂代理下载下来，然后离线安装即可 Gitlab Hook：用于触发 GitLab 的一些 WebHooks 来构建项目 Gitlab Authentication 这个插件提供了使用GitLab进行用户认证和授权的方案 Docker Commons Plugin Docker plugin Kubernetes Pre SCM BuildStep Plugin 在拉代码之前插入一些步骤 GitHub Pull Request Builder Github Pull Request时自动构建 GitHub API Plugin Github API插件 NodeJS Plugin Docker 的 Jenkins 与 Docker 结合使用 运行镜像命令：docker run --name jenkins-master -p 8123:18080 -p 50000:50000 -v /etc/localtime:/etc/localtime -v /data/jenkins/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -e JAVA_OPTS=\"-Duser.timezone=Asia/Shanghai\" -d --restart always jenkins/jenkins:lts 比上面多了一步：-v /var/run/docker.sock:/var/run/docker.sock 这样，在 jenkins 里面写 shell 脚本调用 docker 程序，就可以直接调用宿主机的 docker 了。 Jenkins 安装（YUM） 需要 JDK8 环境 当前最新版本：2.138.1-1.1（201810） 官网安装说明 RedHat Linux RPM packages：https://pkg.jenkins.io/redhat-stable/ 官网在线安装（72M），该安装方式会自己生成一个 jenkins 用户组和用户： sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key sudo yum install -y jenkins 查看安装后的情况：rpm -ql jenkins /etc/init.d/jenkins /etc/logrotate.d/jenkins /etc/sysconfig/jenkins /usr/lib/jenkins /usr/lib/jenkins/jenkins.war /usr/sbin/rcjenkins /var/cache/jenkins /var/lib/jenkins /var/log/jenkins jenkins 相关目录释义： /usr/lib/jenkins/：jenkins安装目录，war 包会放在这里。 /etc/sysconfig/jenkins：jenkins配置文件，“端口”，“JENKINS_HOME” 等都可以在这里配置。 /var/lib/jenkins/：默认的 JENKINS_HOME。 /var/log/jenkins/jenkins.log：jenkins 日志文件。 控制台输出方式启动：java -jar /usr/lib/jenkins/jenkins.war --httpPort=18080 内置 Jetty，默认是 18080 端口，你也可以改为其他（建议修改为其他） 可以看到有一个这个重点内容，这是你的初始化密码，等下会用到的： Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: daacc724767640a29ddc99d159a80cf8 This may also be found at: /root/.jenkins/secrets/initialAdminPassword 守护进程启动：nohup java -jar /usr/lib/jenkins/jenkins.war --httpPort=18080 > /dev/null 2>&1 & 浏览器访问 Jenkins 首页开始配置：http://192.168.0.105:18080/ 特殊情况： 如果配置插件过程遇到这个错误：No valid crumb was included in the request，则多重试几次。 登录后把：http://192.168.0.105:18080/configureSecurity/ 下面的 防止跨站点请求伪造 勾选去掉。遇到问题多试几次。 pipeline 语法 全局 pipeline 语法说明：http://192.168.0.105:18080/job/react/pipeline-syntax/globals 其他资料 http://www.cnblogs.com/fengjian2016/p/8227532.html https://github.com/nbbull/jenkins2Book https://github.com/mcpaint/learning-jenkins-pipeline https://www.cnblogs.com/fengjian2016/p/8227532.html https://blog.csdn.net/diantun00/article/details/81075007 内置的参数 BUILD_NUMBER = ${env.BUILD_NUMBER}\" BUILD_ID = ${env.BUILD_ID}\" BUILD_DISPLAY_NAME = ${env.BUILD_DISPLAY_NAME}\" JOB_NAME = ${env.JOB_NAME}\" JOB_BASE_NAME = ${env.JOB_BASE_NAME}\" WORKSPACE = ${env.WORKSPACE}\" JENKINS_HOME = ${env.JENKINS_HOME}\" JENKINS_URL = ${env.JENKINS_URL}\" BUILD_URL = ${env.BUILD_URL}\" JOB_URL = ${env.JOB_URL}\" 输出结果： BUILD_NUMBER = 21 BUILD_ID = 21 BUILD_DISPLAY_NAME = #21 JOB_NAME = react JOB_BASE_NAME = react WORKSPACE = /root/.jenkins/workspace/react JENKINS_HOME = /root/.jenkins JENKINS_URL = http://192.168.0.105:18080/ BUILD_URL = http://192.168.0.105:18080/job/react/21/ JOB_URL = http://192.168.0.105:18080/job/react/ 构建时指定参数 如果要构建的时候明确输入参数值，可以用 parameters： pipeline { agent any parameters { string(name: 'assignVersionValue', defaultValue: '1.1.3', description: '构建之前请先指定版本号') } tools { jdk 'JDK8' maven 'MAVEN3' } options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } environment { gitUrl = \"https://gitee.com/youmeek/springboot-jenkins-demo.git\" branchName = \"master\" giteeCredentialsId = \"Gitee\" projectWorkSpacePath = \"${env.WORKSPACE}\" } stages { stage('Check Env') { /*当指定的参数版本号等于空字符的时候进入 steps。这里的 when 对 当前 stage 有效，对其他 stage 无效*/ when { environment name: 'assignVersionValue', value: '' } /*结束整个任务。如果不想结束整个任务，就不要用：exit 1*/ steps { sh \"exit 1\" } } stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目空间文件夹路径 = ${projectWorkSpacePath}\" echo \"======================================构建时自己指定的版本号值 = ${params.assignVersionValue}\" } } } } 定时构建 pipeline { agent any /*采用 linux cron 语法即可*/ triggers { cron('*/1 * * * *') } tools { jdk 'JDK8' maven 'MAVEN3' } options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } environment { gitUrl = \"https://gitee.com/youmeek/springboot-jenkins-demo.git\" branchName = \"master\" giteeCredentialsId = \"Gitee\" projectWorkSpacePath = \"${env.WORKSPACE}\" } stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目空间文件夹路径 = ${projectWorkSpacePath}\" } } } } 同时构建其他 Job stage('运行其他任务') { steps { build job: '任务名称' } } Jenkins 前端 React 项目构建 确保：安装了 Node.js 简单的 pipeline 写法（开源项目） pipeline { agent any options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } /*=======================================常修改变量-start=======================================*/ environment { gitUrl = \"https://github.com/satan31415/heh_umi_template.git\" branchName = \"master\" projectBuildPath = \"${env.WORKSPACE}/dist\" nginxHtmlRoot = \"/usr/share/nginx/react\" } /*=======================================常修改变量-end=======================================*/ stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目 Build 文件夹路径 = ${projectBuildPath}\" echo \"======================================项目 Nginx 的 ROOT 路径 = ${nginxHtmlRoot}\" } } stage('Git Clone'){ steps { git branch: \"${branchName}\", url: \"${gitUrl}\" } } stage('NPM Install') { steps { sh \"npm install\" } } stage('NPM Build') { steps { sh \"npm run build\" } } stage('Nginx Deploy') { steps { sh \"rm -rf ${nginxHtmlRoot}/\" sh \"cp -r ${projectBuildPath}/ ${nginxHtmlRoot}/\" } } } } 简单的 pipeline 写法（闭源项目 -- 码云为例） 新增一个全局凭据：http://192.168.0.105:18080/credentials/store/system/domain/_/newCredentials 类型：Username with password 范围：全局 Username：你的 Gitee 账号 Password：你的 Gitee 密码 ID：只要是唯一值就行，后面要用到 描述：最好跟 ID 一致，方便认 pipeline { agent any options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } /*=======================================常修改变量-start=======================================*/ environment { gitUrl = \"https://gitee.com/youmeek/react-demo.git\" branchName = \"master\" giteeCredentialsId = \"上面全局凭据填写的 ID\" projectBuildPath = \"${env.WORKSPACE}/dist\" nginxHtmlRoot = \"/usr/share/nginx/react\" } /*=======================================常修改变量-end=======================================*/ stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目 Build 文件夹路径 = ${projectBuildPath}\" echo \"======================================项目 Nginx 的 ROOT 路径 = ${nginxHtmlRoot}\" } } stage('Git Clone'){ steps { git branch: \"${branchName}\", credentialsId: \"${giteeCredentialsId}\", url: \"${gitUrl}\" } } stage('NPM Install') { steps { sh \"npm install\" } } stage('NPM Build') { steps { sh \"npm run build\" } } stage('Nginx Deploy') { steps { sh \"rm -rf ${nginxHtmlRoot}/\" sh \"cp -r ${projectBuildPath}/ ${nginxHtmlRoot}/\" } } } } Jenkins 后端 Spring Boot 项目构建 安装 Maven 参考该文章 配置工具 访问：http://192.168.0.105:18080/configureTools/ 我习惯自己安装，所以这里修改配置： 需要注意：配置里面的 别名 不要随便取名字，后面 Pipeline 要用到的。在 tool 标签里面会用到。 具体可以查看该图片说明：点击查看 简单的 pipeline 写法（Jar 方式运行）（闭源项目 -- 码云为例） 用 supervisord 做进程控制 supervisord 的使用 生成 supervisord 的配置文件会写在 Pipeline，所以只要你保证服务器 supervisord 正常运行即可 配置 Jenkins 必须：新增一个全局凭据，方法参考前端部分 pipeline { agent any /*=======================================工具环境修改-start=======================================*/ tools { jdk 'JDK8' maven 'MAVEN3' } /*=======================================工具环境修改-end=======================================*/ options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } /*=======================================常修改变量-start=======================================*/ environment { gitUrl = \"https://gitee.com/youmeek/springboot-jenkins-demo.git\" branchName = \"master\" giteeCredentialsId = \"Gitee\" projectWorkSpacePath = \"${env.WORKSPACE}\" projectBuildTargetPath = \"${env.WORKSPACE}/target\" projectJarNewName = \"${env.JOB_NAME}.jar\" supervisorConfigFileFullPath = \"/etc/supervisor/conf.d/${env.JOB_NAME}.conf\" supervisorLogPath = \"/var/log/supervisor\" } /*=======================================常修改变量-end=======================================*/ stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目空间文件夹路径 = ${projectWorkSpacePath}\" echo \"======================================项目 build 后 jar 路径 = ${projectBuildTargetPath}\" echo \"======================================项目 jar 新名称 = ${projectJarNewName}\" echo \"======================================supervisor 配置文件路径 = ${supervisorConfigFileFullPath}\" echo \"======================================supervisor 存放 log 路径 = ${supervisorLogPath}\" } } stage('Git Clone'){ steps { git branch: \"${branchName}\", credentialsId: \"${giteeCredentialsId}\", url: \"${gitUrl}\" } } stage('Maven Clean') { steps { sh \"mvn clean\" } } stage('Maven Package') { steps { sh \"mvn package -DskipTests\" } } stage('Spring Boot Run') { steps { sh \"\"\" mv ${projectBuildTargetPath}/*.jar ${projectBuildTargetPath}/${projectJarNewName} if [ ! -f ${supervisorConfigFileFullPath} ]; then touch ${supervisorConfigFileFullPath} cat > ${supervisorConfigFileFullPath} [program:${env.JOB_NAME}] command=java -jar ${projectBuildTargetPath}/${projectJarNewName} stdout_logfile=${supervisorLogPath}/${env.JOB_NAME}.log stderr_logfile=${supervisorLogPath}/${env.JOB_NAME}-err.log user=root autostart=true autorestart=false startsecs=5 priority=1 stopasgroup=true killasgroup=true EOF /usr/bin/supervisorctl update fi /usr/bin/supervisorctl restart ${env.JOB_NAME} \"\"\" } } } } 简单的 pipeline 写法（Docker 方式运行）（闭源项目 -- 码云为例） 确保 项目根目录有 Dockerfile 文件（部分内容自己修改），内容模板： FROM java:8 VOLUME /tmp ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone ADD ./target/buildApp.jar /app.jar RUN bash -c 'touch /app.jar' EXPOSE 8081 ENTRYPOINT [\"java\", \"-jar\", \"-Xms512M\", \"-Xmx512M\" , \"-XX:MetaspaceSize=128M\", \"-XX:MaxMetaspaceSize=256M\" ,\"/app.jar\"] Pipeline 写法 pipeline { agent any /*=======================================工具环境修改-start=======================================*/ tools { jdk 'JDK8' maven 'MAVEN3' } /*=======================================工具环境修改-end=======================================*/ options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } /*=======================================常修改变量-start=======================================*/ environment { gitUrl = \"https://gitee.com/youmeek/springboot-jenkins-demo.git\" branchName = \"master\" giteeCredentialsId = \"Gitee\" projectWorkSpacePath = \"${env.WORKSPACE}\" projectBuildTargetPath = \"${env.WORKSPACE}/target\" projectJarNewName = \"buildApp.jar\" dockerImageName = \"docker.youmeek.com/demo/${env.JOB_NAME}:${env.BUILD_NUMBER}\" dockerContainerName = \"${env.JOB_NAME}\" inHostPort = \"8082\" inDockerAndJavaPort = \"8081\" inHostLogPath = \"/data/docker/logs/${dockerContainerName}/${env.BUILD_NUMBER}\" inDockerLogPath = \"/data/logs\" dockerRunParam = \"--name=${dockerContainerName} --hostname=${dockerContainerName} -v /etc/hosts:/etc/hosts -v ${inHostLogPath}:${inDockerLogPath} --restart=always -p ${inHostPort}:${inDockerAndJavaPort}\" } /*=======================================常修改变量-end=======================================*/ stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目空间文件夹路径 = ${projectWorkSpacePath}\" echo \"======================================项目 build 后 jar 路径 = ${projectBuildTargetPath}\" echo \"======================================项目 jar 新名称 = ${projectJarNewName}\" echo \"======================================Docker 镜像名称 = ${dockerImageName}\" echo \"======================================Docker 容器名称 = ${dockerContainerName}\" } } stage('Git Clone'){ steps { git branch: \"${branchName}\", credentialsId: \"${giteeCredentialsId}\", url: \"${gitUrl}\" } } stage('Maven Clean') { steps { sh \"mvn clean\" } } stage('Maven Package') { steps { sh \"mvn package -DskipTests\" } } stage('构建 Docker 镜像') { steps { sh \"\"\" mv ${projectBuildTargetPath}/*.jar ${projectBuildTargetPath}/${projectJarNewName} cd ${projectWorkSpacePath} docker build -t ${dockerImageName} ./ \"\"\" } } stage('运行 Docker 镜像') { steps { sh \"\"\" docker stop ${dockerContainerName} | true docker rm -f ${dockerContainerName} | true docker run -d ${dockerRunParam} ${dockerImageName} \"\"\" } } } } 简单的 pipeline 写法（Docker + Harbor 方式运行）（闭源项目 -- 码云为例） 请先看懂上面 Docker 方式 一共需要 3 台机子（要保证在内网环境，不然一定会有安全问题） 一台部署 Harbor 一台部署 Jenkins 一台运行项目 确保 Jenkins 机子已经 Docker Login Harbor，这个就一次性的动作，所以自己在 Jenkins 服务器上操作即可 确保 Spring Boot 项目运行的机子已经 Docker Login Harbor，这个就一次性的动作，所以自己在 Jenkins 服务器上操作即可 确保 Spring Boot 项目运行的机子 docker remote api 开启（没有身份认证功能，所以才要保证内网） Pipeline 写法 pipeline { agent any /*=======================================工具环境修改-start=======================================*/ tools { jdk 'JDK8' maven 'MAVEN3' } /*=======================================工具环境修改-end=======================================*/ options { timestamps() disableConcurrentBuilds() buildDiscarder(logRotator( numToKeepStr: '20', daysToKeepStr: '30', )) } /*=======================================常修改变量-start=======================================*/ environment { gitUrl = \"https://gitee.com/youmeek/springboot-jenkins-demo.git\" branchName = \"master\" giteeCredentialsId = \"Gitee\" projectWorkSpacePath = \"${env.WORKSPACE}\" projectBuildTargetPath = \"${env.WORKSPACE}/target\" projectJarNewName = \"buildApp.jar\" projectDockerDaemon = \"tcp://192.168.1.12:2376\" harborUrl = \"192.168.1.13\" harborProjectName = \"demo\" dockerImageName = \"${harborUrl}/${harborProjectName}/${env.JOB_NAME}:${env.BUILD_NUMBER}\" dockerContainerName = \"${env.JOB_NAME}\" inHostPort = \"8082\" inDockerAndJavaPort = \"8081\" inHostLogPath = \"/data/docker/logs/${dockerContainerName}/${env.BUILD_NUMBER}\" inDockerLogPath = \"/data/logs\" dockerRunParam = \"--name=${dockerContainerName} --hostname=${dockerContainerName} -v /etc/hosts:/etc/hosts -v ${inHostLogPath}:${inDockerLogPath} --restart=always -p ${inHostPort}:${inDockerAndJavaPort}\" } /*=======================================常修改变量-end=======================================*/ stages { stage('Pre Env') { steps { echo \"======================================项目名称 = ${env.JOB_NAME}\" echo \"======================================项目 URL = ${gitUrl}\" echo \"======================================项目分支 = ${branchName}\" echo \"======================================当前编译版本号 = ${env.BUILD_NUMBER}\" echo \"======================================项目空间文件夹路径 = ${projectWorkSpacePath}\" echo \"======================================项目 build 后 jar 路径 = ${projectBuildTargetPath}\" echo \"======================================项目 jar 新名称 = ${projectJarNewName}\" echo \"======================================Docker 镜像名称 = ${dockerImageName}\" echo \"======================================Docker 容器名称 = ${dockerContainerName}\" echo \"======================================harbor 地址 = ${harborUrl}\" echo \"======================================harbor 项目名称 = ${harborProjectName}\" echo \"======================================项目在宿主机的端口 = ${inHostPort}\" echo \"======================================项目在 Docker 容器中的端口 = ${inDockerAndJavaPort}\" echo \"======================================项目在宿主机的 log 路径 = ${inHostLogPath}\" echo \"======================================项目在 docker 容器的 log 路径 = ${inDockerLogPath}\" echo \"======================================项目运行的 Docker remote ip 信息 = ${projectDockerDaemon}\" echo \"======================================项目运行的参数 = ${dockerRunParam}\" } } stage('Git Clone'){ steps { git branch: \"${branchName}\", credentialsId: \"${giteeCredentialsId}\", url: \"${gitUrl}\" } } stage('Maven Clean') { steps { sh \"mvn clean\" } } stage('Maven Package') { steps { sh \"mvn package -DskipTests\" } } stage('构建 Docker 镜像') { steps { sh \"\"\" mv ${projectBuildTargetPath}/*.jar ${projectBuildTargetPath}/${projectJarNewName} cd ${projectWorkSpacePath} docker build -t ${dockerImageName} ./ \"\"\" } } stage('Push Docker 镜像') { options { timeout(time: 5, unit: 'MINUTES') } steps { sh \"\"\" docker push ${dockerImageName} docker rmi ${dockerImageName} \"\"\" } } stage('运行远程 Docker 镜像') { options { timeout(time: 5, unit: 'MINUTES') } steps { sh \"\"\" docker -H ${projectDockerDaemon} pull ${dockerImageName} docker -H ${projectDockerDaemon} stop ${dockerContainerName} | true docker -H ${projectDockerDaemon} rm -f ${dockerContainerName} | true docker -H ${projectDockerDaemon} run -d ${dockerRunParam} ${dockerImageName} \"\"\" } } } } 多节点 master 与 slave 可以参考这篇：http://www.cnblogs.com/sparkdev/p/7102622.html 资料 http://stackoverflow.com/questions/4969156/java-net-unknownhostexception https://www.jianshu.com/p/b50e679e2409 http://xkcoding.com/2018/01/04/devops-jenkins.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Maven-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Maven-Install-And-Settings.html","title":"Maven 安装和配置","keywords":"","body":"Maven 安装和配置 Maven 资料 官网：http://maven.apache.org/ 官网下载：http://maven.apache.org/download.cgi 历史版本下载：https://archive.apache.org/dist/maven/binaries/ 此时（20160208） Maven 最新版本为：3.3.9 Maven 安装（bash 环境） Maven 3.3 的 JDK 最低要求是 JDK 7 下载压缩包：wget http://mirrors.cnnic.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz 解压：tar zxvf apache-maven-3.3.9-bin.tar.gz 修改目录名，默认的太长了：mv apache-maven-3.3.9/ maven3.3.9/ 移到我个人习惯的安装目录下：mv maven3.3.9/ /usr/local 环境变量设置：vim /etc/profile 在文件最尾巴添加下面内容： # Maven MAVEN_HOME=/usr/local/maven3.3.9 M3_HOME=/usr/local/maven3.3.9 PATH=$PATH:$M3_HOME/bin MAVEN_OPTS=\"-Xms256m -Xmx356m\" export M3_HOME export MAVEN_HOME export PATH export MAVEN_OPTS 刷新配置文件：source /etc/profile 测试是否安装成功：mvn -version Maven 配置 创建本地参数：mkdir -p /opt/maven-repository 配置项目连接上私服 编辑配置文件：vim /usr/local/maven3.3.9/conf/settings.xml /opt/maven-repository nexus-releases admin admin123 nexus-snapshots admin admin123 aliyun-releases * http://maven.aliyun.com/nexus/content/groups/public/ aliyun-snapshots * http://maven.aliyun.com/nexus/content/groups/public/ 资料 http://maven.apache.org/install.html http://www.tutorialspoint.com/maven/index.htm http://maven.apache.org/guides/getting-started/maven-in-five-minutes.html http://maven.apache.org/guides/getting-started/index.html http://maven.apache.org/general.html http://stackoverflow.com/questions/6950346/infrastructure-with-maven-jenkins-nexus http://blog.csdn.net/sxyx2008/article/details/7975129 http://blog.csdn.net/xuke6677/article/details/8482472 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Nexus-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Nexus-Install-And-Settings.html","title":"Nexus 安装和配置","keywords":"","body":"Nexus 安装和配置 Nexus 安装 Nexus 安装 官网：http://www.sonatype.org/nexus/ 官网下载：http://www.sonatype.org/nexus/archived/ 此时（20160207） Nexus 最新版本为：2.12.0-01 JDK 要求是 JDK 7，官网要求 7u6 或之后版本，包括 JDK 8 官网帮助说明 1：http://books.sonatype.com/nexus-book/2.11/reference/install.html 官网帮助说明 2：http://books.sonatype.com/sonatype-clm-book/html/clm-book/installation-configuration.html 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 压缩包下载（由于国内网络的原因不排除你下载不了）：wget http://download.sonatype.com/nexus/oss/nexus-2.12.0-01-bundle.tar.gz 如果地址下载不了，那是因为你需要开 VPN，你也可以选择降低要求下载 2.11.4-01 版本：http://pan.baidu.com/s/1mgSNJtA 解压压缩包：tar zxvf nexus-2.11.4-01-bundle.tar.gz 解压出来有两个文件夹： 这是程序目录：nexus-2.11.4-01 这是仓库目录：sonatype-work 移到目录到我的安装目录下：mv nexus-2.11.4-01/ /usr/program/ 进入安装目录：cd /usr/program/ 把目录名字改为更好看点：mv nexus-2.11.4-01/ nexus2.11.4/ 编辑系统配置文件：vim /etc/profile 在文件的尾巴增加下面内容： # Nexus NEXUS_HOME=/usr/program/nexus2.11.4 export NEXUS_HOME RUN_AS_USER=root export RUN_AS_USER 刷新配置：source /etc/profile 由于目录 sonatype-work 以后是做仓库用的，会存储很多 jar，所以这个目录一定要放在磁盘空间大的区内，目前我们还没第一次启动 Nexus，所以这里还是空文件 我个人习惯把这类目录放在 /opt 下，所以你要特别注意，下面有内容对这个文件夹进行操作的都是基于 opt 目录的：mv /opt/setup/sonatype-work/ /opt/ 设置配置文件：vim /usr/program/nexus2.11.4/conf/nexus.properties 把文件中该值：nexus-work=${bundleBasedir}/../sonatype-work/nexus 改为：nexus-work=/opt/sonatype-work/nexus 默认情况下如果你的 JDK 等系统变量设置好的是无需编辑 Nexus 的配置文件，但是这里还是给大家一下配置文件路径：vim /usr/program/nexus2.11.4/bin/jsw/conf/wrapper.conf 开放防火墙端口： 添加规则：sudo iptables -I INPUT -p tcp -m tcp --dport 8081 -j ACCEPT 保存规则：sudo /etc/rc.d/init.d/iptables save 重启 iptables：sudo service iptables restart 测试安装结果： 启动 Nexus：/usr/program/nexus2.11.4/bin/nexus start 查看启动日志：tail -200f /usr/program/nexus2.11.4/logs/wrapper.log 关闭 Nexus：/usr/program/nexus2.11.4/bin/nexus stop 访问：http://192.168.0.110:8081/nexus 登录账号密码： 账号密码：admin 密码：admin123 Nexus 配置 修改默认端口：vim /usr/program/nexus2.11.4/conf/nexus.properties，修改该值：application-port=8081 下载远程中央库的索引到服务器 如上图标注 4 所示，把默认是 False 改为 True 如上图 gif 所示，创建任务开始进行索引下载。需要特别提醒的是，如果你的私服是虚拟机，那得保证你分配的硬盘足够大，别像我一样吝啬只给 10 G（现在还剩下 1.9 G），结果报：设备上没有空间 项目上配置链接连接私服（下面内容涉及到 maven 的基础知识，请自行私下学习）： 对项目独立设置： 打开项目的 pom.xml 文件： 添加下面内容： Nexus 虚拟机-192.168.0.110-Nexus http://192.168.0.110:8081/nexus/content/groups/public/ 对全局配置进行设置： 打开 maven 的 settings.xml 文件： 添加下面内容： YouMeekNexus YouMeek Nexus * http://192.168.0.110:8081/nexus/content/groups/public/ 本地开发的 jar 发布到 Nexus 上 系统下的 settings.xml 下改为如下： D:\\maven\\my_local_repository nexus-releases admin admin123 nexus-snapshots admin admin123 nexus-releases * http://192.168.1.73:8081/repository/maven-releases/ nexus-snapshots * http://192.168.1.73:8081/repository/maven-snapshots/ maven-aliyun aliyun maven http://maven.aliyun.com/nexus/content/groups/public/ central nexus nexus-releases http://nexus-releases true true nexus-snapshots http://nexus-snapshots true true nexus-releases http://nexus-releases true true nexus-snapshots http://nexus-snapshots true true nexus 在开发的项目 pom.xml 上，添加这一段： nexus-releases http://192.168.1.73:8081/repository/maven-releases/ nexus-snapshots http://192.168.1.73:8081/repository/maven-snapshots/ 然后在项目上执行：mvn deploy 持续集成自动构建后发布到 Nexus 上 在 Maven 的 settings.xml 加上连接服务器信息： nexus-releases admin admin123 nexus-snapshots admin admin123 在项目的 pom.xml 文件加上： nexus-releases Nexus Releases Repository http://192.168.0.110:8081/nexus/content/repositories/releases/ nexus-snapshots Nexus Snapshots Repository http://192.168.0.110:8081/nexus/content/repositories/snapshots/ Nexus 手动更新索引文件 手动更新索引 关闭 Nexus：/usr/program/nexus2.11.4/bin/nexus stop 命令：cd /opt/sonatype-work/nexus/indexer/central-ctx 删除里面默认的文件：rm -rf * 访问官网索引：http://repo.maven.apache.org/maven2/.index/ 下载文件：nexus-maven-repository-index.gz：wget http://repo.maven.apache.org/maven2/.index/nexus-maven-repository-index.gz 下载文件：nexus-maven-repository-index.properties：wget http://repo.maven.apache.org/maven2/.index/nexus-maven-repository-index.properties 下载索引解压工具：wget https://repo1.maven.org/maven2/org/apache/maven/indexer/indexer-cli/5.1.1/indexer-cli-5.1.1.jar 执行解压命令（该命令执行需要4分钟左右）：java -jar indexer-cli-5.1.0.jar -u nexus-maven-repository-index.gz -d ./ 删除解压前文件：rm -rf indexer-cli-5.1.0.jar nexus-maven-repository-index.gz nexus-maven-repository-index.properties 重启服务：/usr/program/nexus2.11.4/bin/nexus start Nexus3 功能 搜索 搜索依赖（支持匹配符）：http://192.168.1.73:8081/#browse/search 创建角色 地址：http://192.168.1.73:8081/#admin/security/roles 创建用户 地址：http://192.168.1.73:8081/#admin/security/users 设置仓库 仓库常见类型： proxy，“代理”类型，代理请求官方仓库，并缓存在本地，图中maven-central即是 hosted，“私有”仓库，公司组织内部自由仓库，比如存储自己的jar包等，maven-releases、maven-snapshots即是 group，“组”，逻辑上的，可以把几个仓库划归到某个组，对外统一的地址访问，maven-public即是，它包含了另外3个 仓库地址：http://192.168.1.73:8081/#admin/repository/repositories 设置 maven-central 的 proxy 为官网默认地址：https://repo1.maven.org/maven2/ proxy 的意义：当客户端访问中央库的时候，如果 Nexus 没有该依赖，则先通过 Proxy 地址下载到 Nexus 仓库，然后客户端再从 Nexus 服务器下载到本地。 一般建议新增一个 maven-aliyun 的 proxy 类型仓库，proxy 地址是：http://maven.aliyun.com/nexus/content/groups/public/ 然后在 maven-public 设置 Member repositories 的时候确保 maven-aliyun 排序大于 maven-central 即可。 设置 maven-public，集成其他仓库：http://192.168.1.73:8081/#admin/repository/repositories:maven-public 一般 Member repositories 的排序建议是：maven-releases、maven-snapshots、maven-aliyun、maven-central 如果需要设置 npm 的仓库可以看这篇文章：Nexus OSS3创建npm 私服 设置 task 地址：http://192.168.1.73:8081/#admin/system/tasks 资料 http://www.cnblogs.com/leefreeman/p/4211530.html http://www.itdadao.com/article/89071/ http://blog.zhaojunling.me/p/17 http://m.blog.csdn.net/article/details?id=49228873 http://mritd.me/2015/12/29/Nexus-2-11-CentOS%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/ http://mritd.me/2015/12/28/Nexus-%E7%A7%81%E6%9C%8D%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/ http://my.oschina.net/liangbo/blog/195739 http://www.mamicode.com/info-detail-1016489.html http://blog.csdn.net/shawyeok/article/details/23564681 http://zyjustin9.iteye.com/blog/2017321 https://www.xncoding.com/2017/09/02/tool/nexus.html https://www.addops.cn/post/a-bite-of-nexus.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Mysql-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Mysql-Install-And-Settings.html","title":"MySQL 安装和配置","keywords":"","body":"MySQL 安装和配置 Docker 安装 MySQL 关掉：SELinux 创建本地数据存储 + 配置文件目录：mkdir -p /data/docker/mysql/datadir /data/docker/mysql/conf /data/docker/mysql/log 在宿主机上创建一个配置文件：vim /data/docker/mysql/conf/mysql-1.cnf，内容如下： # 该编码设置是我自己配置的 [mysql] default-character-set = utf8mb4 # 下面内容是 docker mysql 默认的 start [mysqld] pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock datadir = /var/lib/mysql #log-error = /var/log/mysql/error.log # By default we only accept connections from localhost #bind-address = 127.0.0.1 # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 # 上面内容是 docker mysql 默认的 end # 下面开始的内容就是我自己配置的 log-error=/var/log/mysql/error.log default-storage-engine = InnoDB collation-server = utf8mb4_unicode_ci init_connect = 'SET NAMES utf8mb4' character-set-server = utf8mb4 lower_case_table_names = 1 max_allowed_packet = 50M 赋权（避免挂载的时候，一些程序需要容器中的用户的特定权限使用）：chmod -R 777 /data/docker/mysql/datadir /data/docker/mysql/log 赋权：chown -R 0:0 /data/docker/mysql/conf 配置文件的赋权比较特殊，如果是给 777 权限会报：[Warning] World-writable config file '/etc/mysql/conf.d/mysql-1.cnf' is ignored，所以这里要特殊对待。容器内是用 root 的 uid，所以这里与之相匹配赋权即可。 我是进入容器 bash 内，输入：whoami && id，看到默认用户的 uid 是 0，所以这里才 chown 0 docker run -p 3306:3306 --name cloud-mysql -v /data/docker/mysql/datadir:/var/lib/mysql -v /data/docker/mysql/log:/var/log/mysql -v /data/docker/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 连上容器：docker exec -it cloud-mysql /bin/bash 连上 MySQL：mysql -u root -p 创建表：CREATE DATABASE wormhole DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 关于容器的 MySQL 配置，官网是这样说的：https://hub.docker.com/_/mysql/ The MySQL startup configuration is specified in the file /etc/mysql/my.cnf, and that file in turn includes any files found in the /etc/mysql/conf.d directory that end with .cnf.Settings in files in this directory will augment and/or override settings in /etc/mysql/my.cnf. If you want to use a customized MySQL configuration,you can create your alternative configuration file in a directory on the host machine and then mount that directory location as /etc/mysql/conf.d inside the mysql container. 容器中的 my.cnf 内容如下： # Copyright (c) 2016, Oracle and/or its affiliates. All rights reserved. # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; version 2 of the License. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ docker 的 MySQL 备份和还原： 备份：docker exec cloud-mysql /usr/bin/mysqldump -u root --password=123456 DATABASE_Name > /opt/backup.sql 还原：docker exec -i cloud-mysql /usr/bin/mysql -u root --password=123456 DATABASE_Name MySQL 5.5 安装 来源 设置仓库 rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm rpm -Uvh https://mirror.webtatic.com/yum/el6/latest.rpm 安装：yum install mysql55w mysql55w-server，用同时生产 mysql 的组和用户 启动：service mysqld start 重置密码：mysqladmin -u root password '123456' 默认配置文件：vim /etc/my.cnf log 目录：cd /var/log/mysqld.log 查看服务 log：tail -300 /var/log/mysqld.log 给指定目录增加 mysql 用户组权限：chown mysql.mysql /var/run/mysqld/ 官网 MySQL 启动失败，这篇文章经验值得推荐：CentOS 7下MySQL服务启动失败的解决思路 MySQL 5.6 安装 假设当前用户为：root Mysql 安装 官网：http://www.mysql.com/ 官网下载：http://dev.mysql.com/downloads/mysql/ 官网 5.5 下载：http://dev.mysql.com/downloads/mysql/5.5.html#downloads 官网 5.6 下载：http://dev.mysql.com/downloads/mysql/5.6.html#downloads 官网 5.7 下载：http://dev.mysql.com/downloads/mysql/5.7.html#downloads 官网帮助中心：http://dev.mysql.com/doc/refman/5.6/en/source-installation.html 此时（20160210） Mysql 5.5 最新版本为：5.5.48 此时（20170130） Mysql 5.6 最新版本为：5.6.35 此时（20160210） Mysql 5.7 最新版本为：5.7.11 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 Mysql 5.6 下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.35.tar.gz （大小：31 M） Mysql 5.7 下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.11.tar.gz （大小：47 M） 我们这次安装以 5.6 为实例 进入下载目录：cd /opt/setups 解压压缩包：tar zxvf mysql-5.6.35.tar.gz 移到解压包：mv /opt/setups/mysql-5.6.35 /usr/local/ 安装依赖包、编译包：yum install -y make gcc-c++ cmake bison-devel ncurses-devel autoconf 进入解压目录：cd /usr/local/mysql-5.6.35/ 生成安装目录：mkdir -p /usr/local/mysql/data 生成配置（使用 InnoDB）：cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/usr/local/mysql/data -DMYSQL_UNIX_ADDR=/tmp/mysql.sock -DDEFAULT_CHARSET=utf8mb4 -DDEFAULT_COLLATION=utf8mb4_unicode_ci -DWITH_EXTRA_CHARSETS:STRING=utf8mb4 -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DENABLED_LOCAL_INFILE=1 更多参数说明可以查看：http://dev.mysql.com/doc/refman/5.6/en/source-configuration-options.html 编译：make，这个过程比较漫长，一般都在 30 分钟左右，具体还得看机子配置，如果最后结果有 error，建议删除整个 mysql 目录后重新解压一个出来继续处理 安装：make install 配置开机启动： cp /usr/local/mysql-5.6.35/support-files/mysql.server /etc/init.d/mysql chmod 755 /etc/init.d/mysql chkconfig mysql on 复制一份配置文件： cp /usr/local/mysql-5.6.35/support-files/my-default.cnf /etc/my.cnf 删除安装的目录：rm -rf /usr/local/mysql-5.6.35/ 添加组和用户及安装目录权限 groupadd mysql #添加组 useradd -g mysql mysql -s /bin/false #创建用户mysql并加入到mysql组，不允许mysql用户直接登录系统 chown -R mysql:mysql /usr/local/mysql/data #设置MySQL数据库目录权限 初始化数据库：/usr/local/mysql/scripts/mysql_install_db --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --skip-name-resolve --user=mysql 开放防火墙端口： iptables -I INPUT -p tcp -m tcp --dport 3306 -j ACCEPT service iptables save service iptables restart 禁用 selinux 编辑配置文件：vim /etc/selinux/config 把 SELINUX=enforcing 改为 SELINUX=disabled 常用命令软连接，才可以在终端直接使用：mysql 和 mysqladmin 命令 ln -s /usr/local/mysql/bin/mysql /usr/bin ln -s /usr/local/mysql/bin/mysqladmin /usr/bin ln -s /usr/local/mysql/bin/mysqldump /usr/bin ln -s /usr/local/mysql/bin/mysqlslap /usr/bin MySQL 5.7 YUM 安装 官网：https://dev.mysql.com/doc/refman/5.7/en/linux-installation-yum-repo.html 禁用 selinux：setenforce 0 wget https://repo.mysql.com//mysql57-community-release-el7-11.noarch.rpm yum localinstall mysql57-community-release-el7-11.noarch.rpm yum install mysql-community-server 一共 194M 配置文件：/etc/my.cnf systemctl start mysqld systemctl status mysqld 查看初次使用的临时密码：grep 'temporary password' /var/log/mysqld.log MySQL 配置 官网配置参数解释：http://dev.mysql.com/doc/refman/5.6/en/mysqld-option-tables.html 找一下当前系统中有多少个 my.cnf 文件：find / -name \"my.cnf\"，我查到的结果： /etc/my.cnf /usr/local/mysql/my.cnf /usr/local/mysql/mysql-test/suite/ndb/my.cnf /usr/local/mysql/mysql-test/suite/ndb_big/my.cnf ............. /usr/local/mysql/mysql-test/suite/ndb_rpl/my.cnf 保留 /etc/my.cnf 和 /usr/local/mysql/mysql-test/ 目录下配置文件，其他删除掉。 我整理的一个单机版配置说明（MySQL 5.6，适用于 1G 内存的服务器）： my.cnf 其中我测试的结果，在不适用任何配置修改的情况下，1G 内存安装 MySQL 5.6 默认就会占用 400M 左右的内存，要降下来的核心配置要补上这几个参数： performance_schema_max_table_instances=400 table_definition_cache=400 table_open_cache=256 修改 root 账号密码 启动 Mysql 服务器（CentOS 6）：service mysql start 启动 Mysql 服务器（CentOS 7）：systemctl start mysql 查看是否已经启动了：ps aux | grep mysql 默认安装情况下，root 的密码是空，所以为了方便我们可以设置一个密码，假设我设置为：123456 终端下执行：mysql -uroot 现在进入了 mysql 命令行管理界面，输入：SET PASSWORD = PASSWORD('123456');FLUSH PRIVILEGES; 现在进入了 mysql 命令行管理界面，输入：UPDATE user SET authentication_string=PASSWORD('123456') where USER='root';FLUSH PRIVILEGES; 修改密码后，终端下执行：mysql -uroot -p 根据提示，输入密码进度 mysql 命令行状态。 如果你在其他机子上连接该数据库机子报：Access denied for user 'root'@'localhost' (using password: YES) 解决办法： 在终端中执行（CentOS 6）：service mysql stop 在终端中执行（CentOS 7）：systemctl stop mysql 在终端中执行（前面添加的 Linux 用户 mysql 必须有存在）：/usr/local/mysql/bin/mysqld --skip-grant-tables --user=mysql 此时 MySQL 服务会一直处于监听状态，你需要另起一个终端窗口来执行接下来的操作 在终端中执行：mysql -u root mysql 或者：mysql -h 127.0.0.1 -u root -P 3306 -p 把密码改为：123456，进入 MySQL 命令后执行：UPDATE user SET Password=PASSWORD('123456') where USER='root';FLUSH PRIVILEGES; 然后重启 MySQL 服务（CentOS 6）：service mysql restart 然后重启 MySQL 服务（CentOS 7）：systemctl restart mysql 连接报错：\"Host '192.168.1.133' is not allowed to connect to this MySQL server\" 不允许除了 localhost 之外去连接，解决办法，进入 MySQL 命令行，输入下面内容： 开发机设置允许任何机子访问： vim /etc/my.cnf 中不能有：bind-address = 127.0.0.1 配置：GRANT ALL PRIVILEGES ON *.* TO '数据库用户名'@'%' IDENTIFIED BY '数据库用户名的密码' WITH GRANT OPTION; 更新配置：flush privileges; 生产机设置只运行本机访问： vim /etc/my.cnf 中必须有：bind-address = 127.0.0.1 配置：GRANT ALL PRIVILEGES ON *.* TO '数据库用户名'@'127.0.0.1' IDENTIFIED BY '数据库用户名的密码' WITH GRANT OPTION; 更新配置：flush privileges; 修改密码报错：Your password does not satisfy the current policy requirements MySQL 5.7 安全性要求更高，需要这么做： set global validate_password_policy=0; #密码强度设为最低等级 set global validate_password_length=6; #密码允许最小长度为6 set password = password('新密码'); FLUSH PRIVILEGES; MySQL 5.7 报错内容： Expression #1 of ORDER BY clause is not in GROUP BY clause and contains nonaggregated column 'youmeek.nm.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 查下自己的模式：select version(), @@sql_mode; 解决办法，修改 my.cnf，增加这一行： sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION; 小内存机子，MySQL 频繁挂掉解决办法（1G + CentOS 7.4） 保存系统日志到本地进行查看：cd /var/log/ && sz messages 其中可以看到这样的几句话（可以知道内存不够了）： Jul 6 21:49:14 VM_123_201_centos kernel: Out of memory: Kill process 19452 (httpd) score 36 or sacrifice child Jul 6 21:49:14 VM_123_201_centos kernel: Killed process 19452 (httpd) total-vm:516404kB, anon-rss:36088kB, file-rss:168kB, shmem-rss:12kB 对于 1G 的内存 MySQL（5.6.35），建议重点下面配置： [mysqld] table_definition_cache=400 table_open_cache=256 innodb_buffer_pool_size = 64M max_connections = 100 增加 swap（云服务基本都是没 swap 的） 分别执行下面 shell 命令： dd if=/dev/zero of=/swapfile bs=1M count=1024 mkswap /swapfile swapon /swapfile 修改配置文件：vim /etc/fstab 添加这句在文件最后一行：/swapfile swap swap defauluts 0 0 重启机子：reboot MySQL 主从复制 环境说明和注意点 假设有两台服务器，一台做主，一台做从 MySQL 主信息： IP：12.168.1.113 端口：3306 MySQL 从信息： IP：12.168.1.115 端口：3306 注意点 主 DB server 和从 DB server 数据库的版本一致 主 DB server 和从 DB server 数据库数据一致 主 DB server 开启二进制日志，主 DB server 和从 DB server 的 server-id 都必须唯一 优先操作： 把主库的数据库复制到从库并导入 主库机子操作 主库操作步骤 创建一个目录：mkdir -p /usr/local/mysql/data/mysql-bin 主 DB 开启二进制日志功能：vim /etc/my.cnf， 添加一行：log-bin = /usr/local/mysql/data/mysql-bin 指定同步的数据库，如果不指定则同步全部数据库，其中 ssm 是我的数据库名：binlog-do-db=ssm 主库关掉慢查询记录，用 SQL 语句查看当前是否开启：SHOW VARIABLES LIKE '%slow_query_log%';，如果显示 OFF 则表示关闭，ON 表示开启 重启主库 MySQL 服务 进入 MySQL 命令行状态，执行 SQL 语句查询状态：SHOW MASTER STATUS; 在显示的结果中，我们需要记录下 File 和 Position 值，等下从库配置有用。 设置授权用户 slave01 使用 123456 密码登录主库，这里 @ 后的 IP 为从库机子的 IP 地址，如果从库的机子有多个，我们需要多个这个 SQL 语句。 grant replication slave on *.* to 'slave01'@'192.168.1.135' identified by '123456'; flush privileges; 从库机子操作 从库操作步骤 从库开启慢查询记录，用 SQL 语句查看当前是否开启：SHOW VARIABLES LIKE '%slow_query_log%';，如果显示 OFF 则表示关闭，ON 表示开启。 测试从库机子是否能连上主库机子：mysql -h 192.168.1.105 -u slave01 -p，必须要连上下面的操作才有意义。 由于不能排除是不是系统防火墙的问题，所以建议连不上临时关掉防火墙：service iptables stop 或是添加防火墙规则： 添加规则：iptables -I INPUT -p tcp -m tcp --dport 3306 -j ACCEPT 保存规则：service iptables save 重启 iptables：service iptables restart 修改配置文件：vim /etc/my.cnf，把 server-id 改为跟主库不一样 在进入 MySQL 的命令行状态下，输入下面 SQL： CHANGE MASTER TO master_host='192.168.1.113', master_user='slave01', master_password='123456', master_port=3306, master_log_file='mysql3306-bin.000006',>>>这个值复制刚刚让你记录的值 master_log_pos=1120;>>>这个值复制刚刚让你记录的值 执行该 SQL 语句，启动 slave 同步：START SLAVE; 执行该 SQL 语句，查看从库机子同步状态：SHOW SLAVE STATUS; 在查看结果中必须下面两个值都是 Yes 才表示配置成功： Slave_IO_Running:Yes 如果不是 Yes 也不是 No，而是 Connecting，那就表示从机连不上主库，需要你进一步排查连接问题。 Slave_SQL_Running:Yes 如果你的 Slave_IO_Running 是 No，一般如果你是在虚拟机上测试的话，从库的虚拟机是从主库的虚拟机上复制过来的，那一般都会这样的，因为两台的 MySQL 的 UUID 值一样。你可以检查从库下的错误日志：cat /usr/local/mysql/data/mysql-error.log 如果里面提示 uuid 错误，你可以编辑从库的这个配置文件：vim /usr/local/mysql/data/auto.cnf，把配置文件中的：server-uuid 值随便改一下，保证和主库是不一样即可。 资料 http://www.cnblogs.com/xiongpq/p/3384681.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Mysql-Optimize.html":{"url":"Linux-Tutorial/markdown-file/Mysql-Optimize.html","title":"MySQL 优化","keywords":"","body":"MySQL 优化 下面说的优化基于 MySQL 5.6，理论上 5.5 之后的都算适用，具体还是要看官网 优秀材料 https://notes.diguage.com/mysql/ https://mp.weixin.qq.com/s/Wc6Gw6S5xMy2DhTCrogxVQ <> <> <> 服务状态查询 查看当前数据库的状态，常用的有： 查看系统状态：SHOW STATUS; 查看刚刚执行 SQL 是否有警告信息：SHOW WARNINGS; 查看刚刚执行 SQL 是否有错误信息：SHOW ERRORS; 查看已经连接的所有线程状况：SHOW FULL PROCESSLIST; 输出参数说明：http://www.ibloger.net/article/2519.html 可以结束某些连接：kill id值 查看当前连接数量：SHOW STATUS LIKE 'max_used_connections'; 查看变量，在 my.cnf 中配置的变量会在这里显示：SHOW VARIABLES; 查询慢 SQL 配置：show variables like 'slow%'; 开启慢 SQL：set global slow_query_log='ON' 查询慢 SQL 秒数值：show variables like 'long%'; 调整秒速值：set long_query_time=1; 查看当前MySQL 中已经记录了多少条慢查询，前提是配置文件中开启慢查询记录了. SHOW STATUS LIKE '%slow_queries%'; 查询当前MySQL中查询、更新、删除执行多少条了，可以通过这个来判断系统是侧重于读还是侧重于写，如果是写要考虑使用读写分离。 SHOW STATUS LIKE '%Com_select%'; SHOW STATUS LIKE '%Com_insert%'; SHOW STATUS LIKE '%Com_update%'; SHOW STATUS LIKE '%Com_delete%'; 如果 rollback 过多，说明程序肯定哪里存在问题 SHOW STATUS LIKE '%Com_rollback%'; 显示MySQL服务启动运行了多少时间，如果MySQL服务重启，该时间重新计算，单位秒 SHOW STATUS LIKE 'uptime'; 显示查询缓存的状态情况 SHOW STATUS LIKE 'qcache%'; PS.下面的解释，我目前不肯定是对，还要再找下资料： http://dba.stackexchange.com/questions/33811/qcache-free-memory-not-full-yet-i-get-alot-of-qcache-lowmem-prunes https://dev.mysql.com/doc/refman/5.7/en/query-cache-status-and-maintenance.html https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html http://www.111cn.net/database/110/c0c88da67b9e0c6c8fabfbcd6c733523.htm Qcache_free_blocks，缓存中相邻内存块的个数。数目大说明可能有碎片。如果数目比较大，可以执行： flush query cache; 对缓存中的碎片进行整理，从而得到一个空闲块。 Qcache_free_memory，缓存中的空闲内存大小。如果 Qcache_free_blocks 比较大，说明碎片严重。 如果 Qcache_free_memory 很小，说明缓存不够用了。 Qcache_hits，每次查询在缓存中命中时就增大该值。 Qcache_inserts，每次查询，如果没有从缓存中找到数据，这里会增大该值 Qcache_lowmem_prunes，因内存不足删除缓存次数，缓存出现内存不足并且必须要进行清理, 以便为更多查询提供空间的次数。返个数字最好长时间来看；如果返个数字在不断增长，就表示可能碎片非常严重，或者缓存内存很少。 Qcache_not_cached # 没有进行缓存的查询的数量，通常是这些查询未被缓存或其类型不允许被缓存 Qcache_queries_in_cache # 在当前缓存的查询（和响应）的数量。 Qcache_total_blocks #缓存中块的数量。 查询哪些表在被使用，是否有锁表：SHOW OPEN TABLES WHERE In_use > 0; 查询 innodb 状态（输出内容很多）：SHOW ENGINE INNODB STATUS; 锁性能状态：SHOW STATUS LIKE 'innodb_row_lock_%'; Innodb_row_lock_current_waits：当前等待锁的数量 Innodb_row_lock_time：系统启动到现在、锁定的总时间长度 Innodb_row_lock_time_avg：每次平均锁定的时间 Innodb_row_lock_time_max：最长一次锁定时间 Innodb_row_lock_waits：系统启动到现在、总共锁定次数 帮我们分析表，并提出建议：select * from my_table procedure analyse(); 系统表 当前运行的所有事务：select * from information_schema.INNODB_TRX; 当前事务出现的锁：select * from information_schema.INNODB_LOCKS; 锁等待的对应关系：select * from information_schema.INNODB_LOCK_WAITS; otpimizer trace 作用：输入我们想要查看优化过程的查询语句，当该查询语句执行完成后，就可以到 information_schema 数据库下的OPTIMIZER_TRACE表中查看 mysql 自己帮我们的完整优化过程 是否打开（默认都是关闭）：SHOW VARIABLES LIKE 'optimizer_trace'; one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示，不适合人阅读，所以我们就保持其默认值为off吧。 打开配置：SET optimizer_trace=\"enabled=on\"; 关闭配置：SET optimizer_trace=\"enabled=off\"; 查询优化结果：SELECT * FROM information_schema.OPTIMIZER_TRACE; 我们所说的基于成本的优化主要集中在optimize阶段，对于单表查询来说，我们主要关注optimize阶段的\"rows_estimation\"这个过程，这个过程深入分析了对单表查询的各种执行方案的成本； 对于多表连接查询来说，我们更多需要关注\"considered_execution_plans\"这个过程，这个过程里会写明各种不同的连接方式所对应的成本。 反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用EXPLAIN语句所展现出的那种方案。 如果有小伙伴对使用EXPLAIN语句展示出的对某个查询的执行计划很不理解，大家可以尝试使用optimizer trace功能来详细了解每一种执行方案对应的成本，相信这个功能能让大家更深入的了解MySQL查询优化器。 查询优化(EXPLAIN 查看执行计划) 使用 EXPLAIN 进行 SQL 语句分析：EXPLAIN SELECT * FROM sys_user;，效果如下： id|select_type|table |partitions|type|possible_keys|key|key_len|ref|rows|filtered|Extra| --|-----------|--------|----------|----|-------------|---|-------|---|----|--------|-----| 1|SIMPLE |sys_user| |ALL | | | | | 2| 100| | 简单描述 id：在一个大的查询语句中每个 SELECT 关键字都对应一个唯一的id select_type：SELECT 关键字对应的那个查询的类型 table：表名 partitions：匹配的分区信息 type：针对单表的访问方法 possible_keys：可能用到的索引 key：实际上使用的索引 key_len：实际使用到的索引长度 ref：当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows：预估的需要读取的记录条数 filtered：某个表经过搜索条件过滤后剩余记录条数的百分比 Extra：一些额外的信息 有多个结果的场景分析 有子查询的一般都会有多个结果，id 是递增值。但是，有些场景查询优化器可能对子查询进行重写，转换为连接查询。所以有时候 id 就不是自增值。 对于连接查询一般也会有多个接口，id 可能是相同值，相同值情况下，排在前面的记录表示驱动表，后面的表示被驱动表 UNION 场景会有 id 为 NULL 的情况，这是一个去重后临时表，合并多个结果集的临时表。但是，UNION ALL 不会有这种情况，因为这个不需要去重。 根据具体的描述： id，该列表示当前结果序号 select_type，表示 SELECT 语句的类型，有下面几种 SIMPLE：表示简单查询，其中不包括 UNION 查询和子查询 PRIMARY：对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY UNION：对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION UNION RESULT：MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT SUBQUERY：如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY DEPENDENT SUBQUERY：如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY DEPENDENT UNION：在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION DERIVED：对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED MATERIALIZED：当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED 还有其他一些 table，表名或者是子查询的一个结果集 type，表示表的链接类型，分别有（以下的连接类型的顺序是从最佳类型到最差类型）（这个属性重要）： 性能好： system：当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system，平时不会出现，这个也可以忽略不计。 const：当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const，常用于 PRIMARY KEY 或者 UNIQUE 索引的查询，可理解为 const 是最优化的。 eq_ref：在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref ref：当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref。ref 可用于 = 或 操作符的带索引的列。 ref_or_null：当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null 性能较差： index_merge：该联接类型表示使用了索引合并优化方法。在这种情况下，key 列包含了使用的索引的清单，key_len 包含了使用的索引的最长的关键元素。 unique_subquery：类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery index_subquery：index_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引 range：只检索给定范围的行, 使用一个索引来选择行。 index：该联接类型与 ALL 相同, 除了只有索引树被扫描。这通常比 ALL 快, 因为索引文件通常比数据文件小。 再一次强调，对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些 性能最差： ALL：对于每个来自于先前的表的行组合, 进行完整的表扫描。（性能最差） possible_keys，指出 MySQL 能使用哪个索引在该表中找到行。如果该列为 NULL，说明没有使用索引，可以对该列创建索引来提供性能。（这个属性重要） possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引。 key，显示 MySQL 实际决定使用的键 (索引)。如果没有选择索引, 键是 NULL。（这个属性重要） 不过有一点比较特别，就是在使用index访问方法来查询某个表时，possible_keys列是空的，而key列展示的是实际使用到的索引 key_len，表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度。如果键是可以为 NULL, 则长度多 1。 ref，显示使用哪个列或常数与 key 一起从表中选择行。 rows，显示 MySQL 认为它执行查询时必须检查的行数。（这个属性重要） Extra，该列包含 MySQL 解决查询的详细信息： Distinct MySQL 发现第 1 个匹配行后, 停止为当前的行组合搜索更多的行。 Not exists 当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息 range checked for each record (index map: #) MySQL 没有发现好的可以使用的索引, 但发现如果来自前面的表的列值已知, 可能部分索引可以使用。 Using filesort 有一些情况下对结果集中的记录进行排序是可以使用到索引的 需要注意的是，如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为使用索引进行排序。 Using temporary 在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示 如果我们并不想为包含GROUP BY子句的查询进行排序，需要我们显式的写上：ORDER BY NULL 执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表 Using join buffer (Block Nested Loop) 在连接查询执行过程过，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法 Using where 当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息 当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息 Using sort_union(...), Using union(...), Using intersect(...) 如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；如果出现了Using union(...)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。 Using index condition 有些搜索条件中虽然出现了索引列，但却不能使用到索引 Using index 当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息 Using index for group-by 类似于访问表的 Using index 方式,Using index for group-by 表示 MySQL 发现了一个索引, 可以用来查 询 GROUP BY 或 DISTINCT 查询的所有列, 而不要额外搜索硬盘访问实际的表。 查询不走索引优化 WHERE字句的查询条件里有不等于号（WHERE column!=…），MYSQL将无法使用索引 类似地，如果WHERE字句的查询条件里使用了函数（如：WHERE DAY(column)=…），MYSQL将无法使用索引 在JOIN操作中（需要从多个数据表提取数据时），MYSQL只有在主键和外键的数据类型相同时才能使用索引，否则即使建立了索引也不会使用 如果WHERE子句的查询条件里使用了比较操作符LIKE和REGEXP，MYSQL只有在搜索模板的第一个字符不是通配符的情况下才能使用索引。比如说，如果查询条件是LIKE 'abc%',MYSQL将使用索引；如果条件是LIKE '%abc'，MYSQL将不使用索引。 在ORDER BY操作中，MYSQL只有在排序条件不是一个查询条件表达式的情况下才使用索引。尽管如此，在涉及多个数据表的查询里，即使有索引可用，那些索引在加快ORDER BY操作方面也没什么作用。 如果某个数据列里包含着许多重复的值，就算为它建立了索引也不会有很好的效果。比如说，如果某个数据列里包含了净是些诸如“0/1”或“Y/N”等值，就没有必要为它创建一个索引。 索引有用的情况下就太多了。基本只要建立了索引，除了上面提到的索引不会使用的情况下之外，其他情况只要是使用在WHERE条件里，ORDER BY 字段，联表字段，一般都是有效的。 建立索引要的就是有效果。 不然还用它干吗？ 如果不能确定在某个字段上建立的索引是否有效果，只要实际进行测试下比较下执行时间就知道。 如果条件中有or(并且其中有or的条件是不带索引的)，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)。注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 子查询优化 MySQL 从 4.1 版本开始支持子查询，使用子查询进行 SELECT 语句嵌套查询，可以一次完成很多逻辑上需要多个步骤才能完成的 SQL 操作。 子查询虽然很灵活，但是执行效率并不高。 执行子查询时，MYSQL 需要创建临时表，查询完毕后再删除这些临时表，所以，子查询的速度会受到一定的影响。 优化： 可以使用连接查询（JOIN）代替子查询，连接查询时不需要建立临时表，其速度比子查询快。 其他查询优化 关联查询过程 确保 ON 或者 using子句中的列上有索引 确保任何的 groupby 和 orderby 中的表达式只涉及到一个表中的列。 count()函数优化 count()函数有一点需要特别注意：它是不统计值为NULL的字段的！所以：不能指定查询结果的某一列，来统计结果行数。即 count(xx column) 不太好。 如果想要统计结果集，就使用 count(*)，性能也会很好。 分页查询（数据偏移量大的场景） 不允许跳页，只能上一页或者下一页 使用 where 加上上一页 ID 作为条件(具体要看 explain 分析效果)：select xxx,xxx from test_table where id 创表原则 所有字段均定义为 NOT NULL ，除非你真的想存 Null。因为表内默认值 Null 过多会影响优化器选择执行计划 建立索引原则 使用区分度高的列作为索引，字段不重复的比例，区分度越高，索引树的分叉也就越多，一次性找到的概率也就越高。 尽量使用字段长度小的列作为索引 使用数据类型简单的列（int 型，固定长度） 选用 NOT NULL 的列。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值。 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。这样也可避免索引重复。 数据库结构优化 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 插入数据的优化（适用于 InnoDB） 插入数据时，影响插入速度的主要是索引、唯一性校验、一次插入的数据条数等。 开发环境情况下的考虑： 开发场景中，如果需要初始化数据，导入数据等一些操作，而且是开发人员进行处理的，可以考虑在插入数据之前，先禁用整张表的索引， 禁用索引使用 SQL：ALTER TABLE table_name DISABLE KEYS; 当导入完数据之后，重新让MySQL创建索引，并开启索引：ALTER TABLE table_name ENABLE KEYS; 如果表中有字段是有唯一性约束的，可以先禁用，然后在开启： 禁用唯一性检查的语句：SET UNIQUE_CHECKS = 0; 开启唯一性检查的语句：SET UNIQUE_CHECKS = 1; 禁用外键检查（建议还是少量用外键，而是采用代码逻辑来处理） 插入数据之前执行禁止对外键的检查，数据插入完成后再恢复，可以提供插入速度。 禁用：SET foreign_key_checks = 0; 开启：SET foreign_key_checks = 1; 使用批量插入数据 禁止自动提交 插入数据之前执行禁止事务的自动提交，数据插入完成后再恢复，可以提供插入速度。 禁用：SET autocommit = 0; 开启：SET autocommit = 1; 插入数据之前执行禁止对外键的检查，数据插入完成后再恢复 禁用：SET foreign_key_checks = 0; 开启：SET foreign_key_checks = 1; 服务器优化 好硬件大家都知道，这里没啥好说，如果是 MySQL 单独一台机子，那机子内存可以考虑分配 60%~70% 给 MySQL 通过优化 MySQL 的参数可以提高资源利用率，从而达到提高 MySQL 服务器性能的目的。 可以看我整理的这篇文章：https://github.com/judasn/Linux-Tutorial/blob/master/MySQL-Settings/MySQL-5.6/1G-Memory-Machine/my-for-comprehensive.cnf 由于 binlog 日志的读写频繁，可以考虑在 my.cnf 中配置，指定这个 binlog 日志到一个 SSD 硬盘上。 锁相关 InnoDB支持事务；InnoDB 采用了行级锁。也就是你需要修改哪行，就可以只锁定哪行。 在 Mysql 中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql 语句操作了主键索引，Mysql 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 InnoDB 行锁是通过给索引项加锁实现的，如果没有索引，InnoDB 会通过隐藏的聚簇索引来对记录加锁。也就是说：如果不通过索引条件检索数据，那么InnoDB将对表中所有数据加锁，实际效果跟表锁一样。因为没有了索引，找到某一条记录就得扫描全表，要扫描全表，就得锁定表。 数据库的增删改操作默认都会加排他锁，而查询不会加任何锁。 排他锁：对某一资源加排他锁，自身可以进行增删改查，其他人无法进行任何操作。语法为： select * from table for update; 共享锁：对某一资源加共享锁，自身可以读该资源，其他人也可以读该资源（也可以再继续加共享锁，即 共享锁可多个共存），但无法修改。 要想修改就必须等所有共享锁都释放完之后。语法为： select * from table lock in share mode; 资料 https://my.oschina.net/jsan/blog/653697 https://blog.imdst.com/mysql-5-6-pei-zhi-you-hua/ https://mp.weixin.qq.com/s/qCRfxIr1RoHd9i8-Hk8iuQ https://yancg.cn/detail?id=3 https://www.jianshu.com/p/1ab3cd5551b9 http://blog.brucefeng.info/post/mysql-index-query?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io https://juejin.im/book/5bffcbc9f265da614b11b731 <> <> <> <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Mysql-Test.html":{"url":"Linux-Tutorial/markdown-file/Mysql-Test.html","title":"MySQL 测试","keywords":"","body":"MySQL 测试 mysqlslap 工具 工具的官网说明：https://dev.mysql.com/doc/refman/5.5/en/mysqlslap.html 可能会遇到的报错： 报：mysqlslap: Error when connecting to server: Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2) 可以这样解决：ln -s /usr/program/mysql/data/mysql.sock /tmp/mysql.sock，主要是我的 sock 文件位置是自己的配置的，跟 mysqlslap 默认去读的地方不一样。 进行基准测试： 先做软链接：ln -s /usr/program/mysql/bin/mysqlslap /usr/bin 自动生成简单测试数据并测试：mysqlslap --defaults-file=/etc/my.cnf -a --auto-generate-sql-load-type=mixed --auto-generate-sql-add-autoincrement --engine=innodb --concurrency=50,100 --number-of-queries=1000 --iterations=2 --debug-info -uroot -p123456 该语句表示测试并发为50和100的情况，进行1000次访问(该值一般这样预估出来：并发客户数×每客户查询次数)。这样的测试方法迭代2次，最终显示最大、最小、平均值 其中：-a，表示自动生成要测试的数据，等同于：--auto-generate-sql 其中：--debug-info，代表要额外输出 CPU 以及内存的相关信息。 自动生成复杂测试数据并测试：mysqlslap --defaults-file=/etc/my.cnf --concurrency=50,100,200 --iterations=2 --number-int-cols=7 --number-char-cols=13 --auto-generate-sql --auto-generate-sql-add-autoincrement --auto-generate-sql-load-type=mixed --engine=innodb --number-of-queries=1000 --debug-info -S /tmp/mysql.sock -uroot -p123456 -number-int-cols=7 表示生成的表中必须有 7 个 int 类型的列 -number-char-cols=13 表示生成的表中必须有 13 个 char 类型的列 实际场景请求数较大的时候测试：mysqlslap --defaults-file=/etc/my.cnf --concurrency=50,100,200,500,1000 --iterations=10 --number-int-cols=7 --number-char-cols=13 --auto-generate-sql --auto-generate-sql-add-autoincrement --auto-generate-sql-load-type=mixed --engine=innodb --number-of-queries=10000 --debug-info -S /tmp/mysql.sock -uroot -p123456 测试结果含义解释： Average number of XXXXXXXX：运行所有语句的平均秒数 Minimum number of XXXXXXXX：运行所有语句的最小秒数 Maximum number of XXXXXXXX：运行所有语句的最大秒数 Number of clients XXXXXXXX：客户端数量 Average number of queries per client XXXXXXXX：每个客户端运行查询的平均数。其中这个数和上面的数相乘就等于number-of-queries 对自己的数据库进行测试： 数据库：youmeek_nav 简单测试语句：mysqlslap --defaults-file=/etc/my.cnf --create-schema=youmeek_nav --query=\"SELECT * FROM nav_url;\" --debug-info -uroot -p123456 复杂测试语句：假设我把有3条sql要测试，我把这三条写入到一个 test.sql 文件中，3条sql用分号隔开，文件内容为：SELECT * FROM sys_user;SELECT * FROM nav_column;SELECT * FROM nav_url; 那测试语句可以这样写：mysqlslap --defaults-file=/etc/my.cnf --create-schema=youmeek_nav --query=\"/opt/test.sql\" --delimiter=\";\" --debug-info -uroot -p123456 --delimiter=”;” 表示文件中不同 sql 的分隔符是什么 其他一些参数： mysqlslap --help 查看所有参数 --auto-generate-sql-load-type=XXX，XXX 代表要测试的是读还是写还是两者混合，该值分别有：read,write,update,mixed，默认是 mixed --auto-generate-sql-add-autoincrement 代表对生成的表自动添加 auto_increment 列 --debug-info 代表要额外输出 CPU 以及内存的相关信息。 --only-print 打印压力测试的时候 mysqlslap 到底做了什么事，通过 sql 语句方式告诉我们。 sysbench 工具 工具的官网说明：https://launchpad.net/sysbench 开源地址：https://github.com/akopytov/sysbench 安装 当前（201703）最新版本为：1.0.3，下面的操作也都是基于此版本，网络上的资料很多都是 0.4 和 0.5 不支持本文的语法。 安装编译相关工具包：yum -y install automake libtool 下载：https://github.com/akopytov/sysbench/releases 假设我这边下载下来的文件名为：sysbench-1.0.3.zip 我的 MySQL 安装路径为：/usr/program/mysql include 目录位置：/usr/program/mysql/include libs 目录位置：/usr/program/mysql/lib 设置 MySQL 包路径变量：export LD_LIBRARY_PATH=/usr/program/mysql/lib/ 解压压缩包：unzip sysbench-1.0.3.zip 开始编译安装： cd sysbench-1.0.3 ./autogen.sh ./configure --with-mysql-includes=/usr/program/mysql/include --with-mysql-libs=/usr/program/mysql/lib/ make make install 测试是否安装成功：sysbench --version 安装完之后在这个目录下有一些 lua 测试脚本：cd /usr/local/share/sysbench，等下测试的时候需要指定这些脚本位置，用这些脚本测试的。 默认这些脚本生成的数据都是 10000 个，如果你想要更多，需要修改：vim /usr/local/share/sysbench/oltp_common.lua 文件。常修改的参数： tables，生成多少张表 table_size，每张表多少记录数 开始测试 做不同的类型测试之前，最好都重启下 MySQL 创建一个数据库，名字为：sbtest select 测试： 准备测试数据：sysbench /usr/local/share/sysbench/oltp_point_select.lua --threads=15 --report-interval=10 --time=120 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 prepare 开始测试：sysbench /usr/local/share/sysbench/oltp_point_select.lua --threads=15 --report-interval=10 --time=120 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 run 清除测试数据：sysbench /usr/local/share/sysbench/oltp_point_select.lua --threads=15 --report-interval=10 --time=120 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 cleanup 读写测试： 读写测试我开了线程比较多，也修改了 oltp_common.lua 内容，有可能会报：MySQL error: 1461 \"Can't create more than max_prepared_stmt_count statements，那你需要在 MySQL 中执行这句临时设置 SQL：SET GLOBAL max_prepared_stmt_count=100000; 准备测试数据：sysbench /usr/local/share/sysbench/oltp_read_write.lua --threads=100 --report-interval=10 --time=100 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 prepare 开始测试：sysbench /usr/local/share/sysbench/oltp_read_write.lua --threads=100 --report-interval=10 --time=100 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 run 清除测试数据：sysbench /usr/local/share/sysbench/oltp_read_write.lua --threads=100 --report-interval=10 --time=100 --mysql-user=root --mysql-password=123456 --mysql-host=127.0.0.1 --mysql-port=3306 cleanup 参数说明： --threads=15 表示发起 15 个并发连接 --report-interval=10 表示控制台每 10 秒输出一次测试进度报告 --time=120 总的测试时长为 120 秒 --max-requests=0 表示总请求数为 0，因为上面已经定义了总执行时长，所以总请求数可以设定为 0；也可以只设定总请求数，不设定执行时长 --percentile=99 表示设定采样比例，即丢弃1%的长请求，在剩余的99%里取最大值。默认是 95%， 测试报告 Running the test with following options: Number of threads: 15 Report intermediate results every 10 second(s) Initializing random number generator from current time Initializing worker threads... Threads started! [ 10s ] thds: 15 tps: 337.43 qps: 6773.72 (r/w/o: 4745.03/1351.92/676.76) lat (ms,95%): 73.13 err/s: 0.40 reconn/s: 0.00 [ 20s ] thds: 15 tps: 340.12 qps: 6813.82 (r/w/o: 4772.12/1361.06/680.63) lat (ms,95%): 71.83 err/s: 0.40 reconn/s: 0.00 [ 30s ] thds: 15 tps: 344.78 qps: 6897.36 (r/w/o: 4828.36/1379.23/689.77) lat (ms,95%): 71.83 err/s: 0.20 reconn/s: 0.00 [ 40s ] thds: 15 tps: 343.32 qps: 6876.75 (r/w/o: 4815.15/1374.47/687.14) lat (ms,95%): 71.83 err/s: 0.60 reconn/s: 0.00 [ 50s ] thds: 15 tps: 342.80 qps: 6864.76 (r/w/o: 4806.67/1371.89/686.20) lat (ms,95%): 73.13 err/s: 0.50 reconn/s: 0.00 [ 60s ] thds: 15 tps: 347.90 qps: 6960.74 (r/w/o: 4873.93/1390.71/696.10) lat (ms,95%): 70.55 err/s: 0.30 reconn/s: 0.00 [ 70s ] thds: 15 tps: 346.70 qps: 6942.39 (r/w/o: 4859.29/1389.30/693.80) lat (ms,95%): 70.55 err/s: 0.40 reconn/s: 0.00 [ 80s ] thds: 15 tps: 345.60 qps: 6914.88 (r/w/o: 4841.48/1382.00/691.40) lat (ms,95%): 70.55 err/s: 0.20 reconn/s: 0.00 [ 90s ] thds: 15 tps: 341.10 qps: 6830.31 (r/w/o: 4782.20/1365.40/682.70) lat (ms,95%): 74.46 err/s: 0.50 reconn/s: 0.00 [ 100s ] thds: 15 tps: 341.20 qps: 6829.33 (r/w/o: 4782.12/1364.41/682.80) lat (ms,95%): 74.46 err/s: 0.40 reconn/s: 0.00 [ 110s ] thds: 15 tps: 343.40 qps: 6875.79 (r/w/o: 4812.29/1376.50/687.00) lat (ms,95%): 71.83 err/s: 0.20 reconn/s: 0.00 [ 120s ] thds: 15 tps: 347.00 qps: 6943.51 (r/w/o: 4862.40/1386.70/694.40) lat (ms,95%): 71.83 err/s: 0.40 reconn/s: 0.00 SQL statistics: queries performed: read: 577836 --读总数 write: 164978 --写总数 other: 82503 --其他操作(CURD之外的操作，例如COMMIT) total: 825317 --全部总数 transactions: 41229 (343.51 per sec.) --总事务数(每秒事务数，这个每秒事务数也就是：TPS 吞吐量) queries: 825317 (6876.33 per sec.) --查询总数(查询次数) ignored errors: 45 (0.37 per sec.) --总忽略错误总数(每秒忽略错误次数) reconnects: 0 (0.00 per sec.) --重连总数(每秒重连次数) General statistics: total time: 120.0214s --总耗时 total number of events: 41229 --共发生多少事务数 Latency (ms): min: 7.19 --最小耗时 avg: 43.66 --平均耗时 max: 162.82 --最长耗时 95th percentile: 71.83 --超过95%平均耗时 sum: 1799860.45 Threads fairness: events (avg/stddev): 2748.6000/132.71 --总处理事件数/标准偏差 execution time (avg/stddev): 119.9907/0.00 --总执行时间/标准偏差 QPS 和 TPS 和说明 基本概念 QPS：Queries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。 TPS是TransactionsPerSecond的缩写，也就是事务数/秒。它是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数，最终利用这些信息来估计得分。客户机使用加权协函数平均方法来计算客户机的得分，测试软件就是利用客户机的这些信息使用加权协函数平均方法来计算服务器端的整体TPS得分。 QPS（TPS）= 并发数/平均响应时间 或者 并发数 = QPS平均响应时间 *这里响应时间的单位是秒 举例，我们一个HTTP请求的响应时间是20ms，在10个并发的情况下，QPS就是 QPS=10*1000/20=500。 这里有个关键的点就是QPS一定是跟并发数联系在一起的，离开并发数谈QPS是没意义的。 QPS、TPS和性能的关系 一个系统吞吐量通常由QPS（TPS）、并发数两个因素决定，每套系统这两个值都有一个相对极限值，在应用场景访问压力下，只要某一项达到系统最高值，系统的吞吐量就上不去了，如果压力继续增大，系统的吞吐量反而会下降，原因是系统超负荷工作，上下文切换、内存等等其它消耗导致系统性能下降。 开始，系统只有一个用户，CPU工作肯定是不饱合的。一方面该服务器可能有多个cpu，但是只处理单个进程，另一方面，在处理一个进程中，有些阶段可能是IO阶段，这个时候会造成CPU等待，但是有没有其他请 求进程可以被处理）。随着并发用户数的增加，CPU利用率上升，QPS相应也增加（公式为QPS=并发用户数/平均响应时间。）随着并发用户数的增加，平均响应时间也在增加，而且平均响应时间的增加是一个指数增加曲线。而当并发数增加到很大时，每秒钟都会有很多请求需要处理，会造成进程（线程）频繁切换，反正真正用于处理请求的时间变少，每秒能够处 理的请求数反而变少，同时用户的请求等待时间也会变大，甚至超过用户的心理底线。 ） 结论 我们对单台服务器进行压测有了性能测试数据以后，我们可以根据业务上能接受最大客户响应时间对应到相应的QPS数，从而计算出需要的服务器的数量。举例来说，响应时间10ms和1000ms对通过浏览器的客户是没有明显体验差别的，基于1000ms估算服务器的数量我们的成本会降低很多。 每天300wPV的在单台机器上，这台机器需要多少QPS？对于这样的问题，假设每天80%的访问集中在20%的时间里，这20%时间叫做峰值时间。( 3000000 0.8 ) / (3600 24 * 0.2 ) = 139 (QPS). 如果一台机器的QPS是58，需要几台机器来支持？答：139 / 58 = 3 Percona TPCC-MySQL 测试工具（优先推荐） 可以较好地模拟真实测试结果数据 官网主页：https://github.com/Percona-Lab/tpcc-mysql TPC-C 是专门针对联机交易处理系统（OLTP系统）的规范，一般情况下我们也把这类系统称为业务处理系统。 TPC-C是TPC(Transaction Processing Performance Council)组织发布的一个测试规范，用于模拟测试复杂的在线事务处理系统。其测试结果包括每分钟事务数(tpmC)，以及每事务的成本(Price/tpmC)。 在进行大压力下MySQL的一些行为时经常使用。 安装 先确定本机安装过 MySQL 并且安装过：yum install mysql-devel git clone https://github.com/Percona-Lab/tpcc-mysql cd tpcc-mysql/src make 如果make没报错，就会在tpcc-mysql 根目录文件夹下生成tpcc二进制命令行工具tpcc_load、tpcc_start 测试的几个表介绍 tpcc-mysql的业务逻辑及其相关的几个表作用如下： New-Order：新订单，主要对应 new_orders 表 Payment：支付，主要对应 orders、history 表 Order-Status：订单状态，主要对应 orders、order_line 表 Delivery：发货，主要对应 order_line 表 Stock-Level：库存，主要对应 stock 表 其他相关表： 客户：主要对应customer表 地区：主要对应district表 商品：主要对应item表 仓库：主要对应warehouse表 准备 测试阿里云 ECS 与 RDS 是否相通： 记得在 RDS 添加账号和给账号配置权限，包括：配置权限、数据权限（默认添加账号后都是没有开启的，还要自己手动开启） 还要添加内网 ECS 到 RDS 的白名单 IP 里面 或者在 RDS 上开启外网访问设置，但是也设置 IP 白名单（访问 ip.cn 查看自己的外网 IP 地址，比如：120.85.112.97） RDS 的内网地址和外网地址不一样，要认真看。 ping rm-wz9v0vej02ys79jbj.mysql.rds.aliyuncs.com mysql -h rm-wz9v0vej02ys79jbj.mysql.rds.aliyuncs.com -P 3306 -u myaccount -p 输入密码：Aa123456 创库，名字为：TPCC： CREATE DATABASE TPCC DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 导入项目中的出初始化数据脚本： 创建表：create_table.sql /usr/bin/mysql -h rm-wz9v0vej02ys79jbj.mysql.rds.aliyuncs.com -u myaccount -p tpcc 测试 数据库：阿里云 RDS-MySQL-5.7-2C4G 测试机：阿里云 ECS-4C4G-CentOS7.6 根据测试，不同的 ECS 测试机，不同的 RDS 测试结果有时候差距挺大的，这个很蛋疼。 需要注意的是 tpcc 默认会读取 /var/lib/mysql/mysql.sock 这个 socket 文件。因此，如果你的socket文件不在相应路径的话，可以做个软连接，或者通过TCP/IP的方式连接测试服务器 准备数据： cd /opt/tpcc-mysql ./tpcc_load -h rm-wz9v0vej02ys79jbj.mysql.rds.aliyuncs.com -P 3306 -d TPCC -u myaccount -p Aa123456 -w 80 -w 80 表示创建 80 个仓库数据 这个过程花费时间还是挺长的，建议测试机是高性能计算型。2CPU 差不多要 8h，你自己估量下。 我这边 RDS 监控中，曲线上每秒 insert 差不多在 2W 差不多，如果你没有这个数，速度可能就很慢了。 我这边差不多用了 2.5h 完成数据准备。 插入过程 RDS-2C4G 的监控情况： CPU利用率 24% 内存 30% ~ 40% （随着数据增加而增大） 连接数：1% IOPS：9% 已使用存储空间：5.5G ~ 10G 要模拟出够真实的数据，仓库不要太少，一般要大于 100， 下面是基于 80 个库的最终数据： select count(*) from customer; 2400000 select count(*) from district; 800 select count(*) from history; 2400000 select count(*) from item; 100000 select count(*) from new_orders; 720000 select count(*) from order_line; 23996450 select count(*) from orders; 2400000 select count(*) from stock; 8000000 select count(*) from warehouse; 80 开始测试： ./tpcc_start -h rm-wz9v0vej02ys79jbj.mysql.rds.aliyuncs.com -P 3306 -d TPCC -u myaccount -p Aa123456 -w 80 -c 200 -r 300 -l 1800 -f /opt/mysql_tpcc_100_20190325 -w 100 表示 100 个仓库数据 -c 200 表示并发 200 个线程 -r 300 表示预热 300 秒 -l 1800 表示持续压测 1800 秒 报表 188.000 TpmC TpmC结果值(每分钟事务数，该值是第一次统计结果中的新订单事务数除以总耗时分钟数，例如本例中是：372/2=186) tpmC值在国内外被广泛用于衡量计算机系统的事务处理能力 RDS-2C4G-80个仓库结果： CPU：100%，内存：34%，连接数：17%，IOPS：62%，磁盘空间：20G 1780, trx: 979, 95%: 1849.535, 99%: 2402.613, max_rt: 3401.947, 986|3248.772, 98|698.821, 103|4202.110, 101|4547.416 1790, trx: 1021, 95%: 1898.903, 99%: 2700.936, max_rt: 3848.142, 999|3150.117, 100|500.740, 102|3600.104, 100|5551.834 1800, trx: 989, 95%: 1899.472, 99%: 2847.899, max_rt: 4455.064, 989|3049.921, 101|699.144, 97|3599.021, 102|5151.141 STOPPING THREADS........................................................................................................................................................................................................ [0] sc:2 lt:174378 rt:0 fl:0 avg_rt: 1192.8 (5) [1] sc:253 lt:173935 rt:0 fl:0 avg_rt: 542.7 (5) [2] sc:4726 lt:12712 rt:0 fl:0 avg_rt: 144.7 (5) [3] sc:0 lt:17435 rt:0 fl:0 avg_rt: 3029.8 (80) [4] sc:0 lt:17435 rt:0 fl:0 avg_rt: 3550.7 (20) in 1800 sec. [0] sc:2 lt:174378 rt:0 fl:0 [1] sc:254 lt:174096 rt:0 fl:0 [2] sc:4726 lt:12712 rt:0 fl:0 [3] sc:0 lt:17437 rt:0 fl:0 [4] sc:0 lt:17435 rt:0 fl:0 (all must be [OK]) [transaction percentage] Payment: 43.45% (>=43.0%) [OK] Order-Status: 4.35% (>= 4.0%) [OK] Delivery: 4.35% (>= 4.0%) [OK] Stock-Level: 4.35% (>= 4.0%) [OK] [response time (at least 90% passed)] New-Order: 0.00% [NG] * Payment: 0.15% [NG] * Order-Status: 27.10% [NG] * Delivery: 0.00% [NG] * Stock-Level: 0.00% [NG] * 5812.667 TpmC 升级：RDS-4C8G-80个仓库结果 CPU：100%，内存：55%，连接数：10%，IOPS：20%，磁盘空间：25G 1780, trx: 2303, 95%: 796.121, 99%: 1099.640, max_rt: 1596.883, 2293|2249.288, 232|256.393, 230|1694.050, 235|2550.775 1790, trx: 2336, 95%: 798.030, 99%: 1093.403, max_rt: 1547.840, 2338|2803.739, 234|305.185, 232|1799.869, 228|2453.748 1800, trx: 2305, 95%: 801.381, 99%: 1048.528, max_rt: 1297.465, 2306|1798.565, 229|304.329, 227|1649.609, 233|2549.599 STOPPING THREADS........................................................................................................................................................................................................ [0] sc:7 lt:406567 rt:0 fl:0 avg_rt: 493.7 (5) [1] sc:10485 lt:395860 rt:0 fl:0 avg_rt: 240.1 (5) [2] sc:24615 lt:16045 rt:0 fl:0 avg_rt: 49.4 (5) [3] sc:0 lt:40651 rt:0 fl:0 avg_rt: 1273.6 (80) [4] sc:0 lt:40656 rt:0 fl:0 avg_rt: 1665.3 (20) in 1800 sec. [0] sc:7 lt:406569 rt:0 fl:0 [1] sc:10487 lt:396098 rt:0 fl:0 [2] sc:24615 lt:16045 rt:0 fl:0 [3] sc:0 lt:40655 rt:0 fl:0 [4] sc:0 lt:40659 rt:0 fl:0 (all must be [OK]) [transaction percentage] Payment: 43.46% (>=43.0%) [OK] Order-Status: 4.35% (>= 4.0%) [OK] Delivery: 4.35% (>= 4.0%) [OK] Stock-Level: 4.35% (>= 4.0%) [OK] [response time (at least 90% passed)] New-Order: 0.00% [NG] * Payment: 2.58% [NG] * Order-Status: 60.54% [NG] * Delivery: 0.00% [NG] * Stock-Level: 0.00% [NG] * 13552.467 TpmC 升级：RDS-8C16G-80个仓库结果 CPU：100%，内存：35%，连接数：5%，IOPS：18%，磁盘空间：30G 1780, trx: 4502, 95%: 398.131, 99%: 501.634, max_rt: 772.128, 4473|740.073, 446|183.361, 448|1042.264, 442|1302.569 1790, trx: 4465, 95%: 398.489, 99%: 541.424, max_rt: 803.659, 4476|845.313, 448|152.917, 450|997.319, 454|1250.160 1800, trx: 4506, 95%: 397.774, 99%: 501.334, max_rt: 747.074, 4508|701.625, 453|108.619, 450|1052.293, 451|1107.277 STOPPING THREADS........................................................................................................................................................................................................ [0] sc:20 lt:803738 rt:0 fl:0 avg_rt: 240.5 (5) [1] sc:13844 lt:789535 rt:0 fl:0 avg_rt: 128.5 (5) [2] sc:54560 lt:25817 rt:0 fl:0 avg_rt: 22.1 (5) [3] sc:0 lt:80372 rt:0 fl:0 avg_rt: 739.8 (80) [4] sc:0 lt:80378 rt:0 fl:0 avg_rt: 771.1 (20) in 1800 sec. [0] sc:20 lt:803747 rt:0 fl:0 [1] sc:13845 lt:789916 rt:0 fl:0 [2] sc:54561 lt:25817 rt:0 fl:0 [3] sc:0 lt:80377 rt:0 fl:0 [4] sc:0 lt:80381 rt:0 fl:0 (all must be [OK]) [transaction percentage] Payment: 43.47% (>=43.0%) [OK] Order-Status: 4.35% (>= 4.0%) [OK] Delivery: 4.35% (>= 4.0%) [OK] Stock-Level: 4.35% (>= 4.0%) [OK] [response time (at least 90% passed)] New-Order: 0.00% [NG] * Payment: 1.72% [NG] * Order-Status: 67.88% [NG] * Delivery: 0.00% [NG] * Stock-Level: 0.00% [NG] * 26791.934 TpmC 升级：RDS-16C64G-80个仓库结果 CPU：100%，内存：18%，连接数：2%，IOPS：10%，磁盘空间：40G 1780, trx: 8413, 95%: 203.560, 99%: 279.322, max_rt: 451.010, 8414|441.849, 841|92.900, 839|583.340, 843|644.276 1790, trx: 8269, 95%: 204.599, 99%: 282.602, max_rt: 444.075, 8262|412.414, 827|91.551, 831|665.421, 824|616.396 1800, trx: 8395, 95%: 202.285, 99%: 255.026, max_rt: 436.136, 8404|446.292, 839|87.081, 839|609.221, 842|697.509 STOPPING THREADS........................................................................................................................................................................................................ [0] sc:37 lt:1532893 rt:0 fl:0 avg_rt: 124.8 (5) [1] sc:36091 lt:1496111 rt:0 fl:0 avg_rt: 68.5 (5) [2] sc:105738 lt:47555 rt:0 fl:0 avg_rt: 11.4 (5) [3] sc:0 lt:153285 rt:0 fl:0 avg_rt: 404.6 (80) [4] sc:0 lt:153293 rt:0 fl:0 avg_rt: 389.5 (20) in 1800 sec. [0] sc:37 lt:1532918 rt:0 fl:0 [1] sc:36093 lt:1496868 rt:0 fl:0 [2] sc:105739 lt:47556 rt:0 fl:0 [3] sc:0 lt:153297 rt:0 fl:0 [4] sc:0 lt:153298 rt:0 fl:0 (all must be [OK]) [transaction percentage] Payment: 43.47% (>=43.0%) [OK] Order-Status: 4.35% (>= 4.0%) [OK] Delivery: 4.35% (>= 4.0%) [OK] Stock-Level: 4.35% (>= 4.0%) [OK] [response time (at least 90% passed)] New-Order: 0.00% [NG] * Payment: 2.36% [NG] * Order-Status: 68.98% [NG] * Delivery: 0.00% [NG] * Stock-Level: 0.00% [NG] * 51097.668 TpmC 几轮下来，最终数据量： select count(*) from customer; 2400000 select count(*) from district; 800 select count(*) from history; 5779395 select count(*) from item; 100000 select count(*) from new_orders; 764970 select count(*) from order_line; 57453708 select count(*) from orders; 5745589 select count(*) from stock; 8000000 select count(*) from warehouse; 80 资料 https://my.oschina.net/moooofly/blog/152547 http://www.techug.com/post/mysql-mysqlslap.html http://jixiuf.github.io/blog/mysql%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/ http://blog.chinaunix.net/uid-25723371-id-3498970.html http://nsimple.top/archives/mysql-sysbench-tool.html https://dearhwj.gitbooks.io/itbook/content/test/performance_test_qps_tps.html https://www.hi-linux.com/posts/38534.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Mysql-Tutorial.html":{"url":"Linux-Tutorial/markdown-file/Mysql-Tutorial.html","title":"MySQL 教程","keywords":"","body":"Mysql 教程 Mysql 常用命令 资料 MySQL Tutorial-1 MySQL Tutorial-2 MySQL Tutorial-3 MySQL 教程-1 MySQL 教程-2 MySQL 教程-3 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/PXC-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/PXC-Install-And-Settings.html","title":"Percona XtraDB Cluster（PXC）安装和配置","keywords":"","body":"Percona XtraDB Cluster（PXC）安装和配置 PXC 主要特点 主要特点：强一致性（比较适合比较注重事务的场景） 采用同步复制，事务在所有节点中要嘛是同时提交成功，要嘛不提交，让写入失败 所以，整个集群的写入吞吐量是由最弱的节点限制，如果有一个节点变配置较差，整体质量就是差的 数据同步是双向的，任何节点是从，也是主，都可以进行写入 一般推荐至少 3 个节点 官网资料 官网介绍：https://www.percona.com/software/mysql-database/percona-xtradb-cluster 官网下载：https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/ Docker 方式安装 Docker 官方仓库：https://hub.docker.com/r/percona/percona-xtradb-cluster/ 下载镜像：docker pull percona/percona-xtradb-cluster 创建需要挂载的目录：mkdir -p /data/docker/pxc/node1/mysql /data/docker/pxc/node2/mysql /data/docker/pxc/node3/mysql 创建需要挂载的目录：mkdir -p /data/docker/pxc/node1/backup 赋权：chmod 777 -R /data/docker/pxc 创建 Docker 网段：docker network create --subnet=172.18.0.0/24 pxc-net 启动镜像： # 初次初始化比较慢，给个 2 分钟左右吧，同时这个节点也用来做全量备份 docker run -d -p 3307:3306 -v /data/docker/pxc/node1/mysql:/var/lib/mysql -v /data/docker/pxc/node1/backup:/data/backup -e MYSQL_ROOT_PASSWORD=gitnavi123456 -e CLUSTER_NAME=pxc-cluster -e XTRABACKUP_PASSWORD=gitnavi123456 --privileged --name=pxc-node-1 --net=pxc-net --ip 172.18.0.2 percona/percona-xtradb-cluster 使用 SQLyog 测试是否可以连上去，可以才能继续创建其他节点。 连接地址是宿主机 IP，端口是：3307 docker run -d -p 3308:3306 -v /data/docker/pxc/node2/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=gitnavi123456 -e CLUSTER_NAME=pxc-cluster -e XTRABACKUP_PASSWORD=gitnavi123456 -e CLUSTER_JOIN=pxc-node-1 --privileged --name=pxc-node-2 --net=pxc-net --ip 172.18.0.3 percona/percona-xtradb-cluster docker run -d -p 3309:3306 -v /data/docker/pxc/node3/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=gitnavi123456 -e CLUSTER_NAME=pxc-cluster -e XTRABACKUP_PASSWORD=gitnavi123456 -e CLUSTER_JOIN=pxc-node-1 --privileged --name=pxc-node-3 --net=pxc-net --ip 172.18.0.4 percona/percona-xtradb-cluster 测试集群 用 SQLyog 连上 3 个节点，随便找一个节点创建库，其他几个节点会同时产生库。以此类推，创建表、插入数据，然后查看其他库情况。 负载均衡 因为 PXC 是同步是双向的，都支持读写，所以就可以考虑使用负载均衡实现流量分发 使用使用 HAProxy（支持 HTTP 协议、TCP/IP 协议，并且支持虚拟化，可以直接用 Docker 安装） 创建需要挂载的目录：mkdir -p /data/docker/haproxy/conf 赋权：chmod 777 -R /data/docker/haproxy 创建一个用于 MySQL 心跳检测的用户： 连上 PXC 任意一个数据库：CREATE USER 'haproxy'@'%' IDENTIFIED BY ''; 创建配置文件：vim /data/docker/haproxy/conf/haproxy.cfg global #工作目录 chroot /usr/local/etc/haproxy #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info log 127.0.0.1 local5 info #守护进程运行 daemon defaults log global mode http #日志格式 option httplog #日志中不记录负载均衡的心跳检测记录 option dontlognull #连接超时（毫秒） timeout connect 5000 #客户端超时（毫秒） timeout client 50000 #服务器超时（毫秒） timeout server 50000 #监控界面 listen admin_stats #监控界面的访问的IP和端口 bind 0.0.0.0:8118 #访问协议 mode http # URI 相对地址（访问 haproxy 监控地址：http://192.168.0.105:8118/dbs） stats uri /dbs #统计报告格式 stats realm Global\\ statistics #登陆帐户信息（登录名 admin，密码：gitnavi123456） stats auth admin:gitnavi123456 #数据库负载均衡 listen proxy-mysql #访问的IP和端口 bind 0.0.0.0:3316 #网络协议 mode tcp #负载均衡算法（轮询算法） #轮询算法：roundrobin #权重算法：static-rr #最少连接算法：leastconn #请求源IP算法：source balance roundrobin #日志格式 option tcplog #在 MySQL 中创建一个没有权限的 haproxy 用户，密码为空。Haproxy 使用这个账户对MySQL数据库心跳检测 option mysql-check user haproxy #这里填写的端口是 docker 容器的端口，而不是宿主机端口 server MySQL_1 172.18.0.2:3306 check weight 1 maxconn 2000 server MySQL_2 172.18.0.3:3306 check weight 1 maxconn 2000 server MySQL_3 172.18.0.4:3306 check weight 1 maxconn 2000 #使用keepalive检测死链 option tcpka 官网 Docker 镜像：https://hub.docker.com/_/haproxy/ 运行容器：docker run -it -d -p 4001:8118 -p 4002:3316 -v /data/docker/haproxy/conf:/usr/local/etc/haproxy --name pxc-haproxy-1 --privileged --net=pxc-net haproxy -f /usr/local/etc/haproxy/haproxy.cfg 浏览器访问：http://192.168.0.105:4001/dbs 输入：admin 输入：gitnavi123456 可以看到 HAProxy 监控界面 SQLyog 连接 IP：192.168.0.105 端口：4002 用户：root 密码：gitnavi123456 然后在上面创建对应的数据，如果所有节点都有对应的数据，则表示部署成功 XtraBackup 热备份 XtraBackup 备份过程不锁表 XtraBackup 备份过程不会打断正在执行的事务 XtraBackup 备份资料经过压缩，磁盘空间占用低 全量备份 容器内安装 XtraBackup，并执行备份语句 apt-get update apt-get install -y percona-xtrabackup-24 # 全量备份，备份到 docker 容器的 /data 目录下： innobackupex --user=root --password=gitnavi123456 /data/backup/full/201806 还原全量备份 PXC 还原数据的时候，必须解散集群，删除掉只剩下一个节点，同时删除节点中的数据 进入容器：rm -rf /var/lib/mysql/* 回滚备份时没有提交的事务：innobackupex --user=root --password=gitnavi123456 --apply-back /data/backup/full/2018-04-15_05-09-07/ 还原数据：innobackupex --user=root --password=gitnavi123456 --copy-back /data/backup/full/2018-04-15_05-09-07/ 增量备份（未整理） 资料 https://blog.csdn.net/leshami/article/details/72173732 https://zhangge.net/5125.html https://blog.csdn.net/u012758088/article/details/78643704 https://coding.imooc.com/class/219.html <> <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Redis-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Redis-Install-And-Settings.html","title":"Redis 安装和配置","keywords":"","body":"Redis 安装和配置 如果你用 Spring Data Redis 依赖请注意 请先看官网最新支持到哪个版本的依赖：https://docs.spring.io/spring-data/data-redis/docs/current/reference/html/#new-features 查看锚点为：New in Spring Data Redis 的内容 目前 201712 支持 Redis 3.2 如果你用 RedisDesktopManager 客户端请注意 请查看介绍中支持哪个版本：https://github.com/uglide/RedisDesktopManager 目前 201712 支持 2.8 以上 Docker 下安装 Redis 创建一个宿主机目录用来存放 redis 配置文件：mkdir -p /data/docker/redis/conf 创建一个宿主机以后用来存放数据的目录：mkdir -p /data/docker/redis/db 赋权：chmod 777 -R /data/docker/redis 自己编写一个配置文件 vim /data/docker/redis/conf/redis.conf，内容如下： Redis 默认的配置文件内容： 安全情况的几个特殊配置： bind 127.0.0.1 requirepass adgredis123456 protected-mode yes 免密情况： bind 0.0.0.0 protected-mode no 其他： port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /data/redis_6379.pid loglevel notice logfile \"\" databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir /data slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes 启动镜像：docker run -d -ti -p 6379:6379 -v /data/docker/redis/conf/redis.conf:/etc/redis/redis.conf -v /data/docker/redis/db:/data --restart always --name cloud-redis redis:3.2 redis-server /etc/redis/redis.conf 查看镜像运行情况：docker ps 进入镜像中 redis shell 交互界面：docker exec -it cloud-redis redis-cli -h 127.0.0.1 -p 6379 -a adgredis123456 重新启动服务：docker restart cloud-redis RedisCluster 集群（Docker 方式） Redis 容器准备 目标：3 主 3 从（一般都是推荐奇数个 master） 最小集群数推荐是：3 测试机的最低配置推荐是：2C4G 拉取镜像：docker pull registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster:3.2.3 重新打个 tag（旧名字太长了）：docker tag registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster:3.2.3 redis-to-cluster:3.2.3 创建网段：docker network create --subnet=172.19.0.0/16 net-redis-to-cluster 宿主机创建配置文件：mkdir -p /data/docker/redis-to-cluster/config && vim /data/docker/redis-to-cluster/config/redis.conf bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize yes supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \"\" databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir ./ slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly yes appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 cluster-enabled yes cluster-config-file nodes-6379.conf cluster-node-timeout 15000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes 赋权：chmod 777 -R /data/docker/redis-to-cluster/ 运行 6 个节点： docker run -it -d --name redis-to-cluster-1 -p 5001:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.2 redis-to-cluster:3.2.3 bash docker run -it -d --name redis-to-cluster-2 -p 5002:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.3 redis-to-cluster:3.2.3 bash docker run -it -d --name redis-to-cluster-3 -p 5003:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.4 redis-to-cluster:3.2.3 bash docker run -it -d --name redis-to-cluster-4 -p 5004:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.5 redis-to-cluster:3.2.3 bash docker run -it -d --name redis-to-cluster-5 -p 5005:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.6 redis-to-cluster:3.2.3 bash docker run -it -d --name redis-to-cluster-6 -p 5006:6379 -v /data/docker/redis-to-cluster/config/redis.conf:/usr/redis/redis.conf --net=net-redis-to-cluster --ip 172.19.0.7 redis-to-cluster:3.2.3 bash 配置 redis-to-cluster-1 节点：docker exec -it redis-to-cluster-1 bash 启动容器的 redis：/usr/redis/src/redis-server /usr/redis/redis.conf 其他 5 个节点一样进行启动。 创建 Cluster 集群（通过 redis-trib.rb） 配置 redis-to-cluster-1 节点（或者选择其他任意一个节点）：docker exec -it redis-to-cluster-1 bash mkdir -p /usr/redis/cluster cp /usr/redis/src/redis-trib.rb /usr/redis/cluster/ cd /usr/redis/cluster/ 创建 Cluster 集群（会有交互）（镜像中已经安装了 ruby 了）：./redis-trib.rb create --replicas 1 172.19.0.2:6379 172.19.0.3:6379 172.19.0.4:6379 172.19.0.5:6379 172.19.0.6:6379 172.19.0.7:6379 --replicas 1 表示为每个主节点创建一个从节点 如果正常的话，会出现下面内容： >>> Creating cluster >>> Performing hash slots allocation on 6 nodes... Using 3 masters: 172.19.0.2:6379 172.19.0.3:6379 172.19.0.4:6379 Adding replica 172.19.0.5:6379 to 172.19.0.2:6379 Adding replica 172.19.0.6:6379 to 172.19.0.3:6379 Adding replica 172.19.0.7:6379 to 172.19.0.4:6379 M: 9c1c64b18bfc2a0586be2089f13c330787c1f67b 172.19.0.2:6379 slots:0-5460 (5461 slots) master M: 35a633853329c9ff25bb93a7ce9192699c2ab6a8 172.19.0.3:6379 slots:5461-10922 (5462 slots) master M: 8ea2bfeeeda939abb43e96a95a990bcc55c10389 172.19.0.4:6379 slots:10923-16383 (5461 slots) master S: 9cb00acba065120ea96834f4352c72bb50aa37ac 172.19.0.5:6379 replicates 9c1c64b18bfc2a0586be2089f13c330787c1f67b S: 8e2a4bb11e97adf28427091a621dbbed66c61001 172.19.0.6:6379 replicates 35a633853329c9ff25bb93a7ce9192699c2ab6a8 S: 5d0fe968559af3035d8d64ab598f2841e5f3a059 172.19.0.7:6379 replicates 8ea2bfeeeda939abb43e96a95a990bcc55c10389 Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join...... >>> Performing Cluster Check (using node 172.19.0.2:6379) M: 9c1c64b18bfc2a0586be2089f13c330787c1f67b 172.19.0.2:6379 slots:0-5460 (5461 slots) master M: 35a633853329c9ff25bb93a7ce9192699c2ab6a8 172.19.0.3:6379 slots:5461-10922 (5462 slots) master M: 8ea2bfeeeda939abb43e96a95a990bcc55c10389 172.19.0.4:6379 slots:10923-16383 (5461 slots) master M: 9cb00acba065120ea96834f4352c72bb50aa37ac 172.19.0.5:6379 slots: (0 slots) master replicates 9c1c64b18bfc2a0586be2089f13c330787c1f67b M: 8e2a4bb11e97adf28427091a621dbbed66c61001 172.19.0.6:6379 slots: (0 slots) master replicates 35a633853329c9ff25bb93a7ce9192699c2ab6a8 M: 5d0fe968559af3035d8d64ab598f2841e5f3a059 172.19.0.7:6379 slots: (0 slots) master replicates 8ea2bfeeeda939abb43e96a95a990bcc55c10389 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 连接集群测试： 进入随便一个节点：docker exec -it redis-to-cluster-1 bash /usr/redis/src/redis-cli -c 查看集群情况：cluster nodes 写入数据：set myKey myValue，如果成功会返回：Redirected to slot [16281] located at 172.19.0.4:6379，可以推断它是 redis-to-cluster-3 容器 暂定掉 redis-to-cluster-3 容器：docker pause redis-to-cluster-3 重新连接：/usr/redis/src/redis-cli -c 查看集群情况：cluster nodes 获取值：get myKey 重新启动 redis-to-cluster-3：docker unpause redis-to-cluster-3 查看集群情况：cluster nodes Spring Boot 项目 Docker 容器访问 RedisCluster application.yml 配置的 IP 地址：172.19.0.2 等 docker 容器启动增加 --net=host 使用宿主机网络 Redis 编译安装 Redis 安装 官网：http://redis.io/ 官网下载：http://redis.io/download 官网 Github 地址：https://github.com/redis 此时（20160212） Redis 最新稳定版本为：3.0.7 官网帮助中心：http://redis.io/documentation Redis 下载（/usr/local）：wget http://download.redis.io/releases/redis-3.0.7.tar.gz （大小：1.4 M） 安装依赖包：yum install -y gcc-c++ tcl 解压：tar zxvf redis-3.0.7.tar.gz 进入解压后目录：cd /usr/local/redis-3.0.7/ 编译：make 编译安装：make install 安装完之后会在：/usr/local/bin 目录下生成好几个 redis 相关的文件 复制配置文件：cp /usr/local/redis-3.0.7/redis.conf /etc/ 修改配置：vim /etc/redis.conf 把旧值：daemonize no 改为新值：daemonize yes 启动：/usr/local/bin/redis-server /etc/redis.conf 关闭：redis-cli -h 127.0.0.1 -p 6379 shutdown 关闭（带密码）：redis-cli -h 127.0.0.1 -p 6379 -a 123456 shutdown 查看是否启动：ps -ef | grep redis 进入客户端：redis-cli 关闭客户端：redis-cli shutdown 开机启动配置：echo \"/usr/local/bin/redis-server /etc/redis.conf\" >> /etc/rc.local 开放防火墙端口： 添加规则：iptables -I INPUT -p tcp -m tcp --dport 6379 -j ACCEPT 保存规则：service iptables save 重启 iptables：service iptables restart Redis-3.0.7 配置 编辑配置文件：vim /etc/redis.conf Redis 默认的配置文件内容： # 是否以后台daemon方式运行，默认是 no，一般我们会改为 yes daemonize no pidfile /var/run/redis.pid port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 0 loglevel notice logfile \"\" # 开启数据库的数量，Redis 是有数据库概念的，默认是 16 个，数字从 0 ~ 15 databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir ./ slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-entries 512 list-max-ziplist-value 64 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes Redis-3.2.8 配置 编辑配置文件：vim /etc/redis.conf Redis 默认的配置文件内容： # 默认绑定是：127.0.0.1，这样就只能本机才能连上，为了让所有机子连上，这里需要改为：0.0.0.0 bind 0.0.0.0 protected-mode yes port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize yes supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \"\" databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb dir ./ slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes 设置 Redis 请求密码 打开 vim /etc/redis.conf 配置文件，找到默认是被注释的这一行：# requirepass foobared 去掉注释，把 foobared 改为你想要设置的密码，比如我打算设置为：123456，所以我改为：requirepass 123456 修改之后重启下服务 有了密码之后，进入客户端，就得这样访问：redis-cli -h 127.0.0.1 -p 6379 -a 123456 如果用 IP 进入客户端，但是报：Could not connect to Redis at 192.168.1.121:6379: Connection refused 原因：Redis 默认只允许本机访问，可是有时候我们也需要 Redis 被远程访问。 解决办法： 修改 Redis 配置文件：vim /etc/redis.conf 找到 bind 那行配置，默认是（需要注意的是配置文件中除了注释还有一个默认开启的地方，所以不要漏掉）：# bind 127.0.0.1 去掉 # 注释并改为：bind 0.0.0.0 Redis 常用命令 命令是不区分大小写的，但是这里为了方便和后面的 key value 进行区分所以我全部写大写，你也可以用小写。 但是需要注意的是：key 是完全区分大小写的，比如 key=codeBlog 和 key=codeblog 是两个键值 官网命令列表：http://redis.io/commands SET key value，设值。eg：SET myblog www.youmeek.com GET key，取值 SELECT 0，切换数据库 INCR key，递增数字 DECR key，递减数字 KEYS *，查看当前数据库下所有的 key APPEND key value，给尾部追加内容，如果要追加的 key 不存在，则相当于 SET key value STRLEN key，返回键值的长度，如果键不存在则返回 0 MSET key1 value1 key2 value2，同时设置多值 MGET key1 value1 key2 value2，同时取多值 EXPIRE key 27，设置指定键的生存时间，27 的单位是秒 TTL key，查看键的剩余生存时间 返回 -2，表示不存在，过了生存时间后被删除 返回 -1，表示没有生存时间，永久存储 返回正整数，表示还剩下对应的生存时间 PERSIST key，清除生成时间，重新变成永久存储（重新设置 key 的值也可以起到清除生存时间的效果） FLUSHDB，清空当前数据库所有键值 FLUSHALL，清空所有数据库的所有键值 把 redis 添加到系统服务中 新建文件：vim /etc/init.d/redis 添加如下内容： #!/bin/sh # # redis - this script starts and stops the redis-server daemon # # chkconfig: - 85 15 # description: Redis is a persistent key-value database # processname: redis-server # config: /usr/local/redis-2.4.X/bin/redis-server # config: /usr/local/ /redis-2.4.X/etc/redis.conf # Source function library. . /etc/rc.d/init.d/functions # Source networking configuration. . /etc/sysconfig/network # Check that networking is up. [ \"$NETWORKING\" = \"no\" ] && exit 0 redis=\"/usr/local/bin/redis-server\" prog=$(basename $redis) REDIS_CONF_FILE=\"/etc/redis.conf\" [ -f /etc/sysconfig/redis ] && . /etc/sysconfig/redis lockfile=/var/lock/subsys/redis start() { [ -x $redis ] || exit 5 [ -f $REDIS_CONF_FILE ] || exit 6 echo -n $\"Starting $prog: \" daemon $redis $REDIS_CONF_FILE retval=$? echo [ $retval -eq 0 ] && touch $lockfile return $retval } stop() { echo -n $\"Stopping $prog: \" killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] && rm -f $lockfile return $retval } restart() { stop start } reload() { echo -n $\"Reloading $prog: \" killproc $redis -HUP RETVAL=$? echo } force_reload() { restart } rh_status() { status $prog } rh_status_q() { rh_status >/dev/null 2>&1 } case \"$1\" in start) rh_status_q && exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $\"Usage: $0 {start|stop|status|restart|condrestart|try-restart| reload|orce-reload}\" exit 2 esac 修改权限：chmod 755 /etc/init.d/redis 启动服务：service redis start 停止服务：service redis stop 重启服务：service ngredisnx restart Redis 客户端 Java：http://redis.io/clients#java Jedis 官网：https://github.com/xetorthio/jedis Redis GUI 管理工具 Redis Desktop Manager 官网：http://redisdesktop.com/ 官网下载：http://redisdesktop.com/download 效果如下图： Redis 主从（主从从）配置 Redis 主从架构 假设有两台服务器，一台做主，一台做从 Redis 主信息： IP：12.168.1.114= 端口：6379 Redis 从信息： IP：12.168.1.115 端口：6379 编辑从机的 Redis 配置文件，找到 210 行（大概），默认这一行应该是注释的：# slaveof 我们需要去掉该注释，并且填写我们自己的主机的 IP 和 端口，比如：slaveof 192.168.1.114 6379 配置完成后重启从机 Redis 服务 重启完之后，进入主机的 redis-cli 状态下，输入：INFO replication 可以查询到当前主机的 redis 处于什么角色，有哪些从机已经连上主机。 此时已经完成了主从配置，我们可以测试下： 我们进入主机的 redis-cli 状态，然后 set 某个值，比如：set myblog YouMeek.com 我们切换进入从机的 redis-cli 的状态下，获取刚刚设置的值看是否存在：get myblog，此时，我们可以发现是可以获取到值的。 但是有一个需要注意的：从库不具备写入数据能力，不然会报错。 从库只有只读能力。 Redis主从从架构 Redis 主从从的意思：看桌面上的截图。 优点，除了减少主库连接的压力，还有可以关掉主库的持久化功能，把持久化的功能交给从库进行处理。 第一个从库配置的信息是连上主库，后面的第二个从库配置的连接信息是连上第一个从库， 假如还有第三个从库的话，我们可以把第三个从库的配置信息连上第二个从库上，以此类推。 Redis 3 主 2 从 3 哨兵--配置集群+HA（高可用、故障转移） 3 主 2 从 3 哨兵（平均每台机子是：1 主 2 从 3 哨兵） 一个健康的集群部署，至少需要 3 个 Sentinel 实例，官网（Example 2: basic setup with three boxes）：https://redis.io/topics/sentinel 这里使用了 3 台阿里云服务器，系统：CentOS 6.8 3 主： 192.168.1.1 先做 Redis 集群，然后写个 Spring Data Redis 测试是否可以正常使用该集群 每台服务器各有一个 Redis 程序，然后有 3 个不同配置文件，启动 Redis 的时候指定各自的配置文件，依次表示 3 个 Redis 程序。 先关闭防火墙，避免各种端口未开放问题 Redis 目前支持主从复制，但是主挂掉后，从也只能只读，所以需要在主挂掉后，从一个其中一个从节点中升级到主，这里用到的是：redis sentinel 漂移 IP 这里用到 keepalived Windows 版本的 Redis Windows 是别人改造的版本，需要到这里下载：https://github.com/MSOpenTech/redis/releases 使用 .msi 后缀的文件进行安装，此安装包自带安装 Windows 服务 配置文件也跟原版本不一样，叫做：redis.windows.conf Redis Info 客户端下命令行：info 参考：http://redisdoc.com/server/info.html server 部分记录了 Redis 服务器的信息，它包含以下域： redis_version : Redis 服务器版本 redis_git_sha1 : Git SHA1 redis_git_dirty : Git dirty flag os : Redis 服务器的宿主操作系统 arch_bits : 架构（32 或 64 位） multiplexing_api : Redis 所使用的事件处理机制 gcc_version : 编译 Redis 时所使用的 GCC 版本 process_id : 服务器进程的 PID run_id : Redis 服务器的随机标识符（用于 Sentinel 和集群） tcp_port : TCP/IP 监听端口 uptime_in_seconds : 自 Redis 服务器启动以来，经过的秒数 uptime_in_days : 自 Redis 服务器启动以来，经过的天数 lru_clock : 以分钟为单位进行自增的时钟，用于 LRU 管理 clients 部分记录了已连接客户端的信息，它包含以下域： connected_clients : 已连接客户端的数量（不包括通过从属服务器连接的客户端） client_longest_output_list : 当前连接的客户端当中，最长的输出列表 client_longest_input_buf : 当前连接的客户端当中，最大输入缓存 blocked_clients : 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 memory 部分记录了服务器的内存信息，它包含以下域： used_memory : 由 Redis 分配器分配的内存总量，以字节（byte）为单位 used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量 used_memory_rss : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。 used_memory_peak : Redis 的内存消耗峰值（以字节为单位） used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值 used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位） mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率 mem_allocator : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc 。 常关注信息： used_memory_rss_human：系统给redis分配的内存（即常驻内存） used_memory_peak_human : Redis 的内存消耗峰值 used_memory_lua_human : 系统内存大小 expired_keys : 过期的的键数量 evicted_keys : 因为最大内存容量限制而被驱逐（evict）的键数量 used_cpu_sys_children : Redis 后台进程在 内核态 消耗的 CPU used_cpu_user_children : Redis 后台进程在 用户态 消耗的 CPU Redis 基准压力测试 默认安装包下就自带 官网文档：https://redis.io/topics/benchmarks 运行：redis-benchmark -q -n 100000 -q 表示 quiet 安静执行，结束后直接输出结果即可 -n 100000 请求 10 万次 PING_INLINE: 62189.05 requests per second PING_BULK: 68634.18 requests per second SET: 58241.12 requests per second GET: 65445.03 requests per second INCR: 57703.40 requests per second LPUSH: 61199.51 requests per second RPUSH: 68119.89 requests per second LPOP: 58309.04 requests per second RPOP: 63775.51 requests per second SADD: 58479.53 requests per second HSET: 61500.61 requests per second SPOP: 58241.12 requests per second LPUSH (needed to benchmark LRANGE): 59523.81 requests per second LRANGE_100 (first 100 elements): 60350.03 requests per second LRANGE_300 (first 300 elements): 57636.89 requests per second LRANGE_500 (first 450 elements): 63251.11 requests per second LRANGE_600 (first 600 elements): 58479.53 requests per second MSET (10 keys): 56401.58 requests per second 只测试特定类型：redis-benchmark -t set,lpush -n 100000 -q 资料 http://yanshisan.blog.51cto.com/7879234/1377992 https://segmentfault.com/a/1190000002685224 http://itbilu.com/linux/management/4kB2ninp.html http://keenwon.com/1335.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/MongoDB-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/MongoDB-Install-And-Settings.html","title":"MongoDB 安装和配置","keywords":"","body":"MongoDB 安装和配置 基础概念 MongoDB 元素概念 databases: 数据库; collections:表；（collections组成了databases） documents:行；（documents组成了collections） MongoDB 没有新建数据库的命令，只要进行 insert 或其它操作，MongoDB 就会自动帮你建立数据库和 collection。当查询一个不存在的 collection 时也不会出错，MongoDB 会认为那是一个空的 collection。 一个对象被插入到数据库中时，如果它没有 ID，会自动生成一个 \"_id\" 字段，为 12 字节(24位)16进制数。 当然如果插入文档不带 _id，则系统会帮你自动创建一个，如果自己指定了就用自己指定的。 如果你用 Spring Data MongoDB 依赖请注意 请先看官网最新支持到哪个版本的依赖：https://docs.spring.io/spring-data/mongodb/docs/current/reference/html/#new-features 查看锚点为：What’s new in Spring Data MongoDB 的内容，比如：What’s new in Spring Data MongoDB 1.10，出现这样一句话：Compatible with MongoDB Server 3.4 and the MongoDB Java Driver 3.4 目前 201712 支持 MongoDB 3.4 如果你用 Robomongo 客户端请注意 请查看介绍中支持哪个版本：https://robomongo.org/download 目前 201712 支持 MongoDB 3.4 Docker 下安装 MongoDB（方式一） 先创建一个宿主机以后用来存放数据的目录：mkdir -p /data/docker/mongo/db 赋权：chmod 777 -R /data/docker/mongo/db 首次运行镜像：docker run --name cloud-mongo -p 27017:27017 -v /data/docker/mongo/db:/data/db -d mongo:3.4 进入容器中 mongo shell 交互界面：docker exec -it cloud-mongo mongo adg_mongo_db 创建一个用户： db.createUser( { user: \"adguser\", pwd: \"adg123456\", roles: [ { role: \"dbAdmin\", db: \"adg_mongo_db\" }, { role: \"readWrite\", db: \"adg_mongo_db\" } ] } ) 然后停掉容器：docker stop cloud-mongo 然后删除容器：docker rm cloud-mongo 重新运行镜像，这次增加需要授权才能访问的配置：docker run -d -p 27017:27017 -v /data/docker/mongo/db:/data/db --restart always --name cloud-mongo mongo:3.4 --auth 重新启动服务：docker restart cloud-mongo 导出：docker exec -it cloud-mongo mongoexport -h 127.0.0.1 -u 用户名 -p 密码 -d 库名 -c 集合名 -o /data/db/mongodb.json --type json 导入：docker exec -it cloud-mongo mongoimport -h 127.0.0.1 -u 用户名 -p 密码 -d 库名 -c 集合名 --file /data/db/mongodb.json --type json Docker 下安装 MongoDB（方式二） 先创建一个宿主机以后用来存放数据的目录：mkdir -p /data/docker/mongo/db 赋权：chmod 777 -R /data/docker/mongo/db 运行镜像：docker run --name cloud-mongo2 -p 37017:27017 -v /data/docker/mongo/db:/data/db -d mongo:3.4 --auth 进入容器中 mongo shell 交互界面：docker exec -it cloud-mongo2 mongo 创建一个超级用户： use admin db.createUser( { user: \"mongo-admin\", pwd: \"123456\", roles: [ { role: \"root\", db: \"admin\" } ] } ) db.auth(\"mongo-admin\",\"123456\") 使用 db.auth() 可以对数据库中的用户进行验证，如果验证成功则返回 1，否则返回 0 接着创建一个普通数据库和用户： use my_test_db db.createUser( { user: \"mytestuser\", pwd: \"123456\", roles: [ { role: \"dbAdmin\", db: \"my_test_db\" }, { role: \"readWrite\", db: \"my_test_db\" } ] } ) db.auth(\"mytestuser\",\"123456\") MongoDB 传统方式安装 关闭 SELinux 编辑配置文件：vim /etc/selinux/config 把 SELINUX=enforcing 改为 SELINUX=disabled MongoDB 资料 官网：https://www.mongodb.com 官网文档：https://docs.mongodb.com/manual/reference/method/ 此时（20170228） 最新稳定版本为：3.4.2 官网下载：https://www.mongodb.com/download-center?jmp=nav#community 官网安装方法介绍：https://docs.mongodb.com/master/tutorial/install-mongodb-on-red-hat 官网文档使用的 Package 的安装方式。还有一种安装方式是下载 tar 包的方法，如果需要 tar 包方式可以看这篇文章： https://itjh.net/2016/07/11/centos-install-mongodb yum 卸载 yum remove \"mongodb-org-*\" 3.4.2 yum 安装 新建文件：vim /etc/yum.repos.d/mongodb-org-3.4.repo，文件内容如下： [mongodb-org-3.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc 如果你要安装 2.6 的版本，可以使用下面这个内容： [mongodb-org-2.6] name=MongoDB 2.6 Repository baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64/ gpgcheck=0 enabled=1 上面文件新建好之后，输入安装命令：yum install -y mongodb-org，一共有 5 个包，加起来有 100M 左右，国内下载速度不快，需要等等，可能还会出错，如果出错用国内源：https://mirror.tuna.tsinghua.edu.cn/help/mongodb/ 开放防火墙端口： iptables -A INPUT -p tcp -m tcp --dport 27017 -j ACCEPT service iptables save service iptables restart 3.6 yum 安装： 新建文件：vim /etc/yum.repos.d/mongodb-org-3.6.repo，文件内容如下： [mongodb-org-3.6] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/testing/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc 上面文件新建好之后，输入安装命令：yum install -y mongodb-org，一共有 5 个包，加起来有 100M 左右，国内下载速度不快，需要等等，可能还会出错，如果出错用国内源：https://mirror.tuna.tsinghua.edu.cn/help/mongodb/ 开放防火墙端口： iptables -A INPUT -p tcp -m tcp --dport 27017 -j ACCEPT service iptables save service iptables restart 3.4.10 tar 绿色安装 下载：wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.10.tgz 解压到指定目录，并重命名： tar zxvf mongodb-linux-x86_64-rhel62-3.4.10.gz mv mongodb-linux-x86_64-rhel62-3.4.10 mongodb mv mongodb /usr/program 增加系统变量，我这里是用 zsh vim ~/.zshrc export MONGODB_HOME=/usr/program/mongodb export PATH=$MONGODB_HOME/bin:$PATH source ~/.zshrc 测试是否安装成功：mongod -v，安装成功会得到如下信息： 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] MongoDB starting : pid=31155 port=27017 dbpath=/data/db 64-bit host=youmeek 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] db version v3.4.10 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] git version: 078f28920cb24de0dd479b5ea6c66c644f6326e9 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] allocator: tcmalloc 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] modules: none 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] build environment: 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] distmod: rhel62 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] distarch: x86_64 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] target_arch: x86_64 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] options: { systemLog: { verbosity: 1 } } 2017-12-03T00:08:09.854+0800 D - [initandlisten] User Assertion: 29:Data directory /data/db not found. src/mongo/db/service_context_d.cpp 98 2017-12-03T00:08:09.854+0800 I STORAGE [initandlisten] exception in initAndListen: 29 Data directory /data/db not found., terminating 2017-12-03T00:08:09.854+0800 I NETWORK [initandlisten] shutdown: going to close listening sockets... 2017-12-03T00:08:09.854+0800 I NETWORK [initandlisten] shutdown: going to flush diaglog... 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] now exiting 2017-12-03T00:08:09.854+0800 I CONTROL [initandlisten] shutting down with code:100 创建数据库、日志存放目录： mkdir -p /usr/program/mongodb/data mkdir -p /usr/program/mongodb/log touch /usr/program/mongodb/log/mongodb.log 创建配置文件：vim /etc/mongodb.conf，并写入内容： dbpath=/usr/program/mongodb/data logpath=/usr/program/mongodb/log/mongodb.log logappend=true port=27017 fork=true 看下是否已经有 mongo 在运行，如果有就 kill 掉：ps -ef | grep mongo 通过配置文件启动：mongod -f /etc/mongodb.conf 显示下面信息则表示启动了： about to fork child process, waiting until server is ready for connections. forked process: 29167 child process started successfully, parent exiting 进入 MongoDB 后台管理 Shell：cd /usr/program/mongodb/bin && ./mongo 创建数据库： use youmeek 创建用户，并授权，需要注意的是：dbAdmin 的权限是没有包含 readWrite，所以很多时候要根据需求添加多个权限： db.createUser( { user: \"youmeek\", pwd: \"youmeek123456\", roles: [ { role: \"dbAdmin\", db: \"youmeek\" }, { role: \"readWrite\", db: \"youmeek\" } ] } ) 开放防火墙端口： iptables -A INPUT -p tcp -m tcp --dport 27017 -j ACCEPT service iptables save service iptables restart 修改配置文件：vim /etc/mongodb.conf，在文件最后面增加一行： auth=true 表示开启用户认证，这样后面要连接 mongo 就必须输入数据库、用户名、密码。 然后重启 mongo，开始使用。 其他常用命令： 检查版本：mongod --version 启动：service mongod start 停止：service mongod stop 重启：service mongod restart 添加自启动：chkconfig mongod on 进入客户端：mongo，如果有授权用户格式为：mongo 127.0.0.1:27017/admin -u 用户名 -p 用户密码 卸载命令：yum erase $(rpm -qa | grep mongodb-org) 删除数据库：rm -r /var/lib/mongo 删除 log：rm -r /var/log/mongodb 添加授权用户 先进入 mongo 客户端 ：mongo 输入：use admin，然后输入： db.createUser( { user: \"gitnavi\", pwd: \"123456\", roles: [ { \"role\" : \"dbAdmin\", \"db\" : \"youmeek_nav\" } ] } ) 修改密码：db.changeUserPassword(用户名, 密码) 删除用户：db.removeUser(用户名) 内置角色： read：允许用户读取指定数据库 readWrite：允许用户读写指定数据库 dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profile userAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户 clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。 readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。 root：只在admin数据库中可用。超级账号，超级权限 MongoDB 配置 编辑配置文件：vim /etc/mongod.conf，注意：编辑完记得重启 MongoDB 服务 默认的数据库目录：/var/lib/mongo 默认的日志目录：/var/log/mongodb 默认的配置文件内容： bindIp:127.0.0.1 #注释此行，表示除了本机也可以登陆 # 补充这个，表示必须使用带用户名密码的才能请求 mongodb，比如访问 admin 数据库：mongo 192.168.1.121:27017/admin -u 用户名 -p 用户密码 security: authorization: enabled 常用命令 show dbs，查看已有数据库 use 数据库名，进入指定数据库，如果这个数据库不存在了也是可以进入的，进入之后 insert 一条语句就会自动创建了。 db，显示当前用的数据库 show collections，列出当前数据库的collections(当前数据库下的表) show tables，查看数据库中的集 exit，退出 show users，查看当前库下的用户 db.system.users.find().pretty()，查看所有用户 db.dropAllUsers()，删除所有用户 db.dropDatebase()，删除当前这个数据库 db.集名称.find()，查看集中的所有数据，等同于：select * from 表名称 db.集名称.findOne()，查看集中的一条数据，等同于：select * from 表名称 limit 0,1 db.集名称.find().limit(10)，查看集中的一条数据 db.集名称.find().sort({name:1})，查询列表，根据字段name排序 #1正序 -1倒序 db.集名称.find().sort({x:1}).skip(5).limit(10)，查询列表，根据字段name排序，等同于 select from foo order by x asc limit 5, 10 db.集名称.find({x:10})，查询列表，等同于 select from foo where x = 10 db.集名称.find({x: {$lt:10}})，select from foo where x db.集名称.find({}, {y:true})，select y from foo db.集名称.find({\"address.city\":\"gz\"})，搜索嵌套文档address中city值为gz的记录 db.集名称.find({likes:\"math\"})，搜索数组 db.集名称.insert({\"a\":1,\"b\":2})，插入一个测试数据 db.集名称.find({name:\"lichuang\"})，根据索引或字段查找数据 db.集名称.update({name:\"张三\"},{$set:{name:\"李四\"}})，更新数据，等同于：UPDATE 表名 SET name='李四' WHERE name = '张三' db.集名称.update({name:\"张三\"},{$set:{name:\"李四\"},{upsert:true},{multi:true}})，更新数据，等同于：UPDATE 表名 SET name='李四' WHERE name = '张三'。其中特殊的是 upsert 为 true 的时候，表示如果没有这条数据，则创建一条。multi 表示，所有满足条件的都进行更新，不然默认只找到的第一条更新。 db.集名称.remove({name:\"lichuang\"})，删除数据，等同于：DELETE FROM 表名 WHERE name='lichuang' db.集名称.drop()，删除这个集合 db.集名称.getIndexes()，查看集合索引 db.集名称.dropIndex(\"name_1\")，删除索引 db.集名称.ensureIndex({title:1})，创建索引 db.集名称.ensureIndex({titile:1},{name:\"indexname\"})，创建索引，第二个属性设置索引名称 db.集名称.ensureIndex({titile:1},{unique:true/false})，创建唯一索引，第二个属性设置为true说明该字段中值不能重复，false可以重复 db.集名称.ensureIndex({name:1,age:1})，复合索引 db.集名称.ensureIndex({\"address.city\":1})，在嵌套文档的字段上建索引 db.集名称.insert({\"article\",\"text\"})，全文索引，指定为text类型,每个数据集合中只允许创建一个全文索引 db.adminCommand( {setParameter:1, textSearchEnabled:true})，开启全文本索引功能 一些符号说明： $lt ->less then 小于 $lte ->less than and equal 不大于 $lt ->less then 小于 $gt ->greater then 大于 $gte ->greater then and equal 不小于） $ne ->not equal 不等于 其他材料： https://segmentfault.com/a/1190000007550421 https://segmentfault.com/a/1190000005095959 http://blog.csdn.net/endlu/article/details/51098518 http://www.cnblogs.com/shaosks/p/5666764.html http://www.cookqq.com/blog/51277786-c26c-4f94-9be2-428f3633d9e5 http://www.thinksaas.cn/topics/0/513/513705.html https://www.fedte.cc/p/511.html 导入 / 导出 / 备份 /还原 数据的导出、导入 导出：mongoexport -h 127.0.0.1 -u 用户名 -p 密码 -d 库名 -c 集合名 -o /opt/mongodb.json --type json 导入：mongoimport -h 127.0.0.1 -u 用户名 -p 密码 -d 库名 -c 集合名 --file /opt/mongodb.json --type json Java 包 spring-data-mongodb：http://projects.spring.io/spring-data-mongodb/ mongo-java-driver：https://github.com/mongodb/mongo-java-driver GUI 管理工具 Robomongo：https://robomongo.org/ 基准测试 https://github.com/brianfrankcooper/YCSB/tree/master/mongodb#4-run-ycsb 随机生成测试数据 https://github.com/feliixx/mgodatagen 资料 http://www.cnblogs.com/zhoujinyi/p/4610050.html http://lvtraveler.github.io/2016/05/22/%E3%80%90MongoDB%E3%80%91MongoDB%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Solr-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Solr-Install-And-Settings.html","title":"Solr 安装和配置","keywords":"","body":"Solr 安装和配置 Solr 安装 官网：https://lucene.apache.org/solr/ 此时（20160329） Solr 最新稳定版本为：5.5.0 官网下载：http://www.apache.org/dyn/closer.lua/lucene/solr/5.5.0 官网新手入门：https://lucene.apache.org/solr/quickstart.html 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 必须先装有 JDK，我这里安装的是 JDK 7 先开放防火墙端口： 添加规则：sudo iptables -A INPUT -p tcp -m tcp --dport 8983 -j ACCEPT 保存规则：sudo service iptables save 重启 iptables：sudo service iptables restart 我这里使用 solr-4.10.2.zip：（大小：148 M） 解压：unzip solr-4.10.2.zip 移动到我个人安装目录：mv solr-4.10.2/ /usr/program/ 进入解压后目录：cd /usr/program/solr-4.10.2/example/ 启动 Solr 自带测试案例：java -jar start.jar 打开浏览器，访问：http://192.168.1.115:8983/solr/，可以看到 Solr 相关页面就表示成功了 给 Solr 这个默认测试案例添加数据：cd /usr/program/solr-4.10.2/example/exampledocs ; java -jar post.jar solr.xml monitor.xml 此时，你在 Solr 的默认测试 Core 为 collection1 的 query 下查询就可以看到一些数据。 Solr 新建 Core 进入目录：cd /usr/program/solr-4.10.2/example 创建项目目录：mkdir ssm-solr 复制配置文件：cp /usr/program/solr-4.10.2/example/solr/solr.xml /usr/program/solr-4.10.2/example/ssm-solr 创建一个 Core 目录及两个配置和数据目录：mkdir -p /usr/program/solr-4.10.2/example/ssm-solr/user/conf /usr/program/solr-4.10.2/example/ssm-solr/user/data 复制配置文件：cp /usr/program/solr-4.10.2/example/solr/collection1/core.properties /usr/program/solr-4.10.2/example/ssm-solr/user/ 编辑配置文件：vim /usr/program/solr-4.10.2/example/ssm-solr/user/core.properties 把：name=collection1，改为：name=user。这个名字就是你的新 Core 的命名，因为我测试的跟会员有关系，所以这里命名为 user 配置 IK 分词器 IKAnalyzer-2012-4x.jar 放在 /usr/program/solr-4.10.2/example/solr-webapp/webapp/WEB-INF/lib 目录下 复制配置文件：cp /usr/program/solr-4.10.2/example/solr/collection1/conf/schema.xml /usr/program/solr-4.10.2/example/ssm-solr/user/conf/ 复制配置文件：cp /usr/program/solr-4.10.2/example/solr/collection1/conf/solrconfig.xml /usr/program/solr-4.10.2/example/ssm-solr/user/conf/ 编辑配置文件：vim /usr/program/solr-4.10.2/example/ssm-solr/user/conf/schema.xml，内容改为如下： sysUserId --> --> --> --> --> 编辑配置文件：vim /usr/program/solr-4.10.2/example/ssm-solr/user/conf/solrconfig.xml，内容改为如下： 需要修改的内容有： 将所有的 标签注释掉，如我中文注释内容 把所有的 text 替换成 sysUserLoginName，这个 sysUserLoginName 是我在 schema.xm 定义的其中一个字段名字。df 是代表默认 field。 注释掉： 的内容 4.10.2 --> ${solr.data.dir:} ${solr.hdfs.home:} ${solr.hdfs.confdir:} ${solr.hdfs.blockcache.enabled:true} ${solr.hdfs.blockcache.global:true} ${solr.lock.type:native} true false ${solr.ulog.dir:} ${solr.autoCommit.maxTime:15000} false ${solr.autoSoftCommit.maxTime:-1} 1024 true 20 200 static firstSearcher warming in solrconfig.xml false 2 explicit 10 sysUserLoginName explicit json true sysUserLoginName true json true {!xport} xsort false query explicit velocity browse layout Solritas edismax text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4 title^10.0 description^5.0 keywords^5.0 author^2.0 resourcename^1.0 sysUserLoginName 100% *:* 10 *,score text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4 title^10.0 description^5.0 keywords^5.0 author^2.0 resourcename^1.0 text,features,name,sku,id,manu,cat,title,description,keywords,author,resourcename 3 on true cat manu_exact content_type author_s ipod GB 1 cat,inStock after price 0 600 50 popularity 0 10 3 manufacturedate_dt NOW/YEAR-10YEARS NOW +1YEAR before after on content features title name true html &lt;b&gt; &lt;/b&gt; 0 title 0 name 3 200 content 750 on false 5 2 5 true true 5 3 spellcheck true ignored_ true links ignored_ solrpingquery all explicit true text_general default text solr.DirectSolrSpellChecker internal 0.5 2 1 5 4 0.01 wordbreak solr.WordBreakSolrSpellChecker name true true 10 sysUserLoginName default wordbreak on true 10 5 5 true true 10 5 spellcheck mySuggester FuzzyLookupFactory DocumentDictionaryFactory cat price string true 10 suggest sysUserLoginName true tvComponent lingo org.carrot2.clustering.lingo.LingoClusteringAlgorithm clustering/carrot2 stc org.carrot2.clustering.stc.STCClusteringAlgorithm kmeans org.carrot2.clustering.kmeans.BisectingKMeansClusteringAlgorithm true true name id features true false edismax text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4 *:* 10 *,score clustering true false terms string elevate.xml --> explicit sysUserLoginName elevator 100 70 0.5 [-\\w ,/\\n\\&quot;&apos;]{20,200} ]]> ]]> ,, ,, ,, ,, ,]]> ]]> 10 .,!? &#9;&#10;&#13; WORD en US text/plain; charset=UTF-8 5 *:* 先按 Ctrl + C 停掉刚刚启动做测试的 solr 程序 重启：cd /usr/program/solr-4.10.2/example/ ; java -Dsolr.solr.home=ssm-solr -jar start.jar 重新访问，我们可以看到新的 Core 已经可以用了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Jira-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Jira-Install-And-Settings.html","title":"Jira 安装和配置","keywords":"","body":"Jira 安装和配置 Jira 安装 Jira 安装 官网：https://www.atlassian.com/software/jira 官网下载：https://www.atlassian.com/software/jira/download 中文在线试用：http://www.jira.cn/secure/Dashboard.jspa 官网帮助说明：https://confluence.atlassian.com/jira/installing-jira-on-linux-191501165.html 官网中文语言包：https://translations.atlassian.com/dashboard/download?lang=zh_CN#/JIRA/6.3.6 Jira 6.3.6 网盘下载：http://pan.baidu.com/s/1eRjrz5C Jira 6.3.6 中文语言包网盘下载：http://pan.baidu.com/s/1i3VEsC1 环境要求： JDK 7 或更新版本； Mysql 我们要使用的版本：atlassian-jira-6.3.6.tar.gz 我个人习惯 /opt 目录下创建一个目录 setups 用来存放各种软件安装包；在 /usr 目录下创建一个 program 用来存放各种解压后的软件包，下面的讲解也都是基于此习惯 我个人已经使用了第三方源：EPEL、RepoForge，如果你出现 yum install XXXXX 安装不成功的话，很有可能就是你没有相关源，请查看我对源设置的文章 解压：tar zxvf atlassian-jira-6.3.6.tar.gz 修改目录名：mv atlassian-jira-6.3.6/ jira6.3.6/ 移到我个人的安装目录下：mv jira6.3.6/ /usr/program/ 创建存放数据目录：mkdir -p /usr/program/jira6.3.6/data/ 设置环境变量： 编辑：vim /etc/profile 在文件尾部添加：JIRA_HOME=/usr/program/jira6.3.6/data/ export JIRA_HOME 刷新配置：source /etc/profile 运行：/usr/program/jira6.3.6/bin/start-jira.sh 访问：http://192.168.0.111:8080/ 汉化：cp JIRA-6.3.6-language-pack-zh_CN.jar /usr/program/jira6.3.6/atlassian-jira/WEB-INF/lib/ 配置过程： 重新激活页面地址：http://192.168.0.111:8090/secure/admin/ViewLicense.jspa Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/TeamCity-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/TeamCity-Install-And-Settings.html","title":"TeamCity 安装和配置","keywords":"","body":"TeamCity 安装和配置 本文初衷 让大家了解持续集成（CI），以及入门了解 JetBrains 家的 TeamCity 的一些简单实用。 TeamCity 的一些复杂使用我暂时也不会，一样也是要看文档的，所以不管怎样你都要养成看官网文档的习惯。 TeamCity 和 Jenkins、Hudson 其实是非常一样的，基本流程都是差不多的，所以如果你会其他的几个 CI 工具的话，学习起来很快。 Docker 已经开始在引入到 CI、CD（持续交付）过程中，可以大大简化整体的过程，也许这是未来的一个方向，有兴趣的可以了解更多。 它是什么 官网定义（就一句话）：Powerful Continuous Integration out of the box 官网首页：https://www.jetbrains.com/teamcity/ 官网特性总结：https://www.jetbrains.com/teamcity/features/ 百度百科：http://baike.baidu.com/view/3703414.htm 官网文档：https://confluence.jetbrains.com/display/TCD9/TeamCity+Documentation 支持的平台、环境如下图（看不懂也没关系，只要知道它最友好的是 Java 开发即可）： 对上图的具体讲解可以看（很重要）：https://confluence.jetbrains.com/display/TCD9/Supported+Platforms+and+Environments 为什么会出现 TeamCity 的出现需要了解这个概念：持续集成（Continuous Integration） 百科定义：http://baike.baidu.com/view/5253255.htm 网络文章：http://www.ruanyifeng.com/blog/2015/09/continuous-integration.html 哪些人喜欢它 持续集成学习笔记－入门篇（1）持续集成基本概念 7 reasons why you should be using Continuous Integration What is CI and why use it? 哪些人不喜欢它 Google 不到结果，应该是没人不喜欢，只是有些人用不惯 为什么学习它 更好地保证项目质量 同类工具 Jenkins：http://jenkins-ci.org/ Travis CI：http://travis-ci.org/ Bamboo：http://www.atlassian.com/software/bamboo Hudson：http://hudson-ci.org/ QuickBuild：http://www.pmease.com/ 其他：http://www.oschina.net/project/tag/344/ci?lang=0&os=0&sort=view&p=1 好的网络文章介绍： 持续集成工具的选择 TeamCity 入门 先来看一段官网的介绍视频 这个视频其实已经很清楚地说明了一个整理流程是怎样的，我今天只是做一个更加清晰的细节讲解而已 你需要穿越：https://www.youtube.com/watch?v=J-iYMMG6jmc#action=share TeamCity 安装部署（Linux 环境） 在我讲之前，如果你英文还可以，就到官网这里看下： Installation Quick Start 安装环境要求： JDK 1.7 以上，如果你要使用的是 2016 最新的 TeamCity 9.1 的话，JDK 官网推荐的 1.8 安装包下载：https://www.jetbrains.com/teamcity/download/#section=linux-version 开始安装（eg：TeamCity-9.1.6.tar.gz）： 解压压缩包（解压速度有点慢）：tar zxf TeamCity-9.1.6.tar.gz 解压完的目录结构讲解：https://confluence.jetbrains.com/display/TCD9/TeamCity+Home+Directory 下载的 tar.gz 的本质是已经里面捆绑了一个 Tomcat，所以如果你会 Tomcat 的话，有些东西你可以自己改的。 按我个人习惯，把解压缩的目录放在 usr 目录下：mv TeamCity/ /usr/program/ 进入解压目录：cd /usr/program/TeamCity/ 启动程序：/usr/program/TeamCity/bin/runAll.sh start 停止程序：/usr/program/TeamCity/bin/runAll.sh stop 启动需要点时间，最好能给它一两分钟吧 首次进入 假设我们已经启动了 TeamCity 访问（TeamCity 默认端口是：8111）：http://192.168.1.113:8111/ 如果访问不了，请先关闭防火墙：service iptables stop 你也可以选择把端口加入白名单中： sudo iptables -I INPUT -p tcp -m tcp --dport 8111 -j ACCEPT sudo /etc/rc.d/init.d/iptables save sudo service iptables restart 如果你要改变端口，找到下面这个 8111 位置：vim /usr/program/TeamCity/conf/server.xml 在假设你已经可以访问的情况，我们开始进入 TeamCity 的设置向导： 如上图英文所示，TeamCity 的一些软件安装的配置、服务的配置默认都会放在：/root/.BuildServer 如果你要了解更多 TeamCity Data Directory 目录，你可以看：https://confluence.jetbrains.com/display/TCD9/TeamCity+Data+Directory 如上图英文所示，TeamCity 的一些构建历史、用户信息、构建结果等这类数据是需要放在关系型数据库上的，而默认它给我们内置了一个。 如果你要了解更多 TeamCity External Database，你可以看：https://confluence.jetbrains.com/display/TCD9/Setting+up+an+External+Database 首次使用，官网是建议使用默认的：Internal(HSQLDB)，这样我们无需在一开始使用的就考虑数据库迁移或安装的问题，我们只要好好感受 TeamCity 给我们的，等我们决定要使用了，后续再更换数据也是可以的。但是内置的有一个注意点：'TeamCity with the native MSSQL external database driver is not compatible with Oracle Java 6 Update 29, due to a bug in Java itself. You can use earlier or later versions of Oracle Java.' 假设我们就选 Internal(HSQLDB) ，则在创建初始化数据库的过程稍微需要点时间，我这边是几分钟。 如上图所示，接受下协议 如上图所示，我们要创建一个顶级管理员账号，我个人习惯测试的账号是：admin，123456 如上图所示，安装完首次进来地址：http://192.168.1.113:8111/profile.html?tab=userGeneralSettings 我们可以完善一些管理员信息和基础配置信息，这些配置不配置都无所谓了，只是完善了可以更加好用而已 如果你有 SMTP 的邮箱，你可以来这里开启邮件通知功能：http://192.168.1.113:8111/admin/admin.html?item=email 如果你要开启通知功能那肯定下一步就是考虑通知内容的模板要如何设定：https://confluence.jetbrains.com/display/TCD9//Customizing+Notifications 模板存放路径在：/root/.BuildServer/config/_notifications，用的是 FreeMarker 的语法 项目的构建、管理 建议可以看下官网：https://confluence.jetbrains.com/display/TCD9/Configure+and+Run+Your+First+Build 现在让我们开始创建一个项目进行构建 项目管理地址：http://192.168.1.113:8111/admin/admin.html?item=projects > 假设我现在有一个项目的结构是这样的： Youshop-Parent，输出是 pom Youshop-manage，输出是 pom Youshop-pojo，输出 jar 我们现在以 Youshop-pojo 为例，让它自动构建并发布到 Nexus 中，其他项目道理是一样的，这里就不多说。 如上图，由于目前只要是公司的项目都应该是在版本控制的，所以这里我们选择：Create project from URL 如上图，我们可以看出 TeamCity 也支持 HTTP、SVN、Git 等链接方式。 输入你项目托管商的账号密码，我这里用的是 oschina 的。 账号、密码验证通过，现在可以给这个项目配置一个项目基本信息。 在从版本控制中下载文件和扫描文件 TeamCity 自动扫描到我是用 Maven 构建的项目，所以把 POM 文件找出来了，如果你一个项目有多种构建方式，有对应的配置文件的话，这里都会显示出来的。 我们勾选 Maven 前面的复选框，点击：Use Selected 由于我们的目标是构建完自动发布到 Nexus，所以我们的 Maven Goals 应该是：clean install deploy，这里我们应该点击：Edit，进行编辑。 如果你不懂 Maven Goals，那你需要学习下，这个很重要。 官网：http://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html 如上图，这台服务器必须装有 Maven、JDK 如上图，Goals 我们的目标是 clean install deploy 如上图，Maven Home 我建议是自己自定义路径，这样肯定不会有问题。所以你服务器上的 Maven 安装路径是什么你就在这里填写上去。Maven 目前支持的最高版本是：3.2.5 下载 Maven 3.2.5：http://archive.apache.org/dist/maven/maven-3/3.2.5/binaries/ 如上图，Java Parameters 我建议也是自己自定义路径，别选择其他选项。 如上图，点击 run，开始手动构建该项目 如上图，我们看到简略的构建日志 如上 2 张图，我们看到详细的构建内容 如上图，当我们版本控制中有提交的时候，TeamCity 会识别到记录 如上图，我们可以看到提交的 Commit Message 信息。 如上图，右边红圈的三个按钮是用来处理这次提交的，常用的是第一次按钮，点击对此次版本进行构建 如上图，如果你要看所有的提交记录，可以在 Change Log 看到并且指定版本构建 如上图，如果在你不想要这个项目的时候可以进行删除 如上图，因为 Goals 里面有 deploy 命令，所以构建完成会发布到 Nexus 中，这样团队的人就可以用到最新的代码了 如上 gif 图演示，项目常去的几个配置地方就是这样些了 配置自动构建触发行为 官网提供的触发行为有：https://confluence.jetbrains.com/display/TCD9/Configuring+Build+Triggers 下面我们举例说常见的：VCS Trigger、Schedule Trigger 如上图，点击 Add new trigger 添加触发器 如上图，常见的触发器就这些了 如上图，配置好 VCS Trigger 效果是，当我们有代码提交的时候，TeamCity 检查到新版本之后自动构建，这个最常用 如上图，Schedule Trigger 的作用就是定时构建，除了常用的几个输入框设置定时外，TeamCity 还可以使用 Cron 语法进行设置 TeamCity 采用的 Cron 语法是 Quartz，具体你可以看：Quartz CronTrigger Tutorial 如果你不懂 Cron 语法那就算了，但是我想做 Java 这个应该要会的 集成 IntelliJ IDEA 安装 IntelliJ IDEA：https://confluence.jetbrains.com/display/TCD9/IntelliJ+Platform+Plugin 如上图，我们可以直接连上 TeamCity 服务器，这里的用户名密码是 TeamCity 的账号系统。 如上图，连上去的效果是这里会打钩 如上图，我们可以直接把别人提交的内容做 patch 直接用 IntelliJ IDEA 进行整合 还有其他很多结合玩法大家可以自己去尝试下 其他 TeamCity 的插件列表：https://confluence.jetbrains.com/display/TW/TeamCity+Plugins 使用外部数据库： 使用外部数据库：https://confluence.jetbrains.com/display/TCD9/Setting+up+an+External+Database 迁移到外部数据库：https://confluence.jetbrains.com/display/TCD9/Migrating+to+an+External+Database 数据备份：https://confluence.jetbrains.com/display/TCD9/TeamCity+Data+Backup 代码检查功能： https://confluence.jetbrains.com/display/TCD9/Code+Quality+Tools https://confluence.jetbrains.com/display/TCD9/Code+Quality+Tools#CodeQualityTools-IntelliJIDEA-poweredCodeAnalysisTools https://confluence.jetbrains.com/pages/viewpage.action?pageId=74847276 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Nginx-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Nginx-Install-And-Settings.html","title":"Nginx 安装和配置","keywords":"","body":"Nginx 安装和配置 Nginx 说明 Nginx 是一个很强大的高性能 Web 和反向代理服务器，常被我们用作负载均衡服务器，也可以作为邮件代理服务器 Nginx WIKI：https://zh.wikipedia.org/zh/Nginx Nginx 百科：http://baike.baidu.com/item/nginx Nginx 官网：http://nginx.org/en/ Nginx 官网下载：http://nginx.org/en/download.html 源码包方式下载：http://nginx.org/en/download.html，注意该页面的：Stable version，这个表示稳定版本，2016-03-22 最新版本是：nginx-1.8.1，这是一个 tar.gz 的文件链接。 构建包方式下载：http://nginx.org/en/linux_packages.html#stable Nginx 文档： 优先：https://www.nginx.com/resources/wiki/ 次要：http://nginx.org/en/docs/ Nginx 模块地址：https://www.nginx.com/resources/wiki/modules/ 来自网络上的一个好介绍 来源：https://help.aliyun.com/knowledge_detail/6703521.html?spm=5176.788314854.2.2.CdMGlB 传统上基于进程或线程模型架构的 Web 服务通过每进程或每线程处理并发连接请求，这势必会在网络和 I/O 操作时产生阻塞，其另一个必然结果则是对内存或 CPU 的利用率低下。生成一个新的进程/线程需要事先备好其运行时环境，这包括为其分配堆内存和栈内存，以及为其创建新的执行上下文等。这些操作都需要占用 CPU，而且过多的进程/线程还会带来线程抖动或频繁的上下文切换，系统性能也会由此进一步下降。 在设计的最初阶段，Nginx 的主要着眼点就是其高性能以及对物理计算资源的高密度利用，因此其采用了不同的架构模型。受启发于多种操作系统设计中基于“事件”的高级处理机制，nginx采用了模块化、事件驱动、异步、单线程及非阻塞的架构，并大量采用了多路复用及事件通知机制。在 Nginx 中，连接请求由为数不多的几个仅包含一个线程的进程 Worker 以高效的回环(run-loop)机制进行处理，而每个 Worker 可以并行处理数千个的并发连接及请求。 如果负载以 CPU 密集型应用为主，如 SSL 或压缩应用，则 Worker 数应与 CPU 数相同；如果负载以 IO 密集型为主，如响应大量内容给客户端，则 Worker 数应该为 CPU 个数的 1.5 或 2 倍。 Nginx会按需同时运行多个进程：一个主进程(Master)和几个工作进程(Worker)，配置了缓存时还会有缓存加载器进程(Cache Loader)和缓存管理器进程(Cache Manager)等。所有进程均是仅含有一个线程，并主要通过“共享内存”的机制实现进程间通信。主进程以root用户身份运行，而 Worker、Cache Loader 和 Cache manager 均应以非特权用户身份运行。 主进程主要完成如下工作： 1.读取并验正配置信息； 2.创建、绑定及关闭套接字； 3.启动、终止及维护worker进程的个数； 4.无须中止服务而重新配置工作特性； 5.控制非中断式程序升级，启用新的二进制程序并在需要时回滚至老版本； 6.重新打开日志文件，实现日志滚动； 7.编译嵌入式perl脚本； Worker 进程主要完成的任务包括： 1.接收、传入并处理来自客户端的连接； 2.提供反向代理及过滤功能； 3.nginx任何能完成的其它任务； Cache Loader 进程主要完成的任务包括： 1.检查缓存存储中的缓存对象； 2.使用缓存元数据建立内存数据库； Cache Manager 进程的主要任务： 1.缓存的失效及过期检验； Nginx 的 Docker 部署 预设好目录，在宿主机上创建下面目录：mkdir -p /data/docker/nginx/logs /data/docker/nginx/conf 重点：先准备好你的 nginx.conf 文件，存放在宿主机的：vim /data/docker/nginx/conf/nginx.conf 目录下，等下需要映射。 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost 127.0.0.1 193.112.221.203 youmeek.com; location / { root /usr/share/nginx/html; index index.html index.htm; } } } 官网镜像：https://hub.docker.com/_/nginx/ 下载镜像：docker pull nginx:1.12.2 运行容器：docker run --name youmeek-nginx -p 80:80 -v /data/docker/nginx/logs:/var/log/nginx -v /data/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx:1.12.2 重新加载配置（目前测试无效，只能重启服务）：docker exec -it youmeek-nginx nginx -s reload 停止服务：docker exec -it youmeek-nginx nginx -s stop 或者：docker stop youmeek-nginx 重新启动服务：docker restart youmeek-nginx YUM 安装（版本一般滞后半年左右） 安装：yum install -y nginx，同时增加了一个 nginx 用户组和用户 默认配置文件位置：vim /etc/nginx/nginx.conf 其他配置文件位置：cd /etc/nginx/conf.d/ 模块配置文件位置：cd /usr/share/nginx/modules/ 默认 HTML 静态文件位置：cd /usr/share/nginx/html log 存放目录：cd /var/log/nginx/ 状态：systemctl status nginx 启动：systemctl start nginx 启动：systemctl stop nginx 刷新配置：nginx -s reload 查看版本和 YUM 自带的模块：nginx -V Nginx 源码编译安装（带监控模块） 官网下载最新稳定版本 1.8.1，大小：814K 官网安装说明：https://www.nginx.com/resources/wiki/start/topics/tutorials/install/ 源码编译配置参数说明： https://www.nginx.com/resources/wiki/start/topics/tutorials/installoptions/ http://nginx.org/en/docs/configure.html 开始安装： 安装依赖包：yum install -y gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel 预设几个文件夹，方便等下安装的时候有些文件可以进行存放： mkdir -p /usr/local/nginx /var/log/nginx /var/temp/nginx /var/lock/nginx 下载源码包：wget http://nginx.org/download/nginx-1.8.1.tar.gz 解压：tar zxvf nginx-1.8.1.tar.gz 进入解压后目录：cd nginx-1.8.1/ 编译配置： ./configure \\ --prefix=/usr/local/nginx \\ --pid-path=/var/local/nginx/nginx.pid \\ --lock-path=/var/lock/nginx/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --with-http_ssl_module \\ --with-http_stub_status_module \\ --http-scgi-temp-path=/var/temp/nginx/scgi 编译：make 安装：make install 启动 Nginx 先检查是否在 /usr/local 目录下生成了 Nginx 等相关文件：cd /usr/local/nginx;ll，正常的效果应该是显示这样的： drwxr-xr-x. 2 root root 4096 3月 22 16:21 conf drwxr-xr-x. 2 root root 4096 3月 22 16:21 html drwxr-xr-x. 2 root root 4096 3月 22 16:21 sbin 如果要检查刚刚编译的哪些模块，可以：nginx -V nginx version: nginx/1.8.0 built by gcc 4.4.7 20120313 (Red Hat 4.4.7-18) (GCC) built with OpenSSL 1.0.1e-fips 11 Feb 2013 TLS SNI support enabled configure arguments: --user=nginx --group=nginx --prefix=/usr/local/nginx --pid-path=/usr/local/nginx/run/nginx.pid --lock-path=/usr/local/nginx/lock/nginx.lock --with-http_ssl_module --with-http_dav_module --with-http_flv_module --with-http_gzip_static_module --with-http_stub_status_module 停止防火墙：service iptables stop 或是把 80 端口加入到的排除列表： sudo iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT sudo service iptables save sudo service iptables restart 启动：/usr/local/nginx/sbin/nginx，启动完成 shell 是不会有输出的 检查 时候有 Nginx 进程：ps aux | grep nginx，正常是显示 3 个结果出来 检查 Nginx 是否启动并监听了 80 端口：netstat -ntulp | grep 80 访问：192.168.1.114，如果能看到：Welcome to nginx!，即可表示安装成功 检查 Nginx 启用的配置文件是哪个：/usr/local/nginx/sbin/nginx -t 刷新 Nginx 配置后重启：/usr/local/nginx/sbin/nginx -s reload 停止 Nginx：/usr/local/nginx/sbin/nginx -s stop 如果访问不了，或是出现其他信息看下错误立即：vim /var/log/nginx/error.log 把 Nginx 添加到系统服务中 新建文件：vim /etc/init.d/nginx 添加如下内容： #!/bin/bash #nginx执行程序路径需要修改 nginxd=/usr/local/nginx/sbin/nginx # nginx配置文件路径需要修改 nginx_config=/usr/local/nginx/conf/nginx.conf # pid 地址需要修改 nginx_pid=/var/local/nginx/nginx.pid RETVAL=0 prog=\"nginx\" # Source function library. . /etc/rc.d/init.d/functions # Source networking configuration. . /etc/sysconfig/network # Check that networking is up. [ ${NETWORKING} = \"no\" ] && exit 0 [ -x $nginxd ] || exit 0 # Start nginx daemons functions. start() { if [ -e $nginx_pid ];then echo \"nginx already running....\" exit 1 fi echo -n $\"Starting $prog: \" daemon $nginxd -c ${nginx_config} RETVAL=$? echo [ $RETVAL = 0 ] && touch /var/lock/subsys/nginx return $RETVAL } # Stop nginx daemons functions. # pid 地址需要修改 stop() { echo -n $\"Stopping $prog: \" killproc $nginxd RETVAL=$? echo [ $RETVAL = 0 ] && rm -f /var/lock/subsys/nginx /var/local/nginx/nginx.pid } # reload nginx service functions. reload() { echo -n $\"Reloading $prog: \" #kill -HUP `cat ${nginx_pid}` killproc $nginxd -HUP RETVAL=$? echo } # See how we were called. case \"$1\" in start) start ;; stop) stop ;; reload) reload ;; restart) stop start ;; status) status $prog RETVAL=$? ;; *) echo $\"Usage: $prog {start|stop|restart|reload|status|help}\" exit 1 esac exit $RETVAL 修改权限：chmod 755 /etc/init.d/nginx 启动服务：service nginx start 停止服务：service nginx stop 重启服务：service nginx restart Nginx 无缝升级 使用新的参数configure后执行make重新编译,注意之后不要执行make install.新构建的Nginx会在objs目录下 备份旧的Nginx cp 老的nginx目录/sbin/nginx 老的nginx目录/sbin/nginx.old 复制新的Nginx cp ./objs/nginx 老的nginx目录/sbin/,可能提示Nginx被占用,如果是则强制覆盖即可. 检查下Makefile的更新指令 cat Makefile,检查下路径是否匹配,一般没什么问题毕竟是根据你的参数生成的文件. upgrade: /usr/local/nginx/sbin/nginx -t kill -USR2 `cat /var/local/nginx/nginx.pid` sleep 1 test -f /var/local/nginx/nginx.pid.oldbin kill -QUIT `cat /var/local/nginx/nginx.pid.oldbin` 更新 make upgrade Nginx 全局变量 $arg_PARAMETER #这个变量包含GET请求中，如果有变量PARAMETER时的值。 $args #这个变量等于请求行中(GET请求)的参数，例如foo=123&bar=blahblah; $binary_remote_addr #二进制的客户地址。 $body_bytes_sent #响应时送出的body字节数数量。即使连接中断，这个数据也是精确的。 $content_length #请求头中的Content-length字段。 $content_type #请求头中的Content-Type字段。 $cookie_COOKIE #cookie COOKIE变量的值 $document_root #当前请求在root指令中指定的值。 $document_uri #与$uri相同。 $host #请求主机头字段，否则为服务器名称。 $hostname #Set to the machine’s hostname as returned by gethostname $http_HEADER $is_args #如果有$args参数，这个变量等于”?”，否则等于”\"，空值。 $http_user_agent #客户端agent信息 $http_cookie #客户端cookie信息 $limit_rate #这个变量可以限制连接速率。 $query_string #与$args相同。 $request_body_file #客户端请求主体信息的临时文件名。 $request_method #客户端请求的动作，通常为GET或POST。 $remote_addr #客户端的IP地址。 $remote_port #客户端的端口。 $remote_user #已经经过Auth Basic Module验证的用户名。 $request_completion #如果请求结束，设置为OK. 当请求未结束或如果该请求不是请求链串的最后一个时，为空(Empty)。 $request_method #GET或POST $request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 $request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。不能修改。 $scheme #HTTP方法（如http，https）。 $server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 $server_name #服务器名称。 $server_port #请求到达服务器的端口号。 $uri #不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。该值有可能和$request_uri 不一致。 $request_uri是浏览器发过来的值。该值是rewrite后的值。例如做了internal redirects后。 Nginx 配置 Nginx 默认配置文件：vim /usr/local/nginx/conf/nginx.conf Nginx 在 1.8.1 版本下的默认配置（去掉注释） user root;#我这里习惯使用 root，所以这里需要这样设置。如果你有为你的 nginx 专门配置一个用户，这里需要改为你的用户 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } HTTP 服务，虚拟主机 停止防火墙：service iptables stop，防止出现特别干扰 编辑默认的配置文件：vim /usr/local/nginx/conf/nginx.conf 设置两个虚拟主机（通过端口来区分开） user root;#我这里习惯使用 root，所以这里需要这样设置。如果你有为你的 nginx 专门配置一个用户，这里需要改为你的用户 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 一个 server 代表一个虚拟主机 server { listen 80; server_name localhost; location / { # 虚拟机根目录是 /usr/local/nginx/html 目录 root html; # 虚拟机首页是 /usr/local/nginx/html 目录下这两个文件 index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { # 第二个虚拟机的端口是 90，服务地址还是本地 listen 90; server_name localhost; location / { root html90; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 设置两个虚拟主机（通过域名来区分开） user root;#我这里习惯使用 root，所以这里需要这样设置。如果你有为你的 nginx 专门配置一个用户，这里需要改为你的用户 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 一个 server 代表一个虚拟主机 server { listen 80; # 两个虚拟主机都使用 80 端口，设置不同域名 server_name code.youmeek.com; location / { # 虚拟机根目录是 /usr/local/nginx/html 目录 root html; # 虚拟机首页是 /usr/local/nginx/html 目录下这两个文件 index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { listen 80; # 两个虚拟主机都使用 80 端口，设置不同域名 server_name i.youmeek.com; location / { root html-i; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 反向代理和负载均衡 最精简的环境：一台虚拟机 1 个 JDK 1 个 Nginx 2 个 Tomcat Nginx 配置： user root;#我这里习惯使用 root，所以这里需要这样设置。如果你有为你的 nginx 专门配置一个用户，这里需要改为你的用户 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 自己定义的两个 tomcat 请求地址和端口 # 也就是当浏览器请求：tomcat.youmeek.com 的时候从下面这两个 tomcat 中去找一个进行转发 upstream tomcatCluster { server 192.168.1.114:8080; server 192.168.1.114:8081; # 添加 weight 字段可以表示权重，值越高权重越大，默认值是 1，最大值官网没说，一般如果设置也就设置 3,5,7 这样的数 # 官网：https://www.nginx.com/resources/admin-guide/load-balancer/#weight # server 192.168.1.114:8080 weight=2; # server 192.168.1.114:8081 weight=1; } server { listen 80; server_name tomcat.youmeek.com; location / { proxy_pass http://tomcatCluster; index index.html index.htm; } } } 配置 HTTPS 服务（SSL 证书配置） 免费申请 SSL 证书渠道 教程：https://www.wn789.com/4394.html SSL For Free：https://www.sslforfree.com 配置要点其实就是下面该图： 一般你会下载下面两个文件：certificate.crt，private.key 如果你需要把 crt 和 key 的证书转换成 keystore（如果你有这个需求的话） 从 key 和 crt 生成 pkcs12 格式的 keystore，生成过程会让人你输入密码，这个密码下面会用到，我这里假设输入 123456 openssl pkcs12 -export -in certificate.crt -inkey private.key -out youmeek.p12 -name youmeek -CAfile certificate.crt -caname -chain keytool -importkeystore -v -srckeystore youmeek.p12 -srcstoretype pkcs12 -srcstorepass 123456 -destkeystore youmeek.keystore -deststoretype jks -deststorepass 123456 修改 nginx 配置文件，增加对 HTTPS 支持（下面的配置是基于默认安装 nginx 后的配置） vim /usr/local/nginx/conf/nginx.conf worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 如果访问 http 也直接跳转到 https server { listen 80; server_name sso.youmeek.com; return 301 https://$server_name$request_uri; } # crt 和 key 文件的存放位置根据你自己存放位置进行修改 server { listen 443; server_name sso.youmeek.com; ssl on; ssl_certificate /opt/ssl/certificate.crt; ssl_certificate_key /opt/ssl/private.key; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } Nginx 压力测试 AB 测试工具安装：yum install -y httpd-tools 使用： ab -n 1000 -c 100 http://www.baidu.com/ -n 总的请求数 -c 单个时刻并发数 压测结果： This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking juejin.im (be patient) Completed 100 requests Completed 200 requests Completed 300 requests Completed 400 requests Completed 500 requests Completed 600 requests Completed 700 requests Completed 800 requests Completed 900 requests Completed 1000 requests Finished 1000 requests Server Software: nginx Server Hostname: juejin.im Server Port: 443 SSL/TLS Protocol: TLSv1.2,ECDHE-RSA-AES256-GCM-SHA384,2048,256 Document Path: / Document Length: 271405 bytes Concurrency Level: 100（并发数：100） Time taken for tests: 120.042 seconds（一共用了 120 秒） Complete requests: 1000（总的请求数：1000） Failed requests: 0（失败的请求次数） Write errors: 0 Total transferred: 271948000 bytes HTML transferred: 271405000 bytes Requests per second: 8.33 [#/sec] (mean)（QPS 系统吞吐量，平均每秒请求数，计算公式 = 总请求数 / 总时间数） Time per request: 12004.215 [ms] (mean)（毫秒，平均每次并发 100 个请求的处理时间） Time per request: 120.042 [ms] (mean, across all concurrent requests)（毫秒，并发 100 下，平均每个请求处理时间） Transfer rate: 2212.34 [Kbytes/sec] received（平均每秒网络流量） Connection Times (ms) min mean[+/-sd] median max Connect: 57 159 253.6 77 1002 Processing: 1139 11570 2348.2 11199 36198 Waiting: 156 1398 959.4 1279 22698 Total: 1232 11730 2374.1 11300 36274 Percentage of the requests served within a certain time (ms) 50% 11300 66% 11562 75% 11863 80% 12159 90% 13148 95% 15814 98% 18882 99% 22255 100% 36274 (longest request) Nginx 常规优化 增加工作线程数和并发连接数 修改参数：worker_processes 1; 该参数是指：nginx 要开启的工作进程数（worker process），默认是 1，一把你不需要修改。（除了工作进程，还有一种 master process 的概念） 但是如果请求数比较多，一般推荐最大是修改成 CPU 的内核数等同的值，以增加能力。 修改 events 参数 events { # 每一个进程可以打开的最大连接数（这个参数是受限制于系统参数的，默认是 1024）（进程数是上面 worker_processes 决定的） worker_connections 1024; # 可以一次建立多个连接 multi_accept on; # epoll 模式效率最高 use epoll; } 启动长连接 http { sendfile on; # 减少文件在应用和内核之间的拷贝 tcp_nopush on; # 当数据包达到一定大小再发送 keepalive_timeout 65; upstream tomcatCluster { server 192.168.1.114:8080; server 192.168.1.114:8081; keepalive 300; # 300 个长连接 } } 启用缓存和压缩 http { gzip on; gzip_buffers 8 16k; # 这个限制了nginx不能压缩大于128k的文件 gzip_min_length 512; # 单位byte gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\"; gzip_http_version 1.1; # 1.0 的版本可能会有问题 gzip_types text/plain text/css application/javascript application/x-javascript application/json application/xml; } 操作系统优化（机器好点的时候） 修改 sysctl 参数 修改配置文件：vim /etc/sysctl.conf net.ipv4.tcp_fin_timeout = 10 #保持在FIN-WAIT-2状态的时间，使系统可以处理更多的连接。此参数值为整数，单位为秒。 net.ipv4.tcp_tw_reuse = 1 #开启重用，允许将TIME_WAIT socket用于新的TCP连接。默认为0，表示关闭。 net.ipv4.tcp_tw_recycle = 0 #开启TCP连接中TIME_WAIT socket的快速回收。默认值为0，表示关闭。 net.ipv4.tcp_syncookies = 1 #开启SYN cookie，出现SYN等待队列溢出时启用cookie处理，防范少量的SYN攻击。默认为0，表示关闭。 net.core.somaxconn = 1024 #定义了系统中每一个端口最大的监听队列的长度, 对于一个经常处理新连接的高负载 web服务环境来说，默认值为128，偏小。 刷新 sysctl 配置：sysctl -p 修改 limits 参数 ElasticSearch 一般也是要修改该参数 修改配置文件：vim /etc/security/limits.conf * soft nofile 262144 * hard nofile 262144 * soft core unlimited * soft stack 262144 Nginx 监控模块 如果你需要监控 nginx 情况可以安装的加入这个模块 http_stub_status_module： ./configure \\ --prefix=/usr/local/nginx \\ --pid-path=/var/local/nginx/nginx.pid \\ --lock-path=/var/lock/nginx/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --with-http_ssl_module \\ --http-scgi-temp-path=/var/temp/nginx/scgi \\ --with-http_stub_status_module 然后在 nginx.conf 文件的 location 区域增加：stub_status on; location /nginx_status { allow 127.0.0.1; deny all; stub_status on; access_log off; } 当你访问：http://127.0.0.1/nginx_status，会得到类似下面的结果 其中配置的 allow 127.0.0.1; 表示只允许本机访问：http://127.0.0.1/nginx_status 才能看到 所以我们也可以通过 curl 访问本机看到结果，不一定要对外开放。 deny all; 除了被允许的，其他所有人都不可以访问 Active connections: 1 server accepts handled requests 3 6 9 Reading: 0 Writing: 5 Waiting: 0 Active connections: 当前活动连接数，包含 waiting 的连接（最常需要看的就是这个参数） Server accepts handled requests: Nginx总共处理了 3 个连接,成功创建 6 次握手(证明中间没有失败的),总共处理了 9 个请求. Reading: Nginx 读取到客户端的 Header 信息数，如果很大，说明现在很多请求正在过来 Writing: Nginx 返回给客户端的 Header 信息数，如果很大，说明现在又很多请求正在响应 Waiting: 开启keep-alive的情况下,这个值等于 active – (reading + writing),意思就是 Nginx 已经处理完成,正在等候下一次请求指令的驻留连接. 所以,在访问效率高,请求很快被处理完毕的情况下,Waiting数比较多是正常的。如果reading + writing数较多,则说明并发访问量非常大,正在处理过程中 Nginx 配置文件常用配置积累 location 配置 = 开头表示精确匹配 ^~ 开头表示uri以某个常规字符串开头，不是正则匹配 ~ 开头表示区分大小写的正则匹配; ~* 开头表示不区分大小写的正则匹配 / 通用匹配, 如果没有其它匹配,任何请求都会匹配到 location / { } location /user { } location = /user { } location /user/ { } location ^~ /user/ { } location /user/youmeek { } location ~ /user/youmeek { } location ~ ^(/cas/|/casclient1/|/casclient2/|/casclient3/) { } location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|ico|woff|woff2|ttf|eot|txt)$ { } location ~ .*$ { } 链接 aa 下，查询参数包含 bb 这里必须使用：IF，但是 IF 是不被推荐的：If Is Evil location /aa/ { if ( $args ~* '(.*bb.*)' ) { return 601; } } location /aa/ { if ($args ~ tag=bb){ return 601; } } HTTP 服务，绑定多个域名 https://www.ttlsa.com/nginx/use-nginx-proxy/ 安装第三方模块 生成规格图 启用 Gzip 压缩 防盗链 https://help.aliyun.com/knowledge_detail/5974693.html?spm=5176.788314853.2.18.s4z1ra Nginx 禁止特定用户代理（User Agents）访问，静止指定 IP 访问 https://www.ttlsa.com/nginx/how-to-block-user-agents-using-nginx/ https://help.aliyun.com/knowledge_detail/5974693.html?spm=5176.788314853.2.18.s4z1ra <> <> <> Nginx 缓存 Nginx 自动分割日志文件 在 Tomcat 安装和配置、优化 文章已经使用了 cronolog，这里也借用 cronolog 来实现分割。具体安装看文章。 创建目录：mkdir -p /data/nginx/log/logs 创建命名管道：mkfifo /data/nginx/log/access_log.log 配置 cronolog（按天）：nohup cat /data/nginx/log/access_log.log | /usr/sbin/cronolog /data/nginx/log/logs/access-%Y-%m-%d.log & 配置 cronolog（按月）：nohup cat /data/nginx/log/access_log.log | /usr/sbin/cronolog /data/nginx/log/logs/access-%Y-%m.log & 编辑 nginx 配置文件，配置 log 位置：access_log /data/nginx/log/access_log.log; 重启 nginx，最终可以在 /data/nginx/log/logs 目录下看到生成的 log Nginx 处理跨域请求 安全相预防 在配置文件中设置自定义缓存以限制缓冲区溢出攻击的可能性 client_body_buffer_size 1K; client_header_buffer_size 1k; client_max_body_size 1k; large_client_header_buffers 2 1k; 将timeout设低来防止DOS攻击 所有这些声明都可以放到主配置文件中。 client_body_timeout 10; client_header_timeout 10; keepalive_timeout 5 5; send_timeout 10; 限制用户连接数来预防DOS攻击 limit_zone slimits $binary_remote_addr 5m; limit_conn slimits 5; 使用 logrotate 做 nginx 日志轮询分割 前提： 我 nginx 的成功日志路径：/var/log/nginx/access.log 我 nginx 的错误日志路径：/var/log/nginx/error.log pid 路径：/var/local/nginx/nginx.pid 一般情况 CentOS 是装有：logrotate，你可以检查下：rpm -ql logrotate，如果有相应结果，则表示你也装了。 logrotate 配置文件一般在： 全局配置：/etc/logrotate.conf 通用配置文件，可以定义全局默认使用的选项。 自定义配置，放在这个目录下的都算是：/etc/logrotate.d/ 针对 nginx 创建自定义的配置文件：vim /etc/logrotate.d/nginx 文件内容如下： /var/log/nginx/access.log /var/log/nginx/error.log { create 644 root root notifempty daily rotate 15 missingok dateext sharedscripts postrotate if [ -f /var/local/nginx/nginx.pid ]; then kill -USR1 `cat /var/local/nginx/nginx.pid` fi endscript } /var/log/nginx/access.log /var/log/nginx/error.log：多个文件用空格隔开，也可以用匹配符：/var/log/nginx/*.log notifempty：如果是空文件的话，不转储 create 644 root root：create mode owner group 转储文件，使用指定的文件模式创建新的日志文件 调用频率，有：daily，weekly，monthly可选 rotate 15：一次将存储15个归档日志。对于第16个归档，时间最久的归档将被删除。 sharedscripts：所有的日志文件都轮转完毕后统一执行一次脚本 missingok：如果日志文件丢失，不报错继续执行下一个 dateext：文件后缀是日期格式,也就是切割后文件是:xxx.log-20131216.gz 这样,如果注释掉,切割出来是按数字递增,即前面说的 xxx.log-1 这种格式 postrotate：执行命令的开始标志 endscripthttp:执行命令的结束标志 if 判断的意思不是中止Nginx的进程，而是传递给它信号重新生成日志，如果nginx没启动不做操作 更多参数可以看：http://www.cnblogs.com/zengkefu/p/5498324.html 手动执行测试：/usr/sbin/logrotate -vf /etc/logrotate.d/nginx 参数：‘-f’选项来强制logrotate轮循日志文件，‘-v’参数提供了详细的输出。 验证是否手动执行成功，查看 cron 的日志即可：grep logrotate /var/log/cron 设置 crontab 定时任务：vim /etc/crontab，添加下面内容： //每天02点10分执行一次 10 02 * * * /usr/sbin/logrotate -f /etc/logrotate.d/nginx 杂七杂八 nginx实现简体繁体字互转以及中文转拼音 nginx记录分析网站响应慢的请求(ngx_http_log_request_speed) nginx空白图片(empty_gif模块) 资料 https://help.aliyun.com/knowledge_detail/5974693.html?spm=5176.788314853.2.18.s4z1ra http://www.ydcss.com/archives/466 http://blog.sae.sina.com.cn/archives/2107 http://www.nginx.cn/273.html http://printfabcd.iteye.com/blog/1200382 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/wrk-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/wrk-Install-And-Settings.html","title":"wrk 安装和配置","keywords":"","body":"wrk 安装和配置 wrk 说明 wrk 相对于 ab 来说最大的优点是它支持多线程，可以有更大的并发量 安装 CentOS 7.4 官网说明：https://github.com/wg/wrk/wiki/Installing-Wrk-on-Linux # 安装工具包的时候差不多有 90 个左右的子工具 sudo yum groupinstall 'Development Tools' sudo yum install -y openssl-devel git git clone --depth=1 https://github.com/wg/wrk.git wrk cd wrk make # move the executable to somewhere in your PATH sudo cp wrk /usr/local/bin 查看帮助：wrk --help 使用 启用 10 个线程，每个线程发起 100 个连接，持续 15 秒：wrk -t10 -c100 -d15s http://www.baidu.com 最终报告： Running 15s test @ http://www.baidu.com 10 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 208.39ms 324.00ms 1.91s 87.70% Req/Sec 82.68 64.81 414.00 70.60% 11345 requests in 15.02s, 166.51MB read Socket errors: connect 0, read 20, write 0, timeout 59 Requests/sec: 755.26 Transfer/sec: 11.08MB 使用 lua 脚本（发送一个 post 请求） 创建：vim /opt/post-wrk.lua wrk.method = \"POST\" wrk.body = \"hms_user_id=222222&routing_key=ad.sys_user.add\" wrk.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\" 测试：wrk -t10 -c100 -d15s --script=/opt/post-wrk.lua --latency http://127.0.0.1:9090/websocket/api/send-by-user-id 其他说明 wrk 使用的是 HTTP/1.1，缺省开启的是长连接 要测试短连接：wrk -H \"Connection: Close\" -c 100 -d 10 http://domain/path 资料 https://huoding.com/2017/05/31/620 https://zjumty.iteye.com/blog/2221040 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/FastDFS-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/FastDFS-Install-And-Settings.html","title":"FastDFS 安装和配置","keywords":"","body":"FastDFS 安装和配置 FastDFS 介绍 FastDFS 介绍：http://www.oschina.net/p/fastdfs 官网下载 1：https://github.com/happyfish100/fastdfs/releases 官网下载 2：https://sourceforge.net/projects/fastdfs/files/ 官网下载 3：http://code.google.com/p/fastdfs/downloads/list 主要场景： 小图片 音频、小视频 其他类型小文件 更加复杂的文件存储场景可以选择：Ceph 支持对象存储、块存储和文件存储 高性能、高可靠性和高扩展 单机安装部署（CentOS 6.7 环境） 环境准备： 已经安装好 Nginx 软件准备： FastDFS_v5.05.tar.gz fastdfs-nginx-module_v1.16.tar.gz libfastcommon-1.0.7.tar.gz 安装依赖包：yum install -y libevent 安装 libfastcommon-1.0.7.tar.gz 解压：tar zxvf libfastcommon-1.0.7.tar.gz 进入解压后目录：cd libfastcommon-1.0.7/ 编译：./make.sh 安装：./make.sh install 设置几个软链接：ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so 设置几个软链接：ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so 设置几个软链接：ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so 设置几个软链接：ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 安装 tracker （跟踪器）服务 FastDFS_v5.08.tar.gz 解压：tar zxvf FastDFS_v5.05.tar.gz 进入解压后目录：cd FastDFS/ 编译：./make.sh 安装：./make.sh install 安装结果：/usr/bin 存放有编译出来的文件 /etc/fdfs 存放有配置文件 配置 tracker 服务 复制一份配置文件：cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf 编辑：vim /etc/fdfs/tracker.conf，编辑内容看下面中文注释disabled=false bind_addr= port=22122 connect_timeout=30 network_timeout=60 # 下面这个路径是保存 store data 和 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/tracker/data-and-log base_path=/opt/fastdfs/tracker/data-and-log max_connections=256 accept_threads=1 work_threads=4 store_lookup=2 store_group=group2 store_server=0 store_path=0 download_server=0 reserved_storage_space = 10% log_level=info run_by_group= run_by_user= allow_hosts=* sync_log_buff_interval = 10 check_active_interval = 120 thread_stack_size = 64KB storage_ip_changed_auto_adjust = true storage_sync_file_max_delay = 86400 storage_sync_file_max_time = 300 use_trunk_file = false slot_min_size = 256 slot_max_size = 16MB trunk_file_size = 64MB trunk_create_file_advance = false trunk_create_file_time_base = 02:00 trunk_create_file_interval = 86400 trunk_create_file_space_threshold = 20G trunk_init_check_occupying = false trunk_init_reload_from_binlog = false trunk_compress_binlog_min_interval = 0 use_storage_id = false storage_ids_filename = storage_ids.conf id_type_in_filename = ip store_slave_file_use_link = false rotate_error_log = false error_log_rotate_time=00:00 rotate_error_log_size = 0 log_file_keep_days = 0 use_connection_pool = false connection_pool_max_idle_time = 3600 http.server_port=8080 http.check_alive_interval=30 http.check_alive_type=tcp http.check_alive_uri=/status.html 启动 tracker 服务：/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf 重启 tracker 服务：/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart 查看是否有 tracker 进程：ps aux | grep tracker storage （存储节点）服务部署 一般 storage 服务我们会单独装一台机子，但是这里为了方便我们安装在同一台。 如果 storage 单独安装的话，那上面安装的步骤都要在走一遍，只是到了编辑配置文件的时候，编辑的是 storage.conf 而已 复制一份配置文件：cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf 编辑：vim /etc/fdfs/storage.conf，编辑内容看下面中文注释disabled=false group_name=group1 bind_addr= client_bind=true port=23000 connect_timeout=30 network_timeout=60 heart_beat_interval=30 stat_report_interval=60 # 下面这个路径是保存 store data 和 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/storage/data-and-log base_path=/opt/fastdfs/storage/data-and-log max_connections=256 buff_size = 256KB accept_threads=1 work_threads=4 disk_rw_separated = true disk_reader_threads = 1 disk_writer_threads = 1 sync_wait_msec=50 sync_interval=0 sync_start_time=00:00 sync_end_time=23:59 write_mark_file_freq=500 store_path_count=1 # 图片实际存放路径，如果有多个，这里可以有多行： # store_path0=/opt/fastdfs/storage/images-data0 # store_path1=/opt/fastdfs/storage/images-data1 # store_path2=/opt/fastdfs/storage/images-data2 # 创建目录：mkdir -p /opt/fastdfs/storage/images-data store_path0=/opt/fastdfs/storage/images-data subdir_count_per_path=256 # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 log_level=info run_by_group= run_by_user= allow_hosts=* file_distribute_path_mode=0 file_distribute_rotate_count=100 fsync_after_written_bytes=0 sync_log_buff_interval=10 sync_binlog_buff_interval=10 sync_stat_file_interval=300 thread_stack_size=512KB upload_priority=10 if_alias_prefix= check_file_duplicate=0 file_signature_method=hash key_namespace=FastDFS keep_alive=0 use_access_log = false rotate_access_log = false access_log_rotate_time=00:00 rotate_error_log = false error_log_rotate_time=00:00 rotate_access_log_size = 0 rotate_error_log_size = 0 log_file_keep_days = 0 file_sync_skip_invalid_record=false use_connection_pool = false connection_pool_max_idle_time = 3600 http.domain_name= http.server_port=8888 启动 storage 服务：/usr/bin/fdfs_storaged /etc/fdfs/storage.conf，首次启动会很慢，因为它在创建预设存储文件的目录 重启 storage 服务：/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart 查看是否有 storage 进程：ps aux | grep storage 测试是否部署成功 利用自带的 client 进行测试 复制一份配置文件：cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf 编辑：vim /etc/fdfs/client.conf，编辑内容看下面中文注释connect_timeout=30 network_timeout=60 # 下面这个路径是保存 store log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/client/data-and-log base_path=/opt/fastdfs/client/data-and-log # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 log_level=info use_connection_pool = false connection_pool_max_idle_time = 3600 load_fdfs_parameters_from_tracker=false use_storage_id = false storage_ids_filename = storage_ids.conf http.tracker_server_port=80 在终端中通过 shell 上传 opt 目录下的一张图片：/usr/bin/fdfs_test /etc/fdfs/client.conf upload /opt/test.jpg 如下图箭头所示，生成的图片地址为：http://192.168.1.114/group1/M00/00/00/wKgBclb0aqWAbVNrAAAjn7_h9gM813_big.jpg 即使我们现在知道图片的访问地址我们也访问不了，因为我们还没装 FastDFS 的 Nginx 模块 安装 fastdfs-nginx-module_v1.16.tar.gz，安装 Nginx 第三方模块相当于这个 Nginx 都是要重新安装一遍的 解压 Nginx 模块：tar zxvf fastdfs-nginx-module_v1.16.tar.gz，得到目录地址：/opt/setups/FastDFS/fastdfs-nginx-module 编辑 Nginx 模块的配置文件：vim /opt/setups/FastDFS/fastdfs-nginx-module/src/config 找到下面一行包含有 local 字眼去掉，因为这三个路径根本不是在 local 目录下的。CORE_INCS=\"$CORE_INCS /usr/local/include/fastdfs /usr/local/include/fastcommon/\" 改为如下：CORE_INCS=\"$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/\" 复制文件：cp /opt/setups/FastDFS/FastDFS/conf/http.conf /etc/fdfs 复制文件：cp /opt/setups/FastDFS/FastDFS/conf/mime.types /etc/fdfs 安装 Nginx 和 Nginx 第三方模块 安装 Nginx 依赖包：yum install -y gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel 预设几个文件夹，方便等下安装的时候有些文件可以进行存放： mkdir -p /usr/local/nginx /var/log/nginx /var/temp/nginx /var/lock/nginx 解压 Nginx：tar zxvf /opt/setups/nginx-1.8.1.tar.gz 进入解压后目录：cd /opt/setups/nginx-1.8.1/ 编译配置：（注意最后一行）./configure \\ --prefix=/usr/local/nginx \\ --pid-path=/var/local/nginx/nginx.pid \\ --lock-path=/var/lock/nginx/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --http-scgi-temp-path=/var/temp/nginx/scgi \\ --add-module=/opt/setups/FastDFS/fastdfs-nginx-module/src 编译：make 安装：make install 复制 Nginx 模块的配置文件：cp /opt/setups/FastDFS/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs 编辑 Nginx 模块的配置文件：vim /etc/fdfs/mod_fastdfs.conf，编辑内容看下面中文注释 如果在已经启动 Nginx 的情况下修改下面内容记得要重启 Nginx。 connect_timeout=2 network_timeout=30 # 下面这个路径是保存 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/fastdfs-nginx-module/data-and-log base_path=/opt/fastdfs/fastdfs-nginx-module/data-and-log load_fdfs_parameters_from_tracker=true storage_sync_file_max_delay = 86400 use_storage_id = false storage_ids_filename = storage_ids.conf # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 storage_server_port=23000 group_name=group1 # 因为我们访问图片的地址是：http://192.168.1.114/group1/M00/00/00/wKgBclb0aqWAbVNrAAAjn7_h9gM813_big.jpg # 该地址前面是带有 /group1/M00，所以我们这里要使用 true，不然访问不到（原值是 false） url_have_group_name = true store_path_count=1 # 图片实际存放路径，如果有多个，这里可以有多行： # store_path0=/opt/fastdfs/storage/images-data0 # store_path1=/opt/fastdfs/storage/images-data1 # store_path2=/opt/fastdfs/storage/images-data2 store_path0=/opt/fastdfs/storage/images-data log_level=info log_filename= response_mode=proxy if_alias_prefix= flv_support = true flv_extension = flv group_count = 0 编辑 Nginx 配置文件 # 注意这一行行，我特别加上了使用 root 用户去执行，不然有些日记目录没有权限访问 user root; worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; # 访问本机 server_name 192.168.1.114; # 拦截包含 /group1/M00 请求，使用 fastdfs 这个 Nginx 模块进行转发 location /group1/M00 { ngx_fastdfs_module; } } } 启动 Nginx 停掉防火墙：service iptables stop 启动：/usr/local/nginx/sbin/nginx，启动完成 shell 是不会有输出的 访问：192.168.1.114，如果能看到：Welcome to nginx!，即可表示安装成功 检查 时候有 Nginx 进程：ps aux | grep nginx，正常是显示 3 个结果出来 刷新 Nginx 配置后重启：/usr/local/nginx/sbin/nginx -s reload 停止 Nginx：/usr/local/nginx/sbin/nginx -s stop 如果访问不了，或是出现其他信息看下错误立即：vim /var/log/nginx/error.log 多机安装部署（CentOS 6.7 环境） http://blog.csdn.net/ricciozhang/article/details/49402273 资料 fastdfs+nginx安装配置 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/FastDFS-Nginx-Lua-GraphicsMagick.html":{"url":"Linux-Tutorial/markdown-file/FastDFS-Nginx-Lua-GraphicsMagick.html","title":"FastDFS 结合 GraphicsMagick","keywords":"","body":"FastDFS 结合 GraphicsMagick 单机安装部署（CentOS 6.7 环境） 先安装 FastDFS 软件准备： 我这边统一提供了一个压缩包，方便使用。 下载地址：http://pan.baidu.com/s/1hsg2brA 安装依赖包：yum install -y gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel libevent 安装 libfastcommon-1.0.7.tar.gz 解压：tar zxvf libfastcommon-1.0.7.tar.gz 进入解压后目录：cd libfastcommon-1.0.7/ 编译：./make.sh 安装：./make.sh install 设置几个软链接：ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so 设置几个软链接：ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so 设置几个软链接：ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so 设置几个软链接：ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 安装 tracker （跟踪器）服务 FastDFS_v5.08.tar.gz 解压：tar zxvf FastDFS_v5.05.tar.gz 进入解压后目录：cd FastDFS/ 编译：./make.sh 安装：./make.sh install 配置 tracker 服务 复制一份配置文件：cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf 编辑：vim /etc/fdfs/tracker.conf，编辑内容看下面中文注释disabled=false bind_addr= port=22122 connect_timeout=30 network_timeout=60 # 下面这个路径是保存 store data 和 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/tracker/data-and-log base_path=/opt/fastdfs/tracker/data-and-log max_connections=256 accept_threads=1 work_threads=4 store_lookup=2 store_group=group2 store_server=0 store_path=0 download_server=0 reserved_storage_space = 10% log_level=info run_by_group= run_by_user= allow_hosts=* sync_log_buff_interval = 10 check_active_interval = 120 thread_stack_size = 64KB storage_ip_changed_auto_adjust = true storage_sync_file_max_delay = 86400 storage_sync_file_max_time = 300 use_trunk_file = false slot_min_size = 256 slot_max_size = 16MB trunk_file_size = 64MB trunk_create_file_advance = false trunk_create_file_time_base = 02:00 trunk_create_file_interval = 86400 trunk_create_file_space_threshold = 20G trunk_init_check_occupying = false trunk_init_reload_from_binlog = false trunk_compress_binlog_min_interval = 0 use_storage_id = false storage_ids_filename = storage_ids.conf id_type_in_filename = ip store_slave_file_use_link = false rotate_error_log = false error_log_rotate_time=00:00 rotate_error_log_size = 0 log_file_keep_days = 0 use_connection_pool = false connection_pool_max_idle_time = 3600 http.server_port=8080 http.check_alive_interval=30 http.check_alive_type=tcp http.check_alive_uri=/status.html 启动 tracker 服务：/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf 重启 tracker 服务：/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart 查看是否有 tracker 进程：ps aux | grep tracker storage （存储节点）服务部署 一般 storage 服务我们会单独装一台机子，但是这里为了方便我们安装在同一台。 如果 storage 单独安装的话，那上面安装的步骤都要在走一遍，只是到了编辑配置文件的时候，编辑的是 storage.conf 而已 复制一份配置文件：cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf 编辑：vim /etc/fdfs/storage.conf，编辑内容看下面中文注释disabled=false group_name=group1 bind_addr= client_bind=true port=23000 connect_timeout=30 network_timeout=60 heart_beat_interval=30 stat_report_interval=60 # 下面这个路径是保存 store data 和 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/storage/data-and-log base_path=/opt/fastdfs/storage/data-and-log max_connections=256 buff_size = 256KB accept_threads=1 work_threads=4 disk_rw_separated = true disk_reader_threads = 1 disk_writer_threads = 1 sync_wait_msec=50 sync_interval=0 sync_start_time=00:00 sync_end_time=23:59 write_mark_file_freq=500 store_path_count=1 # 图片实际存放路径，如果有多个，这里可以有多行： # store_path0=/opt/fastdfs/storage/images-data0 # store_path1=/opt/fastdfs/storage/images-data1 # store_path2=/opt/fastdfs/storage/images-data2 # 创建目录：mkdir -p /opt/fastdfs/storage/images-data store_path0=/opt/fastdfs/storage/images-data subdir_count_per_path=256 # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 log_level=info run_by_group= run_by_user= allow_hosts=* file_distribute_path_mode=0 file_distribute_rotate_count=100 fsync_after_written_bytes=0 sync_log_buff_interval=10 sync_binlog_buff_interval=10 sync_stat_file_interval=300 thread_stack_size=512KB upload_priority=10 if_alias_prefix= check_file_duplicate=0 file_signature_method=hash key_namespace=FastDFS keep_alive=0 use_access_log = false rotate_access_log = false access_log_rotate_time=00:00 rotate_error_log = false error_log_rotate_time=00:00 rotate_access_log_size = 0 rotate_error_log_size = 0 log_file_keep_days = 0 file_sync_skip_invalid_record=false use_connection_pool = false connection_pool_max_idle_time = 3600 http.domain_name= http.server_port=8888 启动 storage 服务：/usr/bin/fdfs_storaged /etc/fdfs/storage.conf，首次启动会很慢，因为它在创建预设存储文件的目录 重启 storage 服务：/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart 查看是否有 storage 进程：ps aux | grep storage 测试是否部署成功 利用自带的 client 进行测试 复制一份配置文件：cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf 编辑：vim /etc/fdfs/client.conf，编辑内容看下面中文注释connect_timeout=30 network_timeout=60 # 下面这个路径是保存 store log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/client/data-and-log base_path=/opt/fastdfs/client/data-and-log # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 log_level=info use_connection_pool = false connection_pool_max_idle_time = 3600 load_fdfs_parameters_from_tracker=false use_storage_id = false storage_ids_filename = storage_ids.conf http.tracker_server_port=80 在终端中通过 shell 上传 opt 目录下的一张图片：/usr/bin/fdfs_test /etc/fdfs/client.conf upload /opt/test.jpg 如下图箭头所示，生成的图片地址为：http://192.168.1.114/group1/M00/00/00/wKgBclb0aqWAbVNrAAAjn7_h9gM813_big.jpg 即使我们现在知道图片的访问地址我们也访问不了，因为我们还没装 FastDFS 的 Nginx 模块 安装 nginx-lua-GraphicsMagick 来源：https://github.com/yanue/nginx-lua-GraphicsMagick/blob/master/nginx-install.md 添加专用用户，后面有用 groupadd www useradd -g www www -s /bin/false 安装依赖包 yum install -y gcc gcc-c++ zlib zlib-devel openssl openssl-devel pcre pcre-devel yum install -y libpng libjpeg libpng-devel libjpeg-devel ghostscript libtiff libtiff-devel freetype freetype-devel yum install -y GraphicsMagick GraphicsMagick-devel 下面的这些软件都在本文在开头的那个压缩包里面。现在我们需要解压这些压缩包 cd /opt/setups tar -zxvf nginx-1.8.0.tar.gz tar -zxvf LuaJIT-2.0.4.tar.gz tar -zxvf GraphicsMagick-1.3.21.tar.gz tar -zxvf zlib-1.2.8.tar.gz 安装 LuaJIT cd /opt/setups/LuaJIT-2.0.4 make make install export LUAJIT_LIB=/usr/local/lib export LUAJIT_INC=/usr/local/include/luajit-2.0 ln -s /usr/local/lib/libluajit-5.1.so.2 /lib64/libluajit-5.1.so.2 修改一些配置文件 编辑 Nginx 模块的配置文件：vim /opt/setups/fastdfs-nginx-module/src/config 找到下面一行包含有 local 字眼去掉，因为这三个路径根本不是在 local 目录下的。（如果你的配置文件没有这个 local，那这一步跳过）CORE_INCS=\"$CORE_INCS /usr/local/include/fastdfs /usr/local/include/fastcommon/\" 改为如下：CORE_INCS=\"$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/\" 复制文件：cp /opt/setups/FastDFS/conf/http.conf /etc/fdfs 复制文件：cp /opt/setups/FastDFS/conf/mime.types /etc/fdfs 开始安装 Nginx cd /opt/setups/nginx-1.8.0 mkdir -p /usr/local/nginx /var/log/nginx /var/temp/nginx /var/lock/nginx 执行下面编译语句：./configure --prefix=/usr/local/nginx \\ --user=www \\ --group=www \\ --pid-path=/var/local/nginx/nginx.pid \\ --lock-path=/var/lock/nginx/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --http-scgi-temp-path=/var/temp/nginx/scgi \\ --sbin-path=/usr/local/nginx/sbin/nginx \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_sub_module \\ --with-http_flv_module \\ --with-http_dav_module \\ --with-http_gzip_static_module \\ --with-http_stub_status_module \\ --with-http_addition_module \\ --with-http_spdy_module \\ --with-pcre \\ --with-zlib=/opt/setups/zlib-1.2.8 \\ --add-module=/opt/setups/nginx-http-concat \\ --add-module=/opt/setups/lua-nginx-module \\ --add-module=/opt/setups/ngx_devel_kit \\ --add-module=/opt/setups/fastdfs-nginx-module/src make make install 修改一下配置 复制 Nginx 模块的配置文件：cp /opt/setups/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs 编辑 Nginx 模块的配置文件：vim /etc/fdfs/mod_fastdfs.conf，编辑内容看下面中文注释 如果在已经启动 Nginx 的情况下修改下面内容记得要重启 Nginx。connect_timeout=2 network_timeout=30 # 下面这个路径是保存 log 的地方，需要我们改下，指向我们一个存在的目录 # 创建目录：mkdir -p /opt/fastdfs/fastdfs-nginx-module/data-and-log base_path=/opt/fastdfs/fastdfs-nginx-module/data-and-log load_fdfs_parameters_from_tracker=true storage_sync_file_max_delay = 86400 use_storage_id = false storage_ids_filename = storage_ids.conf # 指定 tracker 服务器的 IP 和端口 tracker_server=192.168.1.114:22122 storage_server_port=23000 group_name=group1 # 因为我们访问图片的地址是：http://192.168.1.114/group1/M00/00/00/wKgBclb0aqWAbVNrAAAjn7_h9gM813_big.jpg # 该地址前面是带有 /group1/M00，所以我们这里要使用 true，不然访问不到（原值是 false） url_have_group_name = true store_path_count=1 # 图片实际存放路径，如果有多个，这里可以有多行： # store_path0=/opt/fastdfs/storage/images-data0 # store_path1=/opt/fastdfs/storage/images-data1 # store_path2=/opt/fastdfs/storage/images-data2 store_path0=/opt/fastdfs/storage/images-data log_level=info log_filename= response_mode=proxy if_alias_prefix= flv_support = true flv_extension = flv group_count = 0 创建文件夹：mkdir -p /opt/fastdfs/thumb 编辑 Nginx 配置文件 vim /usr/local/nginx/conf/nginx.conf ``` nginx注意这一行行，我特别加上了使用 root 用户去执行，不然有些日记目录没有权限访问 user root; worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server{ listen 80; server_name 192.168.1.112; set $img_thumbnail_root /opt/fastdfs/thumb; set $img_file $img_thumbnail_root$uri; # like：/pic/M00/xx/xx/xx.jpg_200x100.jpg # /group1/M00 location ~* ^(\\/(\\w+)(\\/M00)(.+\\.(jpg|jpeg|gif|png))_(\\d+)+x(\\d+)+\\.(jpg|jpeg|gif|png))$ { root $img_thumbnail_root; set $fdfs_group_root /opt/fastdfs/storage/images-data/data; # 如果缩略图不存在 if (!-f $img_file) { add_header X-Powered-By 'Nginx+Lua+GraphicsMagick By Yanue'; add_header file-path $request_filename; set $request_filepath $fdfs_group_root$4; set $img_width $6; set $img_height $7; set $img_ext $5; content_by_lua_file /opt/setups/lua/cropSize.lua; } } location /group1/M00 { alias /opt/fastdfs/storage/images-data/data; ngx_fastdfs_module; } } } ``` - 启动 Nginx - 停掉防火墙：`service iptables stop` - 启动：`/usr/local/nginx/sbin/nginx`，启动完成 shell 是不会有输出的 - 访问：`192.168.1.114`，如果能看到：`Welcome to nginx!`，即可表示安装成功 - 检查 时候有 Nginx 进程：`ps aux | grep nginx`，正常是显示 3 个结果出来 - 刷新 Nginx 配置后重启：`/usr/local/nginx/sbin/nginx -s reload` - 停止 Nginx：`/usr/local/nginx/sbin/nginx -s stop` - 如果访问不了，或是出现其他信息看下错误立即：`vim /var/log/nginx/error.log` 多机安装部署（CentOS 6.7 环境） 多机部署的情况，对生成大小图的 Nginx 也有地方要修改。 资料：http://blog.csdn.net/ricciozhang/article/details/49402273 资料 fastdfs+nginx安装配置 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/RabbitMQ-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/RabbitMQ-Install-And-Settings.html","title":"RabbitMQ 安装和配置","keywords":"","body":"RabbitMQ 安装和配置 Docker 安装 RabbitMQ 官网镜像：https://hub.docker.com/_/rabbitmq/ 官网镜像说明：https://docs.docker.com/samples/library/rabbitmq 一般情况，运行： docker run -d --name cloud-rabbitmq -p 5671:5671 -p 5672:5672 -p 4369:4369 -p 25672:25672 -p 15671:15671 -p 15672:15672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=adg123456 rabbitmq:3-management 带有 websocket stomp 功能（不知道是什么就不用管它）： docker run -d --name cloud-rabbitmq -p 5671:5671 -p 5672:5672 -p 4369:4369 -p 25672:25672 -p 15671:15671 -p 15672:15672 -p 61613:61613 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=adg123456 rabbitmq:3-management 进入 Docker 容器启动 stomp 插件： docker exec -it cloud-rabbitmq /bin/bash cd /plugins rabbitmq-plugins enable rabbitmq_web_stomp 参数解释： rabbitmq:3-management：只有带 management 后缀的才有 web 端管理入口 15672：表示 RabbitMQ 控制台端口号，可以在浏览器中通过控制台来执行 RabbitMQ 的相关操作。容器启动成功后，可以在浏览器输入地址：http://ip:15672/ 访问控制台 5672: 表示 RabbitMQ 所监听的 TCP 端口号，应用程序可通过该端口与 RabbitMQ 建立 TCP 连接，完成后续的异步消息通信 RABBITMQ_DEFAULT_USER：用于设置登陆控制台的用户名，这里我设置 admin RABBITMQ_DEFAULT_PASS：用于设置登陆控制台的密码，这里我设置 admin RabbitMQ 说明 MQ 全称为 Message Queue, 消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写出入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。 RabbitMQ 是一个在 AMQP 基础上完整的，可复用的企业消息系统。他遵循 Mozilla Public License 开源协议。 RabbitMQ WIKI：https://zh.wikipedia.org/zh/RabbitMQ RabbitMQ 百科：http://baike.baidu.com/view/4095865.htm RabbitMQ 官网：http://www.rabbitmq.com/ RabbitMQ 官网下载：http://www.rabbitmq.com/download.html RabbitMQ 官网安装文档：http://www.rabbitmq.com/install-rpm.html RabbitMQ 文档： 优先：http://www.rabbitmq.com/getstarted.html 次要：http://www.rabbitmq.com/documentation.html 基于 epel 源进行安装（优先） 来源：http://blog.csdn.net/wh211212/article/details/53893085 rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm # 安装依赖环境erlang yum install erlang -y # 安装rabbitmq yum install rabbitmq-server -y 本地安装 本地安装 Erlang 有 EPEL 源的情况（需要安装的内容较多，宽带要能跟上）：sudo yum install erlang RabbitMQ 官网提供 Erlang 安装包： 下载地址：http://www.rabbitmq.com/releases/erlang/ 下载好之后，安装下面两个文件： sudo yum localinstall -y esl-erlang_18.1-1~centos~6_amd64.rpm sudo yum localinstall -y esl-erlang-compat-18.1-1.noarch.rpm rpm 安装 RabbitMQ 此时（2016-04），最新版：3.6.1 安装：rpm --import https://www.rabbitmq.com/rabbitmq-signing-key-public.asc 安装：sudo yum install -y rabbitmq-server-3.6.1-1.noarch.rpm 启动服务： 先看下自己的主机名：hostname，我的主机名是：judasnHost2 先修改一下 host 文件：vim /etc/hosts，添加一行：127.0.0.1 judasnHost2（必须这样做） 启动：service rabbitmq-server start，启动一般都比较慢，所以别急 停止：service rabbitmq-server stop 重启：service rabbitmq-server restart 设置开机启动：chkconfig rabbitmq-server on 配置 查找默认配置位置：find / -name \"rabbitmq.config.example\"，我这边搜索结果是：/usr/share/doc/rabbitmq-server-3.6.1/rabbitmq.config.example 复制默认配置：cp /usr/share/doc/rabbitmq-server-3.6.1/rabbitmq.config.example /etc/rabbitmq/ 修改配置文件名：cd /etc/rabbitmq ; mv rabbitmq.config.example rabbitmq.config 编辑配置文件，开启用户远程访问：vim rabbitmq.config 在 64 行，默认有这样一句话：%% {loopback_users, []},，注意，该语句最后有一个逗号，等下是要去掉的 我们需要改为：{loopback_users, []}， 开启 Web 界面管理：rabbitmq-plugins enable rabbitmq_management 如果你是用 epel 安装的话，则是这样运行：cd /usr/lib/rabbitmq/bin;./rabbitmq-plugins enable rabbitmq_management 重启 RabbitMQ 服务：service rabbitmq-server restart 开放防火墙端口： sudo iptables -I INPUT -p tcp -m tcp --dport 15672 -j ACCEPT sudo iptables -I INPUT -p tcp -m tcp --dport 5672 -j ACCEPT sudo service iptables save sudo service iptables restart 浏览器访问：http://192.168.1.114:15672 默认管理员账号：guest 默认管理员密码：guest 添加新授权用户（如下图所示）： 添加 Host（如下图所示）： 给添加的 Host 设置权限（如下图所示）： 交换机绑定队列（如下图所示）： 集群环境（镜像队列） TODO 消息重复 消息重复无法避免，比如消费端异常重启就有可能，或者 MQ 应用挂了重启之后等场景，任何 MQ 应用没有保证消息不会重复发送。 对于一定要保证幂等性的业务场景，在消费端做好标识。比如在 Redis 或 JVM 缓存中存有上一次消费的记录，业务操作之前下判断。 如果是插入操作类的，也可以考虑用唯一约束的方式来保证插入不会重复等。 消息丢失 单节点，纯内存情况下一般有三种情况： 生产者提交消息到 MQ，但是网络抖动了，丢了。或是 MQ 拿到之后突然挂了，来不及登记 MQ 拿到消息，消费者还没消费，但是 MQ 挂了 消费者拿到消息来不及处理，自己挂了，MQ 认为已经消费成功了。 分别解决办法： 把 channel 设置为 confirm 模式 持久化队列：创建 queue 的时候持久化 durable=true。持久化消息：生产者发送消息时候：deliveryMode = 2 手动 ACK 消息顺序 一般场景不需要消息顺序，要去做一般也开销很大，需要执行考虑。 在能保证消息顺序的情况下，可以用来做数据同步 解决： 消费者只有单个应用，并且内容不要使用异步或者多线程。在这种场景下绑定 queue，基于消息队列本质是队列，消息是 FIFO（先进先出）的，这样消息就能按顺序。但是缺点很明显：吞吐太差，效率太低，适合低效率的业务。 基于上面方案并且对队列进行分片。假设我们原来 repay.queue下面有 10 个消费者线程，那么我们可以创建10个队列，每个队列下面只允许有一个消费者。一个比较简单的方式是，队列命名为 repay.queue-0，repay.queue-2…repay.queue-9，然后生产者推送信息的时候，基于用户的ID（Long类型）mod 10 取模 0…9（取余），再选择发送到相应的队列即可，这样就等保证同一个用户的顺序。 消息积压 如果消费者挂掉，消息会一直积压在 MQ 中 解决办法 如果原来有 3 个消费者应用，现在需要准备多倍的消费者应用，假设现在有 10 个新的消费者应用。 创建一个临时的 topic，假设叫做 topic_abc 新的 10 个消费者应用绑定在新的 topic_abc 上 修改原来 3 个消费者应用代码，改为接受到 MQ 消息后不操作入数据库了，而是直接发给 topic_abc 这样原来积压的消息就有 10 个消费者一起来分摊 如果此时这个 MQ 扛不住压力，那只能让旧的 3 个消费者应用写到其他 MQ，然后 10 个新消费者消费新的 MQ。 然后深夜找个时间重新欢迎旧的 3 个消费者代码。 资料 http://throwable.coding.me/2018/03/25/out-of-order-consume/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Openfire-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Openfire-Install-And-Settings.html","title":"Openfire 安装和配置","keywords":"","body":"Openfire 安装和配置 本机环境 系统：CentOS 6.7 64 位 JDK 1.7 64 位 MySQL 5.6 Openfire 说明 官网：http://www.igniterealtime.org/projects/openfire/ 官网下载：http://www.igniterealtime.org/downloads/index.jsp 官网插件列表：http://www.igniterealtime.org/projects/openfire/plugins.jsp 官网文档：http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/ 官网安装手册：http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/install-guide.html 官网安装手册-中文翻译版本：http://wiki.jabbercn.org/Openfire:%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97 官网数据库部署手册：http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/database.html javadoc 文档：http://www.igniterealtime.org/builds/openfire/docs/latest/documentation/javadoc/ 连接管理工具（上千用户的时候用）：http://www.igniterealtime.org/projects/openfire/connection_manager.jsp > 下载 官网下载：http://www.igniterealtime.org/downloads/index.jsp 当前最新版本：4.0.2，下载文件：openfire-4.0.2-1.i386.rpm 安装 MySQL、JDK MySQL 安装和配置 JDK 安装 安装 Openfire 要求：JDK 1.7 或以上，我这里使用 1.7 CentOS 系列（Red Hat、Fedora）官网推荐安装 RPM 文件，因为有一些相关环境他们帮我们考虑了 安装命令：rpm -ivh openfire-4.0.2-1.i386.rpm Openfire 默认给我们生成安装目录：/opt/openfire 修改 JDK VM 参数：vim /etc/sysconfig/openfire，找到 23 行，打开 OPENFIRE_OPTS 删除这一行注释，分配多少 VM 你根据自己的机子来配置。 初始化数据库： 请确保数据库是运行状态 默认的初始化数据库脚本在（其他数据库类型的脚本也在这个目录下）：/opt/openfire/resources/database/openfire_mysql.sql 进入 MySQL 命令行状态：mysql -u root -p 创建数据库并授权：create databaseopenfirecharacter set utf8;grant all privileges on openfire.* to 'root'@'%';flush privileges; 退出 MySQL 命令行模式，在终端命令状态下，执行：sudo mysql -u root -p openfire 先停掉防火墙：service iptables stop 启动：/etc/init.d/openfire start 查看进程：ps aux | grep openfire 停止：/etc/init.d/openfire stop 重启：/etc/init.d/openfire restart 配置 访问 Web 管理界面：http://192.168.1.113:9090 开始向导配置 Openfire 具体步骤看下列一系列图： 需要注意的是：在配置数据库 URL 需要特别注意的是需要加入编码设置（你可以按我的这个链接来，但是 IP 和数据库名你自己修改）： jdbc:mysql://192.168.1.113:3306/openfire?rewriteBatchedStatements=true&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8 如果连接不了数据库，可以看错误日志：cat /opt/openfire/logs/error.log 使用 Spark 客户端 Spark 是一个类似 QQ 的 IM 软件，只是功能比较简单。 官网下载：http://www.igniterealtime.org/downloads/index.jsp 官网 Windows 下有两个版本： spark_2_7_7.exe Offline installation, includes Java JRE (推荐安装这个，即使你有本机已经有了 JDK 环境) spark_2_7_7_online.exe Online installation, does not include Java JRE 安装完 Spark，启动软件，更玩 QQ 一样道理，具体看下图。你可以用 Spark 登录 Admin 账号。 现在访问 Web 管理界面，给所有在线的会员发个消息，如果你的 Spark 可以收到消息，那就说明整个环境是通的。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Rap-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Rap-Install-And-Settings.html","title":"Rap 安装和配置","keywords":"","body":"Rap 安装和配置 本机环境 系统：CentOS 6.7 64 位 MySQL 5.6 JDK 1.8 Tomcat 8 Redis 3.0.7 Rap 0.14.1 Rap 说明 官网：https://github.com/thx/RAP 在线版：http://rap.taobao.org/ 官网 Wiki：https://github.com/thx/RAP/wiki/home_cn 官网部署手册：https://github.com/thx/RAP/wiki/deploy_manual_cn 用户手册：https://github.com/thx/RAP/wiki/user_manual_cn 下载 官网下载：https://github.com/thx/RAP/releases 当前最新版本：0.14.1 下载 war 部署包：https://github.com/thx/RAP/releases 安装 MySQL、JDK、Tomcat、Redis MySQL 安装和配置 JDK 安装 Tomcat 安装和配置、优化 Redis 安装和配置 安装 Rap 创建数据库，并创建权限用户 CREATE DATABASE `rap_db` CHARACTER SET utf8; CREATE USER 'rap'@'%' IDENTIFIED BY '123456'; GRANT ALL PRIVILEGES ON rap_db.* TO 'rap'@'%'; FLUSH PRIVILEGES; 把 RAP-0.14.1-SNAPSHOT.war 移动到 tomcat 的 webapp 目录下，删除其他多余的文件夹 解压：unzip -x RAP-0.14.1-SNAPSHOT.war -d ROOT 初始化数据库：mysql -u rap -p rap_db 修改连接数据库的配置信息：vim /usr/program/tomcat8/webapps/ROOT/WEB-INF/classes/config.properties 停掉防火墙：service iptables stop 启动 Redis：/usr/local/bin/redis-server /etc/redis.conf 启动 Tomcat：sh /usr/program/tomcat8/bin/startup.sh ; tail -200f /usr/program/tomcat8/logs/catalina.out Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Nginx-Keepalived-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Nginx-Keepalived-Install-And-Settings.html","title":"Nginx + Keepalived 高可用","keywords":"","body":"Nginx + Keepalived 高可用 说明 高可用 HA（High Availability），简单讲就是：我某个应用挂了，自动有另外应用起来接着扛着，致使整个服务对外来看是没有中断过的。这里的重点就是不中断，致使公司整个业务能不断进行中，把影响减到最小，赚得更多。 因为要不中断，所以我们就需要用到了 Keepalived。Keepalived 一般不会单独使用，基本都是跟负载均衡软件（LVS、HAProxy、Nginx）一起工作来达到集群的高可用效果。 Keepalived 有双主、主备方案 常用词： 心跳：Master 会主动给 Backup 发送心跳检测包以及对外的网络功能，而 Backup 负责接收 Master 的心跳检测包，随时准备接管主机。为什么叫心跳不知道，但是挺形象的，心跳同步。 选举：Keepalived 配置的时候可以指定各台主机优先级，Master 挂了，各台 Backup 要选举出一个新的 Master。 Keepalived 官网：http://www.keepalived.org/ 官网下载：http://www.keepalived.org/download.html 官网文档：http://www.keepalived.org/documentation.html 搭建 软件版本： Nginx：1.8.1 Keepalived：1.2.20 JDK：8u72 Tomcat：8.0.32 部署环境（下文中以第几台来代表这些主机）： 虚拟 IP（VIP）：192.168.1.50 第一台主机：Nginx 1 + Keepalived 1 == 192.168.1.120（Master） 第二台主机：Nginx 2 + Keepalived 2 == 192.168.1.121（Backup） 第三台主机：Tomcat 1 == 192.168.1.122（Web 1） 第四台主机：Tomcat 2 == 192.168.1.123（Web 2） 所有机子进行时间校准：NTP（Network Time Protocol）介绍 第三、第四台主机部署： JDK 的安装：JDK 安装 Tomcat 的安装：Tomcat 安装和配置、优化 第一、二台主机部署（两台部署内容一样）： Nginx 的安装：Nginx 安装和配置 添加虚拟 IP： 复制一个网卡信息：sudo cp /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network-scripts/ifcfg-eth0:0 编辑配置文件：sudo vim /etc/sysconfig/network-scripts/ifcfg-eth0:0 修改内容为如下信息：DEVICE=eth0:0 >>> 这个需要修改 TYPE=Ethernet UUID=8ddbb256-caab-4ddf-8e9a-6527b4ac5a26 ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=none IPADDR=192.168.1.50 >>> 这个需要修改 PREFIX=24 GATEWAY=192.168.1.1 DNS1=101.226.4.6 DEFROUTE=yes IPV4_FAILURE_FATAL=yes IPV6INIT=no NAME=\"System eth0:0\" >>> 这个需要修改 HWADDR=00:0c:29:f4:17:db LAST_CONNECT=1460213205 重启网卡服务：service network restart 如果你要绑定更多虚拟 IP，则多复制几个网卡配置出来，命名如下：ifcfg-eth0:0，ifcfg-eth0:1，ifcfg-eth0:2 ...... Keepalived 开始安装 安装依赖：sudo yum install -y gcc openssl-devel popt-devel 解压包：cd /opt/setups/ ; tar zxvf keepalived-1.2.20.tar.gz 编译：cd /opt/setups/keepalived-1.2.20 ; ./configure --prefix=/usr/program/keepalived 编译安装：make && make install Keepalived 设置服务和随机启动 复制配置文件到启动脚本目录：cp /usr/program/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived 增加权限：chmod +x /etc/init.d/keepalived 编辑配置文件：vim /etc/init.d/keepalived把 15 行的：. /etc/sysconfig/keepalived，改为： . /usr/program/keepalived/etc/sysconfig/keepalived（注意：前面有一个点和空格需要注意） 添加环境变量：vim /etc/profile# Keepalived 配置 KEEPALIVED_HOME=/usr/program/keepalived PATH=$PATH:$KEEPALIVED_HOME/sbin export KEEPALIVED_HOME export PATH 刷新环境变量：source /etc/profile 检测环境变量：keepalived -v ln -s /usr/program/keepalived/sbin/keepalived /usr/sbin/ vim /usr/program/keepalived/etc/sysconfig/keepalived把 14 行的：KEEPALIVED_OPTIONS=\"-D\"，改为： KEEPALIVED_OPTIONS=\"-D -f /usr/program/keepalived/etc/keepalived/keepalived.conf\" 加入随机启动：chkconfig keepalived on 第一、二台主机配置（两台在 Keepalived 配置上稍微有不一样）： 健康监测脚本（我个人放在：/opt/bash 目录下）：nginx_check.sh 健康监测脚本添加执行权限：chmod 755 /opt/bash/nginx_check.sh 运行监测脚本，看下是否有问题：sh /opt/bash/nginx_check.sh，如果没有报错，则表示改脚本没有问题 这个脚本很重要，如果脚本没法用，在启用 Keepalived 的时候可能会报：Keepalived_vrrp[5684]: pid 5959 exited with status 1 nginx 配置（两台一样配置）： worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # （重点） upstream tomcatCluster { server 192.168.1.122:8080 weight=1; server 192.168.1.123:8080 weight=1; } # （重点） server { listen 80; server_name 192.168.1.50; location / { proxy_pass http://tomcatCluster; index index.html index.htm; } } } Keepalived 配置文件编辑（第一、二台配置稍微不同，不同点具体看下面重点说明） 编辑：vim /usr/program/keepalived/etc/keepalived/keepalived.conf ``` nginx ! Configuration File for keepalived 全局配置 global_defs { 邮箱通知配置，keepalived 在发生切换时需要发送 email 到的对象，一行一个 notification_email { #acassen@firewall.loc #failover@firewall.loc #sysadmin@firewall.loc } 指定发件人 notification_email_from Alexandre.Cassen@firewall.loc 指定smtp服务器地址 smtp_server 192.168.200.1 指定smtp连接超时时间，单位秒 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict } （重点）脚本监控实现 vrrp_script check_nginx { 运行脚本 script \"/opt/bash/nginx_check.sh\" 时间间隔，2秒 interval 2 权重 weight 2 } vrrp_instance VI_1 { # （重点）Backup 机子这里是设置为：BACKUP state MASTER interface eth0 virtual_router_id 51 # （重点）Backup 机子要小于当前 Master 设置的 100，建议设置为 99 priority 100 # Master 与 Backup 负载均衡器之间同步检查的时间间隔，单位是秒 advert_int 1 authentication { auth_type PASS auth_pass 1111 } # （重点）配置虚拟 IP 地址，如果有多个则一行一个 virtual_ipaddress { 192.168.1.50 } # （重点）脚本监控调用 track_script { check_nginx } } ``` 启动各自服务 四台机子都停掉防火墙：service iptables stop 先启动两台 Tomcat：sh /usr/program/tomcat8/bin/startup.sh ; tail -200f /usr/program/tomcat8/logs/catalina.out 检查两台 Tomcat 是否可以单独访问，最好给首页加上不同标识，好方便等下确认是否有负载 http://192.168.1.122:8080 http://192.168.1.123:8080 启动两台 Nginx 服务：/usr/local/nginx/sbin/nginx 启动两台 Keepalived 服务：service keepalived start 查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 高可用测试 模拟 Keepalived 挂掉 关闭 Master 主机的 Keepalived，查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 关闭服务：service keepalived stop 如果第二台机接管了，则表示成功 重新开启 Master 主机的 Keepalived，查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 重启服务：service keepalived restart 如果第一台机重新接管了，则表示成功 模拟 Nginx 挂掉 关闭 Master 主机的 Nginx，查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 关闭服务：/usr/local/nginx/sbin/nginx -s stop 如果第二台机接管了，则表示成功 重新开启 Master 主机的 Nginx，查看 Master 和 Backup 两台主机的对应日志：tail -f /var/log/messages 重启 Nginx 服务：/usr/local/nginx/sbin/nginx -s reload 重启 Keepalived 服务：service keepalived restart 如果第一台机重新接管了，则表示成功 可以优化的地方，改为双主热备，监控脚本上带有自启动相关细节，后续再进行。 日志中常用的几句话解释： Entering to MASTER STATE，变成 Master 状态 Netlink reflector reports IP 192.168.1.50 added，一般变为 Master 状态，都要重新加入虚拟 IP，一般叫法叫做：虚拟 IP 重新漂移到 Master 机子上 Entering BACKUP STATE，变成 Backup 状态 Netlink reflector reports IP 192.168.1.50 removed，一般变为 Backup 状态，都要移出虚拟 IP，一般叫法叫做：虚拟 IP 重新漂移到 Master 机子上 VRRP_Script(check_nginx) succeeded，监控脚本执行成功 资料 http://xutaibao.blog.51cto.com/7482722/1669123 https://m.oschina.net/blog/301710 http://blog.csdn.net/u010028869/article/details/50612571 http://blog.csdn.net/wanglei_storage/article/details/51175418 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Was-Hacked.html":{"url":"Linux-Tutorial/markdown-file/Was-Hacked.html","title":"黑客入侵检查","keywords":"","body":"黑客入侵检查 思路 扫描木马工具：clamAV 官网：http://pkgs.repoforge.org/clamav/ CentOS 安装：yum install -y clamav* 启动 clamAV 服务：service clamd restart 更新病毒库：freshclam 扫描方法： 扫描 /etc 目录，并把扫描结果放在 /root 目录下：clamscan -r /etc --max-dir-recursion=5 -l /root/etcclamav.log 扫描 /bin 目录，并把扫描结果放在 /root 目录下：clamscan -r /bin --max-dir-recursion=5 -l /root/binclamav.log 扫描 /usr 目录，并把扫描结果放在 /root 目录下：clamscan -r /usr --max-dir-recursion=5 -l /root/usrclamav.log 如果日志有类似内容，表示有木马病毒： /usr/bin/.sshd: Linux.Trojan.Agent FOUND /usr/sbin/ss: Linux.Trojan.Agent FOUND /usr/sbin/lsof: Linux.Trojan.Agent FOUND 看下当前有多少登录者：who 看下最近有哪些登录者：last 查看最近尝试登录的账号信息：grep \"sshd\" /var/log/secure 很多这种信息就表示有人在不断地尝试用 root 登录：Failed password for root from 222.186.56.168 port 4080 ssh2 查看最近登录成功的账号信息：grep \"Accepted\" /var/log/secure，可以看到：pop3, ssh, telnet, ftp 类型 看下查看系统资源占用有无异常：top 看下所有进程：ps aux 查看当前系统登录者有哪些，及其登录记录：last | more 把最近执行的所有命令输出到一个文件，然后下载下来细细研究：history >> /opt/test.txt 查看当前系统所有用户有哪些：cat /etc/passwd |awk -F \\: '{print $1}' 更多详细可以用：cat /etc/passwd 查看开放的端口，比如常用的80,22,8009，后面的箭头表示端口对应占用的程序：netstat -lnp 检查某个端口的具体信息：lsof -i :18954 检查启动项：chkconfig 检查定时器（重要）：cat /etc/crontab 检查定时器（重要）：crontab -l vim /var/spool/cron/crontabs/root vim /var/spool/cron/root 检查其他系统重要文件： cat /etc/rc.local cd /etc/init.d;ll 检查文件： find / -uid 0 –perm -4000 –print find / -size +10000k –print find / -name \"…\" –print find / -name \".. \" –print find / -name \". \" –print find / -name \" \" –print 下载 iftop 分析流量，查看是否被黑客当做肉鸡使用 安装 iftop 官网：http://www.ex-parrot.com/~pdw/iftop/ 使用文章：https://linux.cn/article-1843-1.html 没有安装第三方源的情况： 安装依赖包：yum install -y flex byacc libpcap ncurses ncurses-devel libpcap-devel 下载源码包：wget http://www.ex-parrot.com/pdw/iftop/download/iftop-0.17.tar.gz 解压：tar zxf iftop-0.17.tar.gz 进入解压目录：cd iftop-0.17/ 编译：./configure 安装：make && make install 有第三方源的情况（eg：EPEL）： yum install -y iftop 运行：iftop 显示端口与 IP 信息：iftop -nP 中间部分：外部连接列表，即记录了哪些ip正在和本机的网络连接 右边部分：实时参数分别是该访问 ip 连接到本机 2 秒，10 秒和 40 秒的平均流量 => 代表发送数据， 禁用 root 账号登录：vim /etc/ssh/sshd_config 把 PermitRootLogin 属性 yes 改为 no 如果安全度要更高，可以考虑禁用口令登录，采用私钥/公钥方式：vim /etc/ssh/sshd_config 设置属性：PasswordAuthentication 为 no 如果还要限制指定 IP 登录，可以考虑编辑：hosts.allow 和 hosts.deny 两个文件 重要系统软件更新（更新之前最好先做好系统镜像或是快照，以防万一）： yum update kernel yum update kernel-devel yum update kernel-firmware yum update kernel-headers yum update openssh yum update openssh-clients yum update openssh-server 实战 挖矿程序 先查看调度任务是否有新增内容 vim /var/spool/cron/root vim /var/spool/cron/crontabs/root 如果有，先停止定时任务：systemctl stop crond 如果对方有去 wget curl 指定网站，则先在 hosts 里面映射为 127.0.0.1，比如：127.0.0.1 prax0zma.ru 查看当前最占用 CPU 的进程 PID，加入发现是 22935，则：cd /proc/22935 && ll，发现程序目录是：/root/.tmp00/bash64 我们就把该程序去掉执行任务的权限：chmod -R -x /root/.tmp00/，然后再 kill 掉该程序 打开别人的脚本，看下是如何书写的，发现有写入几个目录，这里进行删除： rm -rf /tmp/.ha /boot/.b /boot/.0 /root/.tmp00 最后检查下是否有免密内容被修改：cd ~/.ssh/ && cat authorized_keys 资料 http://www.jianshu.com/p/97b9dc47b88c http://monklof.com/post/10/ http://yafeilee.me/blogs/54be6e876c69341430050000 http://coolnull.com/4174.html http://www.oicqzone.com/pc/2014110420118.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Mycat-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Mycat-Install-And-Settings.html","title":"Mycat 安装和配置","keywords":"","body":"Mycat 安装和配置 部署的环境 系统：CentOS 6.7 / CentOS 7.4 JDK：jdk-8u72-linux-x64.tar.gz Mycat：Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz 推荐测试机子配置：1 CPU + 2 GB RAM + Docker Docker 快速部署 MySQL docker run --name mysql1 -p 3316:3306 -e MYSQL_ROOT_PASSWORD=root -d daocloud.io/library/mysql:5.7.13 docker run --name mysql2 -p 3326:3306 -e MYSQL_ROOT_PASSWORD=root -d daocloud.io/library/mysql:5.7.13 docker run --name mysql3 -p 3336:3306 -e MYSQL_ROOT_PASSWORD=root -d daocloud.io/library/mysql:5.7.13 Mycat 安装 官网（页头有一个 PDF 要记得下载，这本资料写得很好）：http://mycat.io/ 官网下载（官网下载地址很乱，如果哪天右边这个地址不行了，到官网加群问下吧）：http://dl.mycat.io/ 项目 Github：https://github.com/MyCATApache/Mycat-Server 此时（20170122） 最新稳定版本为：1.6，下载下来的文件名称：Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz 安装前的准备： 这台机子必须装有 JDK，并且配置好 JAVA_HOME。JDK 的安装看：https://github.com/judasn/Linux-Tutorial/blob/master/JDK-Install.md 开始安装： 假设 Mycat 安装包的完整路径是：/opt/Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz 解压：cd /opt ; tar -zxvf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz 移动解压的目录到官方建议的目录下：mv /opt/mycat /usr/local/ 设置 Mycat 的环境变量 vim /etc/profile，添加如下内容： export MYCAT_HOME=/usr/local/mycat export PATH=$PATH:$MYCAT_HOME/bin 刷新配置：source /etc/profile 到这里就安装好了，但是先不启动，需要先去配置相应的配置文件。 Mycat 只分库配置，并且要可预期租户数量（没有分表、读写分离、高可用） 请至少预留 512M 内存给 Mycat 使用 Mycat 这几个配置文件必定会改动到。这一个文件所代表的含义几句话说不了，还请你自己看下官网的文档，在目录：conf 下 server.xml 配置详解 server.xml，主要用于配置系统变量、用户管理、用户权限等。 默认配置中有一个 TESTDB 相关的数据库用户配置，都要自己手工去掉。建议可以把配置文件中 TESTDB 相关的单词都替换成你的数据库名称（你最好能读懂各个配置）。 配置用户，添加下面内容： 123456 adg_system false 0 0 --> mycat 中间件的全局配置： 0 0 2 1--> 5.6.20--> 40960--> 1--> 32--> 0 65535--> 0--> 1--> 1--> 16--> 8066 9066 300000 0.0.0.0 4096 32 --> 0 1 1m 1k 0 384m false schema.xml 配置详解 schema.xml，用于设置 Mycat 的逻辑库、表、数据节点、dataHost 等内容，分库分表、读写分离等等都是在这里进行配置的 schema.xml 中特别注意的是分片节点的配置。如下，其中 adg_system_0000,adg_system_0001,adg_system_0002 是需要我们自己在 mysql 对应的机子上人工创建这三个空白数据库和表。 select user() select user() select user() 如果节点数据很多的情况，我们有一种简便写法 select user() rule.xml 配置详解 rule.xml，设置分片规则。 下面我设置一个根据店铺进行分片的规格： id func1 user_id func1 sharding_id hash-int id rang-long id mod-long id murmur id crc32slot create_time partbymonth calldate latestMonth id rang-mod id jump-consistent-hash shop_id by-shop-id sharding-by-shop-id.txt 1 0 0 2 160 weightMapFile 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 --> /etc/mycat/bucketMapPath 用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 --> 2 partition-hash-int.txt autopartition-long.txt 3 8 128 24 yyyy-MM-dd 2015-01-01 partition-range-mod.txt 3 还需要在 conf 新增文件 sharding-by-shop-id.txt 文件，内容是： 需要注意的是： 417454619141211000=0 417454619141211001=1 417454619141211002=2 表示 shop_id 为 417454619141211000 的时候，用 adg_system_0000 库 表示 shop_id 为 417454619141211001 的时候，用 adg_system_0001 库 表示 shop_id 为 417454619141211002 的时候，用 adg_system_0002 库 其中第一个库是从下表 0 开始的。 log4j2.xml 配置详解 log4j2.xml，用于设置 Mycat 的日志输出规则，默认日志文件是输出 mycat 根目录下的 logs 目录下。 只分库的 demo 脚本 在 MySQL 实例 1 中执行如下初始化脚本： CREATE DATABASE /*!32312 IF NOT EXISTS*/`adg_system_0000` /*!40100 DEFAULT CHARACTER SET utf8mb4 */; USE `adg_system_0000`; 在 MySQL 实例 2 中执行如下初始化脚本： CREATE DATABASE /*!32312 IF NOT EXISTS*/`adg_system_0001` /*!40100 DEFAULT CHARACTER SET utf8mb4 */; USE `adg_system_0001`; 在 MySQL 实例 3 中执行如下初始化脚本： CREATE DATABASE /*!32312 IF NOT EXISTS*/`adg_system_0002` /*!40100 DEFAULT CHARACTER SET utf8mb4 */; USE `adg_system_0002`; 其他设置 假设你上面的配置文件都配置好了： 开放 8066 端口 如果只是临时测试，可以临时关掉防火墙：service iptables stop 不然就添加防火墙规则： 添加规则：sudo iptables -I INPUT -p tcp -m tcp --dport 8066 -j ACCEPT 保存规则：sudo service iptables save 重启 iptables：sudo service iptables restart 启动/停止/重启 注意：在启动之前请一定要确保你可以用 mysql 工具连上你们的三个 mysql 和对应的数据库。有时候你连得上 mysql，但是你的账号不一定能连上这个 mysql 对应的数据库。 启动有两种，一种是后台启动，启动后看不到任何信息。一种是控制台启动，启动后进入 Mycat 的控制台界面，显示当前 Mycat 的活动信息，按 Ctrl + C 停止控制台的时候 Mycat 也跟着停止。 进入 Mycat 目录：cd /usr/local/mycat/bin 后台启动：./mycat start && tail -300f /usr/local/mycat/logs/mycat.log 控制台启动：./mycat console 重启：./mycat restart 停止：./mycat stop 连接 Mycat 连接 Mycat 的过程跟连接普通的 MySQL 表面上是没啥区别的，使用的命令都是一个样。但是需要注意的是，很容易出问题。对连接客户端有各种意外，目前我做了总结： 连接命令：mysql -h192.168.1.112 -uroot -p -P8066，然后输入 mycat 的 root 用户密码（在上面介绍的 server.xml 中配置的） 不建议 的连接方式： SQLyog 软件，我这边是报：find no Route:select from youmeek_nav.nav_url limit 0, 1000* Windows 系统下使用 cmd 去连接，我这边是报：ERROR 1105 (HY000): Unknown character set: 'gbk' MySQL-Front 软件，没用过，但是别人说是有兼容性问题 建议 的连接方式： Navicat for mysql 软件 Linux 下的 MySQL 客户端命令行 使用 Navicat 连接 MyCat 测试 SQL 创建表 SQL CREATE TABLE `adg_ads` ( `ads_id` BIGINT(20) NOT NULL COMMENT '广告表ID', `ads_set_id` BIGINT(20) NOT NULL COMMENT '广告组表ID', `ads_title` VARCHAR(32) NOT NULL COMMENT '广告标题', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告表'; CREATE TABLE `adg_ads_set` ( `ads_set_id` BIGINT(20) NOT NULL COMMENT '广告组表ID', `ads_set_title` VARCHAR(32) NOT NULL COMMENT '广告组标题', `ads_campaign_id` BIGINT(20) NOT NULL COMMENT '广告系列表ID', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_set_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告组表'; CREATE TABLE `adg_ads_campaign` ( `ads_campaign_id` BIGINT(20) NOT NULL COMMENT '广告系列表ID', `ads_campaign_title` VARCHAR(32) NOT NULL COMMENT '广告系列标题', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_campaign_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告系列表'; CREATE TABLE `adg_channel` ( `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`channel_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='渠道表'; CREATE TABLE `adg_shop` ( `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', PRIMARY KEY (`shop_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='商品表'; CREATE TABLE `adg_shop_channel` ( `shop_channel_id` BIGINT(20) NOT NULL COMMENT '店铺渠道中间表ID', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`shop_channel_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='店铺渠道中间表'; 创建数据 SQL INSERT INTO `adg_shop`(`shop_id`,`shop_name`) VALUES (417454619141211000,'NC站'); INSERT INTO `adg_shop`(`shop_id`,`shop_name`) VALUES (417454619141211001,'BG站'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (1,'Facebook'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (2,'Google'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (3,'Twitter'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,417454619141211000,2,'NC站','Google'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (3,417454619141211001,1,'BG站','Facebook'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (4,417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads_campaign`(`ads_campaign_id`,`ads_campaign_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,'第1个广告系列',417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads_campaign`(`ads_campaign_id`,`ads_campaign_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,'第2个广告系列',417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads_set`(`ads_set_id`,`ads_set_title`,`ads_campaign_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,'第1个广告集',1,417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads_set`(`ads_set_id`,`ads_set_title`,`ads_campaign_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,'第2个广告集',2,417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads`(`ads_id`,`ads_set_id`,`ads_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,1,'第1个广告',417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads`(`ads_id`,`ads_set_id`,`ads_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,2,'第2个广告',417454619141211001,2,'BG站','Google'); mycat 正常启动的 log 内容 tail -300f wrapper.log 2018-02-05 14:15:41.432 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.(PhysicalDBPool.java:100)) - total resouces of dataHost mysql_host_0 is :1 2018-02-05 14:15:41.435 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.(PhysicalDBPool.java:100)) - total resouces of dataHost mysql_host_2 is :1 2018-02-05 14:15:41.435 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.(PhysicalDBPool.java:100)) - total resouces of dataHost mysql_host_1 is :1 2018-02-05 14:15:41.442 INFO [WrapperSimpleAppMain] (io.mycat.cache.CacheService.createLayeredPool(CacheService.java:125)) - create layer cache pool TableID2DataNodeCache of type encache ,default cache size 10000 ,default expire seconds18000 2018-02-05 14:15:41.445 INFO [WrapperSimpleAppMain] (io.mycat.cache.DefaultLayedCachePool.createChildCache(DefaultLayedCachePool.java:80)) - create child Cache: TESTDB_ORDERS for layered cache TableID2DataNodeCache, size 50000, expire seconds 18000 2018-02-05 14:15:41.597 INFO [WrapperSimpleAppMain] (io.mycat.config.classloader.DynaClassLoader.(DynaClassLoader.java:34)) - dyna class load from ./catlet,and auto check for class file modified every 60 seconds 2018-02-05 14:15:41.601 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:266)) - =============================================== 2018-02-05 14:15:41.602 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:267)) - MyCat is ready to startup ... 2018-02-05 14:15:41.602 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:279)) - Startup processors ...,total processors:1,aio thread pool size:4 each process allocated socket buffer pool bytes ,a page size:2097152 a page's chunk number(PageSize/ChunkSize) is:512 buffer page's number is:20 2018-02-05 14:15:41.602 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:280)) - sysconfig params:SystemConfig [processorBufferLocalPercent=100, frontSocketSoRcvbuf=1048576, frontSocketSoSndbuf=4194304, backSocketSoRcvbuf=4194304, backSocketSoSndbuf=1048576, frontSocketNoDelay=1, backSocketNoDelay=1, maxStringLiteralLength=65535, frontWriteQueueSize=2048, bindIp=0.0.0.0, serverPort=8066, managerPort=9066, charset=utf8mb4, processors=1, processorExecutor=4, timerExecutor=2, managerExecutor=2, idleTimeout=1800000, catletClassCheckSeconds=60, sqlExecuteTimeout=300, processorCheckPeriod=1000, dataNodeIdleCheckPeriod=300000, dataNodeHeartbeatPeriod=10000, clusterHeartbeatUser=_HEARTBEAT_USER_, clusterHeartbeatPass=_HEARTBEAT_PASS_, clusterHeartbeatPeriod=5000, clusterHeartbeatTimeout=10000, clusterHeartbeatRetry=10, txIsolation=3, parserCommentVersion=50148, sqlRecordCount=10, bufferPoolPageSize=2097152, bufferPoolChunkSize=4096, bufferPoolPageNumber=20, maxResultSet=524288, bigResultSizeSqlCount=10, bufferUsagePercent=80, flowControlRejectStrategy=0, clearBigSqLResultSetMapMs=600000, defaultMaxLimit=100, sequnceHandlerType=2, sqlInterceptor=io.mycat.server.interceptor.impl.DefaultSqlInterceptor, sqlInterceptorType=select, sqlInterceptorFile=/usr/local/mycat/logs/sql.txt, mutiNodeLimitType=0, mutiNodePatchSize=100, defaultSqlParser=druidparser, usingAIO=0, packetHeaderSize=4, maxPacketSize=16777216, mycatNodeId=1] 2018-02-05 14:15:41.667 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:381)) - using nio network handler 2018-02-05 14:15:41.681 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:397)) - $_MyCatManager is started and listening on 9066 2018-02-05 14:15:41.682 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:401)) - $_MyCatServer is started and listening on 8066 2018-02-05 14:15:41.682 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:403)) - =============================================== 2018-02-05 14:15:41.682 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.startup(MycatServer.java:407)) - Initialize dataHost ... 2018-02-05 14:15:41.682 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:294)) - init backend myqsl source ,create connections total 10 for hostM1 index :0 2018-02-05 14:15:41.683 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.686 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.686 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.689 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.690 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0000 2018-02-05 14:15:41.834 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=1, lastTime=1517811341834, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=793, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.835 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=2, lastTime=1517811341835, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=797, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.835 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=4, lastTime=1517811341835, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=795, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.836 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=8, lastTime=1517811341836, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=796, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.836 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=7, lastTime=1517811341836, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=794, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.837 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=3, lastTime=1517811341837, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=801, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.837 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=5, lastTime=1517811341837, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=798, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.838 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=6, lastTime=1517811341838, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=799, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.840 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=10, lastTime=1517811341840, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=800, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.842 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=9, lastTime=1517811341842, user=root, schema=adg_system_0000, old shema=adg_system_0000, borrowed=true, fromSlaveDB=false, threadId=802, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3316, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:41.899 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:319)) - init result :finished 10 success 10 target count:10 2018-02-05 14:15:41.899 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.init(PhysicalDBPool.java:265)) - mysql_host_0 index:0 init success 2018-02-05 14:15:41.899 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.saveDataHostIndex(MycatServer.java:604)) - save DataHost index mysql_host_0 cur index 0 2018-02-05 14:15:41.907 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:294)) - init backend myqsl source ,create connections total 10 for hostM1 index :0 2018-02-05 14:15:41.907 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.908 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.908 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.909 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.912 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.915 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.916 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.916 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.916 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:41.917 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0002 2018-02-05 14:15:42.021 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=12, lastTime=1517811342021, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=779, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.023 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=11, lastTime=1517811342022, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=780, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.027 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=13, lastTime=1517811342027, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=781, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.027 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=14, lastTime=1517811342027, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=782, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.030 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=17, lastTime=1517811342030, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=783, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.032 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=16, lastTime=1517811342032, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=785, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.034 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=15, lastTime=1517811342034, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=784, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.036 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=18, lastTime=1517811342036, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=786, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.043 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=19, lastTime=1517811342043, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=787, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.043 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=20, lastTime=1517811342043, user=root, schema=adg_system_0002, old shema=adg_system_0002, borrowed=true, fromSlaveDB=false, threadId=788, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3336, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.118 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:319)) - init result :finished 10 success 10 target count:10 2018-02-05 14:15:42.118 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.init(PhysicalDBPool.java:265)) - mysql_host_2 index:0 init success 2018-02-05 14:15:42.118 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.saveDataHostIndex(MycatServer.java:604)) - save DataHost index mysql_host_2 cur index 0 2018-02-05 14:15:42.118 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:294)) - init backend myqsl source ,create connections total 10 for hostM1 index :0 2018-02-05 14:15:42.119 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.120 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.120 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.121 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.121 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.122 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.122 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.122 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.123 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.123 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDatasource.getConnection(PhysicalDatasource.java:413)) - no ilde connection in pool,create new connection for hostM1 of schema adg_system_0001 2018-02-05 14:15:42.235 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=21, lastTime=1517811342235, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=776, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.237 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=23, lastTime=1517811342237, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=777, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.239 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=26, lastTime=1517811342239, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=781, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.240 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=22, lastTime=1517811342240, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=778, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.243 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=25, lastTime=1517811342243, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=779, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.245 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=24, lastTime=1517811342245, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=780, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.249 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=29, lastTime=1517811342249, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=782, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.249 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=30, lastTime=1517811342249, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=783, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.250 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=28, lastTime=1517811342250, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=785, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.251 INFO [$_NIOREACTOR-0-RW] (io.mycat.backend.mysql.nio.handler.GetConnectionHandler.connectionAcquired(GetConnectionHandler.java:67)) - connected successfuly MySQLConnection [id=27, lastTime=1517811342251, user=root, schema=adg_system_0001, old shema=adg_system_0001, borrowed=true, fromSlaveDB=false, threadId=784, charset=latin1, txIsolation=3, autocommit=true, attachment=null, respHandler=null, host=116.196.111.68, port=3326, statusSync=null, writeQueue=0, modifiedSQLExecuted=false] 2018-02-05 14:15:42.324 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.initSource(PhysicalDBPool.java:319)) - init result :finished 10 success 10 target count:10 2018-02-05 14:15:42.324 INFO [WrapperSimpleAppMain] (io.mycat.backend.datasource.PhysicalDBPool.init(PhysicalDBPool.java:265)) - mysql_host_1 index:0 init success 2018-02-05 14:15:42.324 INFO [WrapperSimpleAppMain] (io.mycat.MycatServer.saveDataHostIndex(MycatServer.java:604)) - save DataHost index mysql_host_1 cur index 0 使用细节 声明： mycat 的库表这里我们称作虚拟库表 创建新表流程： 先编辑 /conf/schema.xml 文件，增加对应的表信息 把创建表 SQL 放在虚拟库上执行，则各个节点的物理库表会增加对应的表结构 只垂直分库流程 垂直切分缺点 如果不采用全局表那就只能通过 API 接口关联表数据（为了增加吞吐，可以考虑多线程并发执行 API 接口后整合） 对于访问频繁、数据大的表，性能瓶颈依旧会存在 这里只是写个大体思路，基础知识上面已经说了。 假设以电商系统为例，拆分出：商品库、用户库、订单库，有 3 个 MySQL 实例各自存储一个业务库 因为不进行水平切分，所以不需要修改 rule.xml 修改 server.xml，增加用户和权限 修改 schema.xml，增加逻辑库配置 dataHost 配置 3 个（只有 3 个 MySQL 实例） dataNode 配置 3 个，分别对应：商品库（1 个）、用户库（1 个）、订单库（1 个） schema 配置： 垂直分库基础上进行水平切分 水平分片原则 能不切分是最好的，能用归档方式分开存储，分开查询的尽可能通过产品思维层面解决 一般只推荐那些数据量大，并且读写频繁的表进行切分 选择合适的切分规则、分片键 尽可能避免跨分片 JOIN 操作 水平分片的步骤 选择分片键和分片算法 一般分片键推荐的是查询条件基本都会带上的那个字段，或者影响面很广的字段 分片键是能尽可能均匀把数据分片到各个节点 没有什么可以选择的时候，推荐就是主键 MyCAT 配置分片节点 测试分片节点 业务数据迁移 对订单相关业务进行水平切分 一般选择订单号或者所属用户 ID 进行分片，这里推荐使用所属用户 ID，因为查询订单信息基本都是从用户角度发起的 前面垂直分库已经修改 server.xml，这里不需要 修改 rule.xml，修改分片规则 user_id by-user-id-to-order 3 修改 schema.xml，增加逻辑库配置 dataHost 配置 3 个（只有 3 个 MySQL 实例） dataNode 配置 5 个，分别对应：商品库（1 个）、用户库（1 个）、订单库（3 个） schema 配置，这里使用取模分片算法： 其他常用配置 SQL 拦截（做审计，不分该 SQL 是否执行成功与否） 修改 server.xml（只拦截 UPDATE,DELETE,INSERT） io.mycat.server.interceptor.impl.StatisticsSqlInterceptor UPDATE,DELETE,INSERT /opt/mycat-log.txt SQL 防火墙 作用 限制某些用户只能通过某些主机访问（whitehost 标签） 屏蔽一些 SQL 语句（blacklist 标签） true 高可用方案（MySQL + MyCAT + Zookeeper + HAProxy + Keepalived） MySQL（3 节点） 端口使用： 3406 3407 3408 docker run -p 3406:3306 --name mycat-mysql-1 -e MYSQL_ROOT_PASSWORD=adgADG123456 -d mysql:5.7 docker run -p 3407:3306 --name mycat-mysql-2 -e MYSQL_ROOT_PASSWORD=adgADG123456 -d mysql:5.7 docker run -p 3408:3306 --name mycat-mysql-3 -e MYSQL_ROOT_PASSWORD=adgADG123456 -d mysql:5.7 MyCAT + Zookeeper Zookeeper 单机多个实例（集群） 端口使用： 2281 2282 2283 创建 docker compose 文件：vim zookeeper.yml 下面内容来自官网仓库：https://hub.docker.com/r/library/zookeeper/ version: '3.1' services: zoo1: image: zookeeper restart: always hostname: zoo1 ports: - 2281:2181 environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper restart: always hostname: zoo2 ports: - 2282:2181 environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper restart: always hostname: zoo3 ports: - 2283:2181 environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888 启动：docker-compose -f zookeeper.yml -p zk_test up -d 参数 -p zk_test 表示这个 compose project 的名字，等价于：COMPOSE_PROJECT_NAME=zk_test docker-compose -f zookeeper.yml up -d 不指定项目名称，Docker-Compose 默认以当前文件目录名作为应用的项目名 报错是正常情况的。 停止：docker-compose -f zookeeper.yml -p zk_test stop MyCAT 单机多个实例 必须有 JDK 环境（我这里使用的是：1.8.0_171） tar -zxvf Mycat-server-1.6.5-release-20180503154132-linux.tar.gz mv mycat mycat-1 cd /usr/local/mycat-1/conf vim server.xml 123456 adg_system false 0 0 cd /usr/local/mycat-1/conf vim schema.xml select user() select user() select user() cd /usr/local/mycat-1/conf vim rule.xml shop_id by-shop-id sharding-by-shop-id.txt 1 0 还需要在 conf 新增文件 sharding-by-shop-id.txt 文件，内容是： 需要注意的是： 417454619141211000=0 417454619141211001=1 417454619141211002=2 CREATE DATABASE adg_system_0000 CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; CREATE DATABASE adg_system_0001 CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; CREATE DATABASE adg_system_0002 CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 使用 Zookeeper 配置 vim /usr/local/mycat-1/conf/myid.properties loadZk=true zkURL=192.168.0.105:2281,192.168.0.105:2282,192.168.0.105:2283 clusterId=mycat-cluster myid=mycat_fz_01 clusterSize=2 clusterNodes=mycat_fz_01,mycat_fz_02 #server booster ; booster install on db same server,will reset all minCon to 2 type=server boosterDataHosts=dataHost1 同步节点配置到 Zookeeper cd /usr/local/mycat-1/conf cp -f schema.xml server.xml rule.xml sharding-by-shop-id.txt zkconf/ sh /usr/local/mycat-1/bin/init_zk_data.sh 重要参数： clusterSize=2 表示有几个 MyCAT 节点数量 解压另外一个节点： tar -zxvf Mycat-server-1.6.5-release-20180503154132-linux.tar.gz mv mycat mycat-2 因为是单机多节点，所以这里需要修改几个端口参数 vim /usr/local/mycat-2/conf/server.xml 旧值： 8066 9066 新值： 8067 9067 vim /usr/local/mycat-2/conf/wrapper.conf 旧值： wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1984 新值： wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1985 修改另外一个节点配置： vim /usr/local/mycat-2/conf/myid.properties loadZk=true zkURL=192.168.0.105:2281,192.168.0.105:2282,192.168.0.105:2283 clusterId=mycat-cluster myid=mycat_fz_02 clusterSize=2 clusterNodes=mycat_fz_01,mycat_fz_02 #server booster ; booster install on db same server,will reset all minCon to 2 type=server boosterDataHosts=dataHost1 启动节点： cd /usr/local/mycat-1/bin 后台启动：./mycat start && tail -300f /usr/local/mycat-1/logs/mycat.log 控制台启动：./mycat console 控制台启动：cd /usr/local/mycat-1/bin && ./mycat console 重启：./mycat restart 停止：./mycat stop cd /usr/local/mycat-2/bin 后台启动：./mycat start && tail -300f /usr/local/mycat-2/logs/mycat.log 控制台启动：./mycat console 控制台启动：cd /usr/local/mycat-2/bin && ./mycat console 重启：./mycat restart 停止：./mycat stop 创建数据结构： CREATE TABLE `adg_ads` ( `ads_id` BIGINT(20) NOT NULL COMMENT '广告表ID', `ads_set_id` BIGINT(20) NOT NULL COMMENT '广告组表ID', `ads_title` VARCHAR(32) NOT NULL COMMENT '广告标题', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告表'; CREATE TABLE `adg_ads_set` ( `ads_set_id` BIGINT(20) NOT NULL COMMENT '广告组表ID', `ads_set_title` VARCHAR(32) NOT NULL COMMENT '广告组标题', `ads_campaign_id` BIGINT(20) NOT NULL COMMENT '广告系列表ID', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_set_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告组表'; CREATE TABLE `adg_ads_campaign` ( `ads_campaign_id` BIGINT(20) NOT NULL COMMENT '广告系列表ID', `ads_campaign_title` VARCHAR(32) NOT NULL COMMENT '广告系列标题', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`ads_campaign_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='广告系列表'; CREATE TABLE `adg_channel` ( `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`channel_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='渠道表'; CREATE TABLE `adg_shop` ( `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', PRIMARY KEY (`shop_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='商品表'; CREATE TABLE `adg_shop_channel` ( `shop_channel_id` BIGINT(20) NOT NULL COMMENT '店铺渠道中间表ID', `shop_id` BIGINT(20) NOT NULL COMMENT '店铺ID', `channel_id` BIGINT(20) NOT NULL COMMENT '渠道ID', `shop_name` VARCHAR(32) NOT NULL COMMENT '店铺名称', `channel_name` VARCHAR(32) NOT NULL COMMENT '渠道名称', PRIMARY KEY (`shop_channel_id`) ) ENGINE=INNODB DEFAULT CHARSET=utf8mb4 COMMENT='店铺渠道中间表'; INSERT INTO `adg_shop`(`shop_id`,`shop_name`) VALUES (417454619141211000,'NC站'); INSERT INTO `adg_shop`(`shop_id`,`shop_name`) VALUES (417454619141211001,'BG站'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (1,'Facebook'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (2,'Google'); INSERT INTO `adg_channel`(`channel_id`,`channel_name`) VALUES (3,'Twitter'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,417454619141211000,2,'NC站','Google'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (3,417454619141211001,1,'BG站','Facebook'); INSERT INTO `adg_shop_channel`(`shop_channel_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (4,417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads_campaign`(`ads_campaign_id`,`ads_campaign_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,'第1个广告系列',417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads_campaign`(`ads_campaign_id`,`ads_campaign_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,'第2个广告系列',417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads_set`(`ads_set_id`,`ads_set_title`,`ads_campaign_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,'第1个广告集',1,417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads_set`(`ads_set_id`,`ads_set_title`,`ads_campaign_id`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,'第2个广告集',2,417454619141211001,2,'BG站','Google'); INSERT INTO `adg_ads`(`ads_id`,`ads_set_id`,`ads_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (1,1,'第1个广告',417454619141211000,1,'NC站','Facebook'); INSERT INTO `adg_ads`(`ads_id`,`ads_set_id`,`ads_title`,`shop_id`,`channel_id`,`shop_name`,`channel_name`) VALUES (2,2,'第2个广告',417454619141211001,2,'BG站','Google'); HAProxy + Keepalived 资料 书：《分布式数据库架构及企业实践-基于 Mycat 中间件》 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Zookeeper-Install.html":{"url":"Linux-Tutorial/markdown-file/Zookeeper-Install.html","title":"Zookeeper 安装和配置","keywords":"","body":"Zookeeper 安装 Docker 部署 Zookeeper 单个实例 官网仓库：https://hub.docker.com/r/library/zookeeper/ 单个实例：docker run -d --restart always --name one-zookeeper -p 2181:2181 -v /etc/localtime:/etc/localtime zookeeper:latest 默认端口暴露是：This image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively) 容器中的几个重要目录（有需要挂载的可以指定）： /data /datalog /conf 单机多个实例（集群） 创建 docker compose 文件：vim zookeeper.yml 下面内容来自官网仓库：https://hub.docker.com/r/library/zookeeper/ version: '3.1' services: zoo1: image: zookeeper restart: always hostname: zoo1 ports: - 2181:2181 environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper restart: always hostname: zoo2 ports: - 2182:2181 environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper restart: always hostname: zoo3 ports: - 2183:2181 environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888 启动：docker-compose -f zookeeper.yml -p zk_test up -d 参数 -p zk_test 表示这个 compose project 的名字，等价于：COMPOSE_PROJECT_NAME=zk_test docker-compose -f zookeeper.yml up -d 不指定项目名称，Docker-Compose 默认以当前文件目录名作为应用的项目名 报错是正常情况的。 停止：docker-compose -f zookeeper.yml -p zk_test stop 先安装 nc 再来校验 zookeeper 集群情况 环境：CentOS 7.4 官网下载：https://nmap.org/download.html，找到 rpm 包 当前时间（201803）最新版本下载：wget https://nmap.org/dist/ncat-7.60-1.x86_64.rpm 安装：sudo rpm -i ncat-7.60-1.x86_64.rpm ln 下：sudo ln -s /usr/bin/ncat /usr/bin/nc 检验：nc --version 校验 命令：echo stat | nc 127.0.0.1 2181，得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.21.0.1:58872[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x100000000 Mode: follower Node count: 4 命令：echo stat | nc 127.0.0.1 2182，得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.21.0.1:36190[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x500000000 Mode: follower Node count: 4 命令：echo stat | nc 127.0.0.1 2183，得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.21.0.1:33344[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x500000000 Mode: leader Node count: 4 多机多个实例（集群） 三台机子： 内网 ip：172.24.165.129，外网 ip：47.91.22.116 内网 ip：172.24.165.130，外网 ip：47.91.22.124 内网 ip：172.24.165.131，外网 ip：47.74.6.138 修改三台机子 hostname： 节点 1：hostnamectl --static set-hostname youmeekhost1 节点 2：hostnamectl --static set-hostname youmeekhost2 节点 3：hostnamectl --static set-hostname youmeekhost3 三台机子的 hosts 都修改为如下内容：vim /etc/hosts 172.24.165.129 youmeekhost1 172.24.165.130 youmeekhost2 172.24.165.131 youmeekhost3 节点 1： docker run -d \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -e ZOO_MY_ID=1 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ --name=zookeeper1 --net=host --restart=always zookeeper 节点 2： docker run -d \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -e ZOO_MY_ID=2 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ --name=zookeeper2 --net=host --restart=always zookeeper 节点 3： docker run -d \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -e ZOO_MY_ID=3 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ --name=zookeeper3 --net=host --restart=always zookeeper 需要环境 JDK 安装 下载安装 官网：https://zookeeper.apache.org/ 此时（201702）最新稳定版本：Release 3.4.9 官网下载：http://www.apache.org/dyn/closer.cgi/zookeeper/ 我这里以：zookeeper-3.4.8.tar.gz 为例 安装过程： mkdir -p /usr/program/zookeeper/data cd /opt/setups tar zxvf zookeeper-3.4.8.tar.gz mv /opt/setups/zookeeper-3.4.8 /usr/program/zookeeper cd /usr/program/zookeeper/zookeeper-3.4.8/conf mv zoo_sample.cfg zoo.cfg vim zoo.cfg 将配置文件中的这个值： 原值：dataDir=/tmp/zookeeper 改为：dataDir=/usr/program/zookeeper/data 防火墙开放2181端口 iptables -A INPUT -p tcp -m tcp --dport 2181 -j ACCEPT service iptables save service iptables restart 启动 zookeeper：sh /usr/program/zookeeper/zookeeper-3.4.8/bin/zkServer.sh start 停止 zookeeper：sh /usr/program/zookeeper/zookeeper-3.4.8/bin/zkServer.sh stop 查看 zookeeper 状态：sh /usr/program/zookeeper/zookeeper-3.4.8/bin/zkServer.sh status 如果是集群环境，下面几种角色 leader follower 集群环境搭建 确定机子环境 集群环境最少节点是：3，且节点数必须是奇数，生产环境推荐是：5 个机子节点。 系统都是 CentOS 6 机子 1：192.168.1.121 机子 2：192.168.1.111 机子 3：192.168.1.112 配置 在三台机子上都做如上文的流程安装，再补充修改配置文件：vim /usr/program/zookeeper/zookeeper-3.4.8/conf/zoo.cfg 三台机子都增加下面内容： server.1=192.168.1.121:2888:3888 server.2=192.168.1.111:2888:3888 server.3=192.168.1.112:2888:3888 在 机子 1 增加一个该文件：vim /usr/program/zookeeper/data/myid，文件内容填写：1 在 机子 2 增加一个该文件：vim /usr/program/zookeeper/data/myid，文件内容填写：2 在 机子 3 增加一个该文件：vim /usr/program/zookeeper/data/myid，文件内容填写：3 然后在三台机子上都启动 zookeeper：sh /usr/program/zookeeper/zookeeper-3.4.8/bin/zkServer.sh start 分别查看三台机子的状态：sh /usr/program/zookeeper/zookeeper-3.4.8/bin/zkServer.sh status，应该会得到类似这样的结果： Using config: /usr/program/zookeeper/zookeeper-3.4.8/bin/../conf/zoo.cfg Mode: follower 或者 Mode: leader Zookeeper 客户端工具 ZooInspector 下载地址：https://issues.apache.org/jira/secure/attachment/12436620/ZooInspector.zip 解压，双击 jar 文件，效果如下： zooweb 下载地址：https://github.com/zhuhongyu345/zooweb Spring Boot 的 Web 项目，直接：java -jar zooweb-1.0.jar 启动 web 服务，然后访问：http://127.0.0.1:9345 资料 https://liwei.io/2017/07/19/zookeeper-cluster-in-docker/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Daemontools.html":{"url":"Linux-Tutorial/markdown-file/Daemontools.html","title":"Daemontools 工具介绍","keywords":"","body":"daemontools 工具 supervisord 注意：Supervisor 能管理非 daemon 的进程，也就是说 Supervisor 不能管理守护进程。否则提示 Exited too quickly (process log may have details) 异常。 Supervisor 不支持 python 3，安装 python 2 方法：http://www.cnblogs.com/alex-xia/p/6062741.html 官网：http://supervisord.org/installing.html 安装过程： 解释：easy_install 是 setuptools 包里带的一个命令，使用 easy_install 实际上是在调用 setuptools 来完成安装模块的工作,所以安装 setuptools 即可。 安装方案： #第一种（推荐） yum install python-setuptools easy_install supervisor #第二种 yum install python-setuptools easy_install pip pip install supervisor #第三种 yum install -y epel-release yum install -y supervisor 如果以上还不能安装，或是安装过程出现各种问题，或是安装完成后使用出现问题，应该就是环境有问题。至少我在京东云上发现会有这个问题。环境是 centos 6.8，python 2.6.6 如果你遇到这种问题需要源码安装。 源码和各个依赖的源码下载地址（密码：j797）：http://pan.baidu.com/s/1hsGhNkK tar zxvf setuptools-36.6.0.tar.gz cd setuptools-36.6.0 python bootstrap.py install python setup.py install tar zxvf meld3.tar.gz cd meld3 python setup.py install tar zxvf elementtree-1.2.6-20050316.tar.gz cd elementtree-1.2.6-20050316 python setup.py install tar zxvf supervisor-3.3.3.tar.gz cd supervisor-3.3.3 python setup.py install 生成配置文件：echo_supervisord_conf > /etc/supervisord.conf 创建专门的程序配置文件目录、日志目录： mkdir -p /var/log/supervisor mkdir -p /etc/supervisor/conf.d/ echo -e \"[include]\\nfiles = /etc/supervisor/conf.d/*.conf\">>/etc/supervisord.conf 安装完成的内容介绍：supervisor 安装完成后会生成三个执行程序： supervisortd：supervisor 的守护进程服务（用于接收进程管理命令） supervisorctl：客户端（用于和守护进程通信，发送管理进程的指令） echo_supervisord_conf：生成初始配置文件程序。 程序位置：/usr/bin/supervisord 配置文件位置：/etc/supervisord.conf Logstash 进程进行守护 默认安装完 Supervisor 是已经启动的，所以在加入新配置之前，需要先停止程序：ps -ef | grep supervisord，kill 对应的 pid 创建配置文件：vim /etc/supervisor/conf.d/logstash.conf [program:gitnavi-logstash] command=/usr/program/elk/logstash-2.4.1/bin/logstash -f /usr/program/elk/logstash-2.4.1/config/logstash.conf stdout_logfile=/var/log/supervisor/supervisord-logstash.log stderr_logfile=/var/log/supervisor/supervisord-logstash-err.log user=root autostart=true autorestart=false startsecs=5 priority=1 stopasgroup=true killasgroup=true 该配置的具体说明可以参考：使用 supervisor 管理进程 启动程序（默认会启动所有子任务）：/usr/bin/supervisord -c /etc/supervisord.conf 管理子任务的命令： 子任务状态：/usr/bin/supervisorctl status 启动所有子任务：/usr/bin/supervisorctl start all 结束所有子任务：/usr/bin/supervisorctl stop all 启动指定子任务：/usr/bin/supervisorctl start gitnavi-logstash 结束指定子任务：/usr/bin/supervisorctl stop gitnavi-logstash 重启指定子任务：/usr/bin/supervisorctl restart gitnavi-logstash 只载入最新的配置文件, 并不重启任何进程：/usr/bin/supervisorctl reread 载入最新的配置文件，停止原来的所有进程并按新的配置启动管理所有进程：/usr/bin/supervisorctl reload 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启：/usr/bin/supervisorctl update 查看所有子任务状态，如果没有运行的子任务则是没有任何反馈信息：/usr/bin/supervisorctl status 管理所有子任务也可以用交互方式，输入命令：supervisorctl，会进入 supervisord 的交互模式下，如果当前有启动的任务，还可以看到对应的任务情况。 在该交互下可以停止指定名称的子任务，比如 logstash 任务：stop gitnavi-logstash 也可以停止所有子任务：stop all 也可以启动所有子任务：start all 更多命令可以输入：help 设置 supervisord 开启自启动 CentOS 6 创建文件：vim /etc/init.d/supervisord #!/bin/sh # # Supervisor is a client/server system that # allows its users to monitor and control a # number of processes on UNIX-like operating # systems. # # chkconfig: - 64 36 # description: Supervisor Server # processname: supervisord # Source init functions . /etc/init.d/functions RETVAL=0 prog=\"supervisord\" pidfile=\"/tmp/supervisord.pid\" lockfile=\"/var/lock/subsys/supervisord\" start() { echo -n $\"Starting $prog: \" daemon --pidfile $pidfile supervisord -c /etc/supervisord.conf RETVAL=$? echo [ $RETVAL -eq 0 ] && touch ${lockfile} } stop() { echo -n $\"Shutting down $prog: \" killproc -p ${pidfile} /usr/bin/supervisord RETVAL=$? echo if [ $RETVAL -eq 0 ] ; then rm -f ${lockfile} ${pidfile} fi } case \"$1\" in start) start ;; stop) stop ;; status) status $prog ;; restart) stop start ;; *) echo \"Usage: $0 {start|stop|restart|status}\" ;; esac chmod 755 /etc/init.d/supervisord chkconfig supervisord on 以后启动可以用：service supervisord start 以后停止可以用：service supervisord stop CentOS 7 创建文件：vim /lib/systemd/system/supervisor.service [Unit] Description=supervisor After=network.target [Service] Type=forking ExecStart=/usr/bin/supervisord -c /etc/supervisord.conf ExecStop=/usr/bin/supervisorctl $OPTIONS shutdown ExecReload=/usr/bin/supervisorctl $OPTIONS reload KillMode=process Restart=on-failure RestartSec=42s [Install] WantedBy=multi-user.target chmod 766 /lib/systemd/system/supervisor.service systemctl enable supervisor.service systemctl daemon-reload 资料 http://blog.csdn.net/xyang81/article/details/51555473 https://www.fangc.xyz/detail/centos6pei-zhi-supervisorkai-j/ http://cpper.info/2016/04/14/supervisor-usage.html https://luckymrwang.github.io/2016/12/23/Supervisor%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/ http://www.aichengxu.com/linux/24569479.htm http://www.tianfeiyu.com/?p=2450 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Tmux-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Tmux-Install-And-Settings.html","title":"Tmux 安装和配置","keywords":"","body":"Tmux 安装和配置 介绍 说明：tmux is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. tmux is intended to be a simple, modern, BSD-licensed alternative to programs such as GNU screen. 大家的主要用途：提供了一个窗体组随时存储和恢复的功能，本质有点类似守护进程感。 官网：https://tmux.github.io/ 官网 Github：https://github.com/tmux 当前（201703）最新版本：2.3 安装 CentOS：yum install -y tmux Ubuntu：apt-get install -y tmux Mac：brew install tmux 也可以看官网 GitHub 进行编译安装。 基本概念 session：一个服务器可以包含多个会话，可以理解成是一个特定的终端组合，通常将同一任务下的工作放到一个会话中。 window：一个会话可以包含多个窗口，一个窗口就相当于普通终端的一个标签，通常在不同的窗口中完成不同的工作。 pane：一个窗口可以被分割成多个小的窗格。 基础用法 启动：tmux 信息查询： tmux list-keys 列出所有可以的快捷键和其运行的 tmux 命令 tmux list-commands 列出所有的 tmux 命令及其参数 tmux info 流出所有的 session, window, pane, 运行的进程号，等。 窗口的控制： session 会话：session是一个特定的终端组合。输入tmux就可以打开一个新的session tmux new -s session_name 创建一个叫做 session_name 的 tmux session tmux attach -t session_name 重新开启叫做 session_name 的 tmux session tmux switch -t session_name 转换到叫做 session_name 的 tmux session tmux list-sessions / tmux ls 列出现有的所有 session tmux detach 离开当前开启的 session tmux kill-server 关闭所有 session window 窗口：session 中可以有不同的 window（但是同时只能看到一个 window） tmux new-window 创建一个新的 window tmux list-windows tmux select-window -t :0-9 根据索引转到该 window tmux rename-window 重命名当前 window pane 面板：window 中可以有不同的 pane（可以把 window 分成不同的部分） tmux split-window 将 window 垂直划分为两个 pane tmux split-window -h 将 window 水平划分为两个 pane tmux swap-pane -U 在指定的方向（方向有：U、D、L、R 四种）交换 pane tmux select-pane -U 在指定的方向（方向有：U、D、L、R 四种）选择下一个 pane 高级用法 注意： 有一个前缀快捷键的概念，也称作：，默认快捷键：Ctrl + B，下面的这些操作都是必须先按这个快捷键后再输入对应的命令： 基本操作 ? 列出所有快捷键；按q返回 d 脱离当前会话,可暂时返回Shell界面 s 选择并切换会话；在同时开启了多个会话时使用 D 选择要脱离的会话；在同时开启了多个会话时使用 : 进入命令行模式；此时可输入支持的命令，例如 kill-server 关闭所有tmux会话 [ 复制模式，光标移动到复制内容位置，空格键开始，方向键选择复制，回车确认，q/Esc退出 ] 进入粘贴模式，粘贴之前复制的内容，按q/Esc退出 ~ 列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 t 显示当前的时间 ctrl + z 挂起当前会话 窗口操作 c 创建新窗口 & 关闭当前窗口 [0-9] 数字键切换到指定窗口 p 切换至上一窗口 n 切换至下一窗口 l 前后窗口间互相切换 w 通过窗口列表切换窗口 , 重命名当前窗口，便于识别 . 修改当前窗口编号，相当于重新排序 f 在所有窗口中查找关键词，便于窗口多了切换 面板操作 \" 将当前面板上下分屏（我自己改成了 |） % 将当前面板左右分屏（我自己改成了 -） x 关闭当前分屏 ! 将当前面板置于新窗口,即新建一个窗口,其中仅包含当前面板 Ctrl + 方向键 以1个单元格为单位移动边缘以调整当前面板大小 Alt + 方向键 以5个单元格为单位移动边缘以调整当前面板大小 q 显示面板编号 o 选择当前窗口中下一个面板 方向键 移动光标选择对应面板 { 向前置换当前面板 } 向后置换当前面板 Alt+o 逆时针旋转当前窗口的面板 Ctrl+o 顺时针旋转当前窗口的面板 z 最大化当前所在面板 page up 向上滚动屏幕，q 退出 page down 向下滚动屏幕，q 退出 资料 http://kuanghy.github.io/2016/09/29/tmux http://harttle.com/2015/11/06/tmux-startup.html http://cenalulu.github.io/linux/tmux/ http://wdxtub.com/2016/03/30/tmux-guide/ https://gist.github.com/ryerh/14b7c24dfd623ef8edc7 http://cenalulu.github.io/linux/tmux/ http://fishcried.com/2014-09-15/tmux/ <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/ELK-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/ELK-Install-And-Settings.html","title":"ELK 日志收集系统安装和配置","keywords":"","body":"ELK（Elasticsearch、Logstash、Kibana）安装和配置 版本说明 本文包含了：Elasticsearch 2.4.X 和 Elasticsearch 5.2.X 和 Elasticsearch 5.5.X，请有针对性地选择。 教程说明 官网：https://www.elastic.co/ 官网总文档：https://www.elastic.co/guide/index.html 官网最终指南：https://www.elastic.co/guide/en/elasticsearch/guide/current/administration.html#administration 官网对各个系统的支持列表：https://www.elastic.co/support/matrix 5.2 版本有一个设置的新特性必须了解，测试建议我们用 CentOS 7：https://www.elastic.co/guide/en/elasticsearch/reference/5.x/breaking-changes-5.2.html#_system_call_bootstrap_check Elasticsearch 开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful 风格接口，多数据源，自动搜索负载等。 Logstash 日志进行收集、分析，并将其存储供以后使用（如，搜索） kibana 为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 Elasticsearch 部署 请看 Elasticsearch 专题文：Elasticsearch 相关知识 logstash 请看 logstash 专题文：logstash 相关知识 资料 http://www.centoscn.com/CentosServer/test/2017/0304/8575.html https://blog.yourtion.com/install-x-pack-for-elasticsearch-and-kibana.html http://www.voidcn.com/blog/987146971/article/p-6290041.html http://www.web520.cn/archives/31077 <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Dubbo-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Dubbo-Install-And-Settings.html","title":"Dubbo 安装和配置","keywords":"","body":"Dubbo 环境安装和配置 Dubbo-Admin 搭建 需要环境 CentOS 6 JDK 1.8（必须是 1.8） Tomcat 7 或 8（我测试都可以） Dubbo 版本：2.5.4-SNAPSHOT（当前这个版本是最新的。2.5.3 我测试是不行的） Dubbo-Admin 需要修改部分代码，让它支持 JDK 8，具体看文章：https://github.com/alibaba/dubbo/issues/50 最终我的 Dubbo-admin 的 pom.xml 为这样的： 4.0.0 com.alibaba dubbo-parent 2.5.4-SNAPSHOT dubbo-admin war ${project.artifactId} The admin module of dubbo project 1.5 / false false com.alibaba dubbo ${project.parent.version} org.springframework spring com.alibaba.citrus citrus-webx-all 3.1.6 org.apache.velocity velocity 1.7 org.javassist javassist org.jboss.netty netty org.apache.mina mina-core org.glassfish.grizzly grizzly-core org.apache.httpcomponents httpclient com.alibaba fastjson com.thoughtworks.xstream xstream org.apache.bsf bsf-api org.apache.zookeeper zookeeper com.github.sgroschupf zkclient com.netflix.curator curator-framework com.googlecode.xmemcached xmemcached org.apache.thrift libthrift com.caucho hessian javax.servlet servlet-api provided log4j log4j org.slf4j slf4j-api org.slf4j slf4j-log4j12 redis.clients jedis javax.validation validation-api org.hibernate hibernate-validator javax.cache cache-api org.mortbay.jetty maven-jetty-plugin ${jetty_version} / 10 8080 60000 部署在 Tomcat 之后，如果需要修改 zookeeper 的地址，以及默认用户的登录密码，可以在这里改：vim /usr/program/tomcat8/webapps/ROOT/WEB-INF/dubbo.properties 里面内容： dubbo.registry.address=zookeeper://127.0.0.1:2181 dubbo.admin.root.password=root dubbo.admin.guest.password=guest 如果有多个节点构成的集群也可以这样写： dubbo.registry.address=zookeeper://192.168.1.121:2181?backup=192.168.1.111:2181,192.168.1.112:2181 dubbo.admin.root.password=root dubbo.admin.guest.password=guest Dubbox-Admin 搭建 需要环境 CentOS 6 JDK 1.8（只测试过 1.8） Tomcat 7 或 8（只测试过 8） Dubbox 版本：dubbo-admin-2.8.4.war（fork 官网最新版本，自己编译的版本） 不需要修改任何 pom.xml 文件，直接就可以运行。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Gitlab-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Gitlab-Install-And-Settings.html","title":"GitLab 安装和配置","keywords":"","body":"Gitlab 安装和配置 Docker Compose 安装方式 创建宿主机挂载目录：mkdir -p /data/docker/gitlab/gitlab /data/docker/gitlab/redis /data/docker/gitlab/postgresql 赋权（避免挂载的时候，一些程序需要容器中的用户的特定权限使用）：chown -R 777 /data/docker/gitlab/gitlab /data/docker/gitlab/redis /data/docker/gitlab/postgresql 这里使用 docker-compose 的启动方式，所以需要创建 docker-compose.yml 文件： gitlab: image: sameersbn/gitlab:10.4.2-1 ports: - \"10022:22\" - \"10080:80\" links: - gitlab-redis:redisio - gitlab-postgresql:postgresql environment: - GITLAB_PORT=80 - GITLAB_SSH_PORT=22 - GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string - GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string - GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string volumes: - /data/docker/gitlab/gitlab:/home/git/data restart: always gitlab-redis: image: sameersbn/redis volumes: - /data/docker/gitlab/redis:/var/lib/redis restart: always gitlab-postgresql: image: sameersbn/postgresql:9.6-2 environment: - DB_NAME=gitlabhq_production - DB_USER=gitlab - DB_PASS=password - DB_EXTENSION=pg_trgm volumes: - /data/docker/gitlab/postgresql:/var/lib/postgresql restart: always 启动：docker-compose up -d 浏览器访问：http://192.168.0.105:10080 Gitlab 高可用方案（High Availability） 官网：https://about.gitlab.com/high-availability/ 本质就是把文件、缓存、数据库抽离出来，然后部署多个 Gitlab 用 nginx 前面做负载。 原始安装方式 环境： CPU：1 core 内存：2G 我习惯使用 root 用户 有开源版本和收费版本，各版本比较：https://about.gitlab.com/products/ 官网：https://about.gitlab.com/ 中文网：https://www.gitlab.com.cn/ 官网下载：https://about.gitlab.com/downloads/ 安装的系统环境要求：https://docs.gitlab.com/ce/install/requirements.html 从文章看目前要求 ruby 2.3，用 yum 版本过低，那就源码安装 ruby 吧，官网当前最新是：2.4.1（大小：14M） 官网安装说明：https://about.gitlab.com/installation/#centos-7 安装 ruby 下载：https://www.ruby-lang.org/en/downloads/ 解压：tar zxvf ruby-2.4.1.tar.gz 编译安装： cd ruby-2.4.1 ./configure make，过程有点慢 make install 默认安装到这个目录：/usr/local 查看当前版本号：ruby -v CentOS 6 安装流程：https://about.gitlab.com/downloads/#centos6 当前（201703）的版本是：GitLab Community Edition 9.0.0 sudo yum install -y curl openssh-server openssh-clients postfix cronie sudo service postfix start sudo chkconfig postfix on sudo lokkit -s http -s ssh curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash sudo yum install gitlab-ce，软件大小：272M，下载速度不稳定 sudo gitlab-ctl reconfigure，这个过程比较慢 如果上面的下载比较慢，也有国内的镜像： 清华：https://mirror.tuna.tsinghua.edu.cn/help/gitlab-ce/ 配置 配置域名 / IP 编辑配置文件：sudo vim /etc/gitlab/gitlab.rb 找到 13 行左右：external_url 'http://gitlab.example.com'，改为你的域名 / IP 重启服务：sudo gitlab-ctl reconfigure 前面的初始化配置完成之后，访问当前机子 IP：http://192.168.1.111:80 默认用户是 root，并且没有密码，所以第一次访问是让你设置你的 root 密码，我设置为：gitlab123456（至少 8 位数） 设置会初始化密码之后，你就需要登录了。输入设置的密码。 root 管理员登录之后常用的设置地址（请求地址都是 RESTful 风格很好懂，也应该不会再变了。）： 用户管理：http://192.168.1.111/admin/users 用户组管理：http://192.168.1.111/admin/groups 项目管理：http://192.168.1.111/admin/projects 添加 SSH Keys：http://192.168.1.111/profile/keys 给新创建的用户设置密码：http://192.168.1.111/admin/users/用户名/edit 新创建的用户，他首次登录会要求他强制修改密码的，这个设定很棒！ 普通用户登录之后常去的链接： 配置 SSH Keys：http://192.168.1.111/profile/keys 配置 Jenkins 拉取代码权限 Gitlab 创建一个 Access Token：http://192.168.0.105:10080/profile/personal_access_tokens 填写任意 Name 字符串 勾选：API Access the authenticated user's API 点击：Create personal access token，会生成一个类似格式的字符串：wt93jQzA8yu5a6pfsk3s，这个 Jenkinsfile 会用到 先访问 Jenkins 插件安装页面，安装下面三个插件：http://192.168.0.105:18080/pluginManager/available Gitlab：可能会直接安装不成功，如果不成功根据报错的详细信息可以看到 hpi 文件的下载地址，挂代理下载下来，然后离线安装即可 Gitlab Hook：用于触发 GitLab 的一些 WebHooks 来构建项目 Gitlab Authentication 这个插件提供了使用GitLab进行用户认证和授权的方案 安装完插件后，访问 Jenkins 这个路径（Jenkins-->Credentials-->System-->Global credentials(unrestricted)-->Add Credentials） 该路径链接地址：http://192.168.0.105:18080/credentials/store/system/domain/_/newCredentials kind 下拉框选择：GitLab API token token 就填写我们刚刚生成的 access token ID 填写我们 Gitlab 账号 权限 官网帮助文档的权限说明：http://192.168.1.111/help/user/permissions 用户组的权限 用户组有这几种权限的概念：Guest、Reporter、Developer、Master、Owner 这个概念在设置用户组的时候会遇到，叫做：Add user(s) to the group，比如链接：http://192.168.1.111/admin/groups/组名称 行为 Guest Reporter Developer Master Owner 浏览组 ✓ ✓ ✓ ✓ ✓ 编辑组 ✓ 创建项目 ✓ ✓ 管理组成员 ✓ 移除组 ✓ 项目组的权限 项目组也有这几种权限的概念：Guest、Reporter、Developer、Master、Owner Guest：访客 Reporter：报告者; 可以理解为测试员、产品经理等，一般负责提交issue等 Developer：开发者; 负责开发 Master：主人; 一般是组长，负责对Master分支进行维护 Owner：拥有者; 一般是项目经理 这个概念在项目设置的时候会遇到，叫做：Members，比如我有一个组下的项目链接：http://192.168.1.111/组名称/项目名称/settings/members 行为 Guest Reporter Developer Master Owner 创建issue ✓ ✓ ✓ ✓ ✓ 留言评论 ✓ ✓ ✓ ✓ ✓ 更新代码 ✓ ✓ ✓ ✓ 下载工程 ✓ ✓ ✓ ✓ 创建代码片段 ✓ ✓ ✓ ✓ 创建合并请求 ✓ ✓ ✓ 创建新分支 ✓ ✓ ✓ 提交代码到非保护分支 ✓ ✓ ✓ 强制提交到非保护分支 ✓ ✓ ✓ 移除非保护分支 ✓ ✓ ✓ 添加tag ✓ ✓ ✓ 创建wiki ✓ ✓ ✓ 管理issue处理者 ✓ ✓ ✓ 管理labels ✓ ✓ ✓ 创建里程碑 ✓ ✓ 添加项目成员 ✓ ✓ 提交保护分支 ✓ ✓ 使能分支保护 ✓ ✓ 修改/移除tag ✓ ✓ 编辑工程 ✓ ✓ 添加deploy keys ✓ ✓ 配置hooks ✓ ✓ 切换visibility level ✓ 切换工程namespace ✓ 移除工程 ✓ 强制提交保护分支 ✓ 移除保护分支 ✓ 批量从一个项目中的成员转移到另外一个项目 项目的设置地址：http://192.168.1.111/组名称/项目名称/settings/members 有一个 Import 按钮，跳转到：http://192.168.1.111/组名称/项目名称/project_members/import 限定哪些分支可以提交、可以 merge 也是在项目设置里面：http://192.168.1.111/组名称/项目名称/settings/repository# 设置 CI （持续集成） 的 key 也是在这个地址上设置。 Gitlab 的其他功能使用 创建用户 地址：http://119.23.252.150:10080/admin/users/ 创建用户是没有填写密码的地方，默认是创建后会发送邮件给用户进行首次使用的密码设置。但是，有时候没必要这样，你可以创建好用户之后，编辑该用户就可以强制设置密码了（即使你设置了，第一次用户使用还是要让你修改密码...真是严苛） 创建群组 地址：http://119.23.252.150:10080/groups 群组主要有三种 Visibility Level： Private（私有，内部成员才能看到），The group and its projects can only be viewed by members. Internal（内部，只要能登录 Gitlab 就可以看到），The group and any internal projects can be viewed by any logged in user. Public（所有人都可以看到），The group and any public projects can be viewed without any authentication. 创建项目 地址：http://119.23.252.150:10080/ 增加 SSH keys 地址：http://119.23.252.150:10080/profile/keys 官网指导：http://119.23.252.150:10080/help/ssh/README 新增 SSH keys：ssh-keygen -t rsa -C \"gitnavi@qq.com\" -b 4096 linux 读取 SSH keys 值：cat ~/.ssh/id_rsa.pub，复制到 gitlab 配置页面 使用 Gitlab 的一个开发流程 - Git flow Git flow：我是翻译成：Git 开发流程建议（不是规范，适合大点的团队），也有一个插件叫做这个，本质是用插件来帮你引导做规范的流程管理。 这几篇文章很好，不多说了： 比较起源的一个说明（英文）：http://nvie.com/posts/a-successful-git-branching-model/ Git-flow 插件也是他开发的，插件地址：https://github.com/nvie/gitflow Git-flow 插件的一些相关资料： https://www.git-tower.com/learn/git/ebook/cn/command-line/advanced-topics/git-flow https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html http://www.ruanyifeng.com/blog/2015/12/git-workflow.html https://zhangmengpl.gitbooks.io/gitlab-guide/content/whatisgitflow.html http://blog.jobbole.com/76867/ http://www.cnblogs.com/cnblogsfans/p/5075073.html 资料 http://blog.smallmuou.xyz/git/2016/03/11/%E5%85%B3%E4%BA%8EGitlab%E8%8B%A5%E5%B9%B2%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98.html https://zhangmengpl.gitbooks.io/gitlab-guide/content/whatisgitflow.html https://blog.coderstory.cn/2017/02/01/gitlab/ https://xuanwo.org/2016/04/13/gitlab-install-intro/ https://softlns.github.io/2016/11/14/jenkins-gitlab-docker/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/JMeter-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/JMeter-Install-And-Settings.html","title":"JMeter 安装和配置","keywords":"","body":"JMeter 安装和配置 JMeter 介绍 JMeter 用 Java 开发，需要 JDK 环境，当前最新版至少需要 JDK 8 官网：https://jmeter.apache.org/ 官网下载：https://jmeter.apache.org/download_jmeter.cgi 官网插件库：https://jmeter-plugins.org/wiki/Start/ 官网 Github 源码：https://github.com/apache/jmeter 当前（201804）最新版本为 4.0 JMeter Windows 安装 因为是绿色版本，直接解压即可，比如我解压后目录：D:\\software\\portable\\apache-jmeter-4.0\\bin 直接双击运行：ApacheJMeter.jar（会根据当前系统是中文自行进行切换语言） 也可以直接双击：jmeter.bat（显示的是英文版本） 如果是作为分布式中的客户端，需要执行：jmeter-server.bat 其他： jmeter.log 是 JMeter 的 log jmeter.properties 是 JMeter 软件配置 JMeter Linux 安装 解压 zip 包，假设我放在 增加环境变量：vim ~/.zshrc（我用的是 zsh） # JMeter JMETER_HOME=/usr/local/apache-jmeter-4.0 PATH=$PATH:$JMETER_HOME/bin export JMETER_HOME export PATH 刷新配置：source ~/.zshrc 测试：jmeter -v，输出结果： Apr 12, 2018 10:14:24 AM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. _ ____ _ ____ _ _ _____ _ __ __ _____ _____ _____ ____ / \\ | _ \\ / \\ / ___| | | | ____| | | \\/ | ____|_ _| ____| _ \\ / _ \\ | |_) / _ \\| | | |_| | _| _ | | |\\/| | _| | | | _| | |_) | / ___ \\| __/ ___ \\ |___| _ | |___ | |_| | | | | |___ | | | |___| _ JMeter Linux 测试 Windows 是 GUI 操作，测试方便，Linux 只能用命令，所以这里进行记录 准备好 jmx 测试脚本文件（可以在 Windows 上使用后，保存的脚本就是 jmx 后缀文件） 测试：jmeter -n -t /opt/myTest.jmx -l /opt/myReport.jtl 参数 n 表示以 nogui 方式运行测试计划 参数 t 表示指定测试计划 参数 l 表示生成测试报告 显示 ... end of run 即脚本运行结束 如果要后台运行 JMeter 可以：setsid jmeter -n -t /opt/myTest.jmx -l /opt/myReport.jtl 将上一步运行结束后生成的.jtl 文件拷贝到 win 上，打开 win 上的 Jmeter，随便新建一个项目，也可以用之前的项目，添加监听器，在监听器界面点击浏览，选择该.jtl 文件，就可以查看结果了 JMeter 分布式安装 分布式环境：分为 server、client server 等同于 controller、master（server 其实也可以作为 Client 使用，但是不推荐这样做） client 等同于 agent、slave 注意事项 1、保持 controller 和 agent 机器的 JDK、jmeter 以及插件等配置版本一致； 2、如果测试数据有用到 CSV 或者其他方式进行参数化，需要将 data pools 在每台 Agent 上复制一份，且读取路径必须保持一致； 3、确保 controller 和 agent 机器在同一个子网里面； 4、检查防火墙是否被关闭，端口是否被占用（防火墙会影响脚本执行和测试结构收集，端口占用会导致 Agent 机报错）； 5、分布式测试中，通过远程启动代理服务器，默认查看结果树中的响应数据为空，只有错误信息会被报回； 6、如果并发较高，建议将 controller 机设置为只启动测试脚本和收集汇总测试结果，在配置文件里去掉 controller 机的 IP； 7、分布式测试中，如果 1S 启动 100 个模拟请求，有 5 个 Agent 机，那么需要将脚本的线程数设置为 20，否则模拟请求数会变成 500，和预期结果相差太大。 分布式测试流程： 运行所有 agent 机器上的 jmeter-server.bat 文件 假定我们使用两台机器 192.168.0.1 和 192.168.0.2 作为 agent 修改 controller 机器的 JMeter /bin/jmeter.properties 文件 默认值是： remote_hosts=127.0.0.1 修改为： remote_hosts=192.168.0.1:1099,192.168.0.2:1099 其中默认 RMI 端口是 1099，如果被占用，可以看 http://jmeter.apache.org/usermanual/remote-test.html 进行修改 启动 Controller 机器上的 JMeter.bat，进入 Run -> Remote Start JMeter 基础知识 线程组 Ramp-up Period（in seconds） 决定多长时间启动所有线程。如果使用 10 个线程，ramp-up period 是 100 秒，那么 JMeter 用 100 秒使所有 10 个线程启动并运行。每个线程会在上一个线程启动后 10 秒（100/10）启动。Ramp-up 需要要充足长以避免在启动测试时有一个太大的工作负载，并且要充足小以至于最后一个线程在第一个完成前启动。一般设置 ramp-up 等于线程数，有需求再根据该值进行调整。 一般不要设置为 0，不然 JMeter 一启动就会发送大量请求，服务器可能瞬间过载，测试不出平时那种因为平均访问带来的高负载情况。 估值方法：假设线程数为 100， 估计的点击率为每秒 10 次， 那么估计的理想 ramp-up period 就是 100/10 = 10 秒。每秒点击率需要自己获取系统数据，或是自己估值。 定时器 默认情况下，Jmeter 线程在发送请求之间没有间歇。建议为线程组添加某种定时器，以便设定请求之间应该隔多长时间。如果测试人员不设定这种延迟，Jmeter 可能会在短时间内产生大量访问请求，导致服务器被大量请求所淹没。 定时器会让作用域内的每一个采样器都在执行前等待一个固定时长。如果测试人员为线程组添加了多个定时器，那么 Jmeter 会将这些定时器的时长叠加起来，共同影响作用域范围内的采样器。定时器可以作为采样器或者逻辑控制器的子项，目的是只影响作用域内的采样器。 synchronized timer（时间集合点，主要用于模拟高并发测试） 该定时器主要是为了阻塞线程，直到指定的线程数量到达后，再一起释放，可以瞬间产生很大的压力。 假设这样的配置： Number of Simulated Users to Group By = 10 Timeout in milliseconds = 0 该配置表示当用户（线程）数达到 10 的时候才开始测试。 下面那个参数 0，表示达到 10 个用户就开始访问。如果填写 1000，则表示达到 10 用户后，延迟 1000 毫秒才开始访问。 Constant Throughput Timer（常数吞吐量定时器，主要用于预设好 QPS 场景下测试） constant throughput timer 常数吞吐量定时器可以让 jmeter 以指定数字的吞吐量（即指定 tps，只是这里要求指定每分钟的执行数，而不是每秒）执行。吞吐量计算的范围可以为指定为当前线程、当前线程组、所有线程组，并且计算吞吐量的依据可以是最近一次线程的执行时延。 聚合报告 Label：每个 JMeter 的 element（例如 HTTP Request）都有一个 Name 属性，这里显示的就是 Name 属性的值 Samples：表示你这次测试中一共发出了多少个请求，如果模拟 10 个用户，每个用户迭代 10 次，那么这里显示 100 Average：平均响应时间——默认情况下是单个 Request 的平均响应时间，当使用了 Transaction Controller 时，也可以以 Transaction 为单位显示平均响应时间（单位是毫秒） Median：中位数，也就是 50％ 用户的响应时间（单位是毫秒） 90% Line：90％ 用户的响应时间 Note：关于 50％ 和 90％ 并发用户数的含义，请参考下文 `http://www.cnblogs.com/jackei/archive/2006/11/11/557972.html Min：最小响应时间 Max：最大响应时间 Error%：本次测试中出现错误的请求的数量 / 请求的总数（怎么测试出整个系统的压力了? 如果 Error% 里面开始出现大量的错误，那就说明系统开始有瓶颈了，基本这时候就是最大压力节点，也就可以得到系统最大并发数是多少了。一般错误率不高于 1%，优秀的情况是不高于 0.01%）（若出现错误就要看服务端的日志，查找定位原因） Throughput：吞吐量——默认情况下表示每秒完成的请求数（Request per Second），当使用了 Transaction Controller 时，也可以表示类似 LoadRunner 的 Transaction per Second 数 KB/Sec：每秒从服务器端接收到的数据量，相当于 LoadRunner 中的 Throughput/Sec，主要看网络传输能力 常见问题 个人经验： 对开发机中的项目测试：一般 100 线程，循环 10 次即可。 对服务器上的小项目测试：一般 300 线程，循环 10 次即可。 对服务器上的中大型项目测试：采用分布式测试，分别测试：300 ~ 5000 线程情况。 假设好一点的机子设置 500 线程，一般的机子设置 300 线程。预计总 5000 线程需要 5 台好机子， 9 台一般机子。 也可以通过修改 JVM 方式来调整每台机子性能上限，修改 /bin/jmeter 的合适值: \"${HEAP:=\"-Xms1g -Xmx1g -XX:MaxMetaspaceSize=256m\"}\" JMeter 即使加了 cookie manage 也没有保存 session 的，一个原因是：HTTP 请求默认值，中服务器 IP，只能填 IP 或是域名，一定不要在这里面在加上某个后缀地址，这样没办法保存的 session 的 测试多用户的同时测试的效果，一种是用 jmeter 的 csv 功能，一种是复制几个脚本，每个脚本里面的登录信息不一样。或者是创建两个不同的线程组，每个线程组的登录信息不一样，然后每个线程组都有自己的 HTTP Cookie 管理器 在压力测试过程中，要监控服务器情况，可以使用 nmon 系统性能监控工具的使用 JMeter 资料 图文： 官网 User's Manual 快速学习Jmeter性能测试工具 jmeter：菜鸟入门到进阶系列 国内视频教程： JMeter 性能测试入门篇 - 慕课网 JMeter 之 HTTP 协议接口性能测试 - 慕课网 JMeter 性能测试进阶案例实战 - 慕课网 性能测试工具—Jmeter- 我要自学网 jmeter 视频教学课程 - 小强 国外视频教程： JMeter Beginner JMeter Advanced 资料 http://gitbook.cn/books/58de71a8be13fa66243873ef/index.html http://blog.51cto.com/ydhome/1862841 http://blog.51cto.com/ydhome/1869970 http://www.zhihu.com/question/22224874/answer/93890576 https://blog.csdn.net/musen518/article/details/50502302 http://www.ltesting.net/html/77/n-159277.html https://blog.csdn.net/github_27109687/article/details/71968662 http://jmeter.apache.org/usermanual/remote-test.html https://blog.csdn.net/qq_34021712/article/details/78682397 http://www.cnblogs.com/imyalost/p/8306866.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Docker-Install-And-Usage.html":{"url":"Linux-Tutorial/markdown-file/Docker-Install-And-Usage.html","title":"Docker 安装和使用","keywords":"","body":"Docker 使用 环境说明 CentOS 7.3（不准确地说：要求必须是 CentOS 7 64位） 不建议在 Windows 上使用 Docker 基本概念 官网：https://www.docker.com/ 宿主机：安装 Docker 的那台电脑 Docker：一个虚拟化软件，你可以认为是类似：VMware、Virtualbox 镜像：可以认为是类似 Windows 下的：XXXX.iso 容器：容器为镜像的实例，可以认为是 Virtualbox 运行 XXXX.iso 后的效果 官网的镜像仓库地址：https://store.docker.com/ 对开发来讲总结一个最简单的说法：在 Maven 未产生的年代，jar 包要随着开发项目走到哪里跟到哪里。有了 Maven 写好 pom.xml 即可。此时的 Docker 就好比如 Maven，帮你省去了开发过程中的部署环境差异，你再也不能随便说：你的系统可以运行，我的系统就不行。现在别人连系统都帮你做好了。 玩法理念：单进程，一个容器最好最专注去做一个事情。虽然它可以既装 MySQL，又装 Nginx 等等，但是让一个容器只做好一件事是最合适的。 其他通俗解释： Docker的思想来自于集装箱，集装箱解决了什么问题？在一艘大船上，可以把货物规整的摆放起来。并且各种各样的货物被集装箱标准化了，集装箱和集装箱之间不会互相影响。那么我就不需要专门运送水果的船和专门运送化学品的船了。只要这些货物在集装箱里封装的好好的，那我就可以用一艘大船把他们都运走。 docker就是类似的理念。现在都流行云计算了，云计算就好比大货轮。docker就是集装箱。 1.不同的应用程序可能会有不同的应用环境，比如.net开发的网站和php开发的网站依赖的软件就不一样，如果把他们依赖的软件都安装在一个服务器上就要调试很久，而且很麻烦，还会造成一些冲突。比如IIS和Apache访问端口冲突。这个时候你就要隔离.net开发的网站和php开发的网站。常规来讲，我们可以在服务器上创建不同的虚拟机在不同的虚拟机上放置不同的应用，但是虚拟机开销比较高。docker可以实现虚拟机隔离应用环境的功能，并且开销比虚拟机小，小就意味着省钱了。 2.你开发软件的时候用的是Ubuntu，但是运维管理的都是centos，运维在把你的软件从开发环境转移到生产环境的时候就会遇到一些Ubuntu转centos的问题，比如：有个特殊版本的数据库，只有Ubuntu支持，centos不支持，在转移的过程当中运维就得想办法解决这样的问题。这时候要是有docker你就可以把开发环境直接封装转移给运维，运维直接部署你给他的docker就可以了。而且部署速度快。 3.在服务器负载方面，如果你单独开一个虚拟机，那么虚拟机会占用空闲内存的，docker部署的话，这些内存就会利用起来。 总之docker就是集装箱原理。 Docker 的优点： 持续部署与测试 Docker在开发与运维的世界中具有极大的吸引力，因为它能保持跨环境的一致性。在开发与发布的生命周期中，不同的环境具有细微的不同，这些差异可能是由于不同安装包的版本和依赖关系引起的。然而，Docker可以通过确保从开发到产品发布整个过程环境的一致性来解决这个问题。 Docker容器通过相关配置，保持容器内部所有的配置和依赖关系始终不变。最终，你可以在开发到产品发布的整个过程中使用相同的容器来确保没有任何差异或者人工干预。 使用Docker，你还可以确保开发者不需要配置完全相同的产品环境，他们可以在他们自己的系统上通过VirtualBox建立虚拟机来运行Docker容器。Docker的魅力在于它同样可以让你在亚马逊EC2实例上运行相同的容器。如果你需要在一个产品发布周期中完成一次升级，你可以很容易地将需要变更的东西放到Docker容器中，测试它们，并且使你已经存在的容器执行相同的变更。这种灵活性就是使用Docker的一个主要好处。和标准部署与集成过程一样，Docker可以让你构建、测试和发布镜像，这个镜像可以跨多个服务器进行部署。哪怕安装一个新的安全补丁，整个过程也是一样的。你可以安装补丁，然后测试它，并且将这个补丁发布到产品中。 环境标准化和版本控制 Docker容器可以在不同的开发与产品发布生命周期中确保一致性，进而标准化你的环境。除此之外，Docker容器还可以像git仓库一样，可以让你提交变更到Docker镜像中并通过不同的版本来管理它们。设想如果你因为完成了一个组件的升级而导致你整个环境都损坏了，Docker可以让你轻松地回滚到这个镜像的前一个版本。这整个过程可以在几分钟内完成，如果和虚拟机的备份或者镜像创建流程对比，那Docker算相当快的，它可以让你快速地进行复制和实现冗余。此外，启动Docker就和运行一个进程一样快。 隔离性 Docker可以确保你的应用程序与资源是分隔开的。几个月前，Gartner发表了一篇报告，这份报告说明了运行Docker 容器进行资源隔离的效果和虚拟机（VM）管理程序一样的好，但是管理与控制方面还需要进行完善。我们考虑这样一个场景，你在你的虚拟机中运行了很多应用程序，这些应用程序包括团队协作软件（例如Confluence）、问题追踪软件（例如JIRA）、集中身份管理系统（例如Crowd）等等。由于这些软件运行在不同的端口上，所以你必须使用Apache或者Nginx来做反向代理。到目前为止，一切都很正常，但是随着你的环境向前推进，你需要在你现有的环境中配置一个内容管理系统（例如Alfresco）。这时候有个问题发生了，这个软件需要一个不同版本的Apache Tomcat，为了满足这个需求，你只能将你现有的软件迁移到另一个版本的Tomcat上，或者找到适合你现有Tomcat的内容管理系统（Alfresco）版本。对于上述场景，使用Docker就不用做这些事情了。Docker能够确保每个容器都拥有自己的资源，并且和其他容器是隔离的。你可以用不同的容器来运行使用不同堆栈的应用程序。除此之外，如果你想在服务器上直接删除一些应用程序是比较困难的，因为这样可能引发依赖关系冲突。而Docker可以帮你确保应用程序被完全清除，因为不同的应用程序运行在不同的容器上，如果你不在需要一款应用程序，那你可以简单地通过删除容器来删除这个应用程序，并且在你的宿主机操作系统上不会留下任何的临时文件或者配置文件。除了上述好处，Docker还能确保每个应用程序只使用分配给它的资源（包括CPU、内存和磁盘空间）。一个特殊的软件将不会使用你全部的可用资源，要不然这将导致性能降低，甚至让其他应用程序完全停止工作。 安全性 如上所述，Gartner也承认Docker正在快速地发展。从安全角度来看，Docker确保运行在容器中的应用程序和其他容器中的应用程序是完全分隔与隔离的，在通信流量和管理上赋予你完全的控制权。Docker容器不能窥视运行在其他容器中的进程。从体系结构角度来看，每个容器只使用着自己的资源（从进程到网络堆栈）。作为紧固安全的一种手段，Docker将宿主机操作系统上的敏感挂载点（例如/proc和/sys）作为只读挂载点，并且使用一种写时复制系统来确保容器不能读取其他容器的数据。Docker也限制了宿主机操作系统上的一些系统调用，并且和SELinux与AppArmor一起运行的很好。此外，在Docker Hub上可以使用的Docker镜像都通过数字签名来确保其可靠性。由于Docker容器是隔离的，并且资源是受限制的，所以即使你其中一个应用程序被黑，也不会影响运行在其它Docker容器上的应用程序。 多云平台 Docker最大的好处之一就是可移植性。在过去的几年里，所有主流的云计算提供商，包括亚马逊AWS和谷歌的GCP，都将Docker融入到他们的平台并增加了各自的支持。Docker容器能运行在亚马逊的EC2实例、谷歌的GCP实例、Rackspace服务器或者VirtualBox这些提供主机操作系统的平台上。举例来说，如果运行在亚马逊EC2实例上的Docker容器能够很容易地移植到其他几个平台上，比如说VirtualBox，并且达到类似的一致性和功能性，那这将允许你从基础设施层中抽象出来。除了AWS和GCP，Docker在其他不同的IaaS提供商也运行的非常好，例如微软的Azure、OpenStack和可以被具有不同配置的管理者所使用的Chef、Puppet、Ansible等。 Docker 安装和基本配置 主要有两个版本： Docker Enterprise Edition (Docker EE) is designed for enterprise development and IT teams who build, ship, and run business critical applications in production at scale. Docker EE is integrated, certified, and supported to provide enterprises with the most secure container platform in the industry to modernize all applications. For more information about Docker EE, including purchasing options, see Docker Enterprise Edition. Docker Community Edition (Docker CE) is ideal for developers and small teams looking to get started with Docker and experimenting with container-based apps. Docker CE is available on many platforms, from desktop to cloud to server. Docker CE is available for macOS and Windows and provides a native experience to help you focus on learning Docker. You can build and share containers and automate the development pipeline all from a single environment. Docker CE has both stable and edge channels. Stable builds are released once per quarter and are supported for 4 months. Edge builds are released once per month, and are supported for that month only. If you subscribe to the Edge channel on Linux distributions, you should also subscribe to the Stable channel. 官网总的安装手册：https://docs.docker.com/install/ 官网 CentOS 安装手册：https://docs.docker.com/install/linux/docker-ce/centos/ 目前也支持 Windows，特别是 Windows 10，直接官网一个安装包即可搞定。 Windows 10 的 Docker 安装说明：https://store.docker.com/editions/community/docker-ce-desktop-windows 我这里选择 Docker CE 版本： CentOS 安装过程： sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加 repo（可能网络会很慢，有时候会报：Timeout，所以要多试几次） sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast sudo yum install -y docker-ce，大小：19M，速度很慢。 查看配置文件位置：systemctl show --property=FragmentPath docker 启动 Docker：systemctl start docker.service 停止 Docker：systemctl stop docker.service 查看状态：systemctl status docker.service 运行 hello world 镜像：sudo docker run hello-world 因为是第一次使用，所以没这个镜像，需要一个下载过程，所以需要几分钟，可能还会报：Timeout。 镜像自动下载好后会输出这样一段内容，表示已经正常安装并可用了： ``` bash Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 78445dd45222: Pull complete Digest: sha256:c5515758d4c5e1e838e9cd307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: The Docker client contacted the Docker daemon. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/ For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ ``` 镜像加速 只要是外国的东西在国内基本都很难有好的速度，所有就有了加速器的概念，目前国内常用的如下： 阿里云：https://dev.aliyun.com/search.html USTC：https://lug.ustc.edu.cn/wiki/mirrors/help/docker daocloud：http://get.daocloud.io/ 网易：https://c.163.com 时速云：https://hub.tenxcloud.com/ 灵雀云：https://hub.alauda.cn/ 推荐优先阿里云，然后是 USTC 我下面的讲解也是基于阿里云加速 阿里云的服务需要注册账号，首次使用需要设置 docker 登录密码（阿里云叫做：修改Registry登录密码），这个以后用私人仓库会用到。 如果忘记了，后面可以在这里修改：https://cr.console.aliyun.com/#/imageList 注册后请访问：https://cr.console.aliyun.com/#/accelerator，你会看到专属的加速地址，比如我是：https://ldhc17y9.mirror.aliyuncs.com，所以下面文章你看到该地址都表示是这个专属地址，请记得自己更换自己的。 以及教你如何使用 Docker 加速器。如果你已经安装了最新版的 Docker 你就不需要用它的脚本进行安装了。 最新版本的 Docker 是新增配置文件：vim /etc/docker/daemon.json，增加如下内容： { \"registry-mirrors\": [\"https://ldhc17y9.mirror.aliyuncs.com\"] } sudo systemctl daemon-reload sudo systemctl restart docker 在以后的生活中如果要经常使用阿里云做为自己仓库，那你还需要做： 在 namespace管理 中创建属于你自己的 namespace：https://cr.console.aliyun.com/#/namespace/index 创建镜像仓库：https://cr.console.aliyun.com/#/imageList 创建好仓库后，点击：管理 进入查看仓库的更多详细信息，这里面有很多有用的信息，包括一个详细的操作指南，这份指南等下会用到。 比如我自己创建一个 redis-to-cluster 仓库，地址是阿里云给我们的：registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster 那我登录这个镜像地址的方式： docker login registry.cn-shenzhen.aliyuncs.com 会让我输入 Username：阿里云邮箱 password：上文提到的--Registry登录密码 然后在我的仓库管理地址有教我如何推送和拉取镜像：https://cr.console.aliyun.com/#/dockerImage/cn-shenzhen/youmeek/redis-to-cluster/detail 拉取：docker pull registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster:[镜像版本号] 推送： docker login docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster:[镜像版本号] docker push registry.cn-shenzhen.aliyuncs.com/youmeek/redis-to-cluster:[镜像版本号] Docker 命令，最终部署 Spring Boot 项目 建议：初期使用的时候尽量用容器 ID / 镜像 ID。如果使用 Tag/Name 在东西多的情况下很容易混乱 还不如就用记不住但是肯定唯一的容器 ID / 镜像 ID 重要的基本概念 可以代表一个完整的镜像名有两种方式： REPOSITORY(仓库):TAG(标签) 其中 TAG 表面上不是必须有的，本质是 docker 帮你用 latest 来代替了。latest 这里最好翻译为默认，而不是最新。 IMAGE ID(镜像ID) 这是一个 Docker 随机给你生成 数字+字母 的字符串 部署一个 Spring Boot 的 Java Web 项目为例 宿主机环境说明： IP 地址：http://192.168.137.128 停止了防火墙：systemctl stop firewalld.service ; systemctl stop iptables.service 停止防火墙后重启 Docker 服务：systemctl restart docker.service JDK（jdk-8u121-linux-x64.tar.gz）、jar 应用（spring-boot-my-demo.jar），存放宿主机位置：/opt/setups Spring Boot 的 jar 应用中配置文件给的端口是：8080，常规情况下的访问路径：http://127.0.0.1:8080/youmeek 下载镜像：docker pull centos:6.8，我的 IMAGE_ID 为：0cd976dc0a98 运行镜像，实例化为一个容器：docker run -i -t -v /opt/setups:/opt 0cd976dc0a98 /bin/bash -v：表示需要将本地宿主机的目录挂载到容器中对应的一个目录上，格式：-v :，所以此时对容器此目录的操作，也是等同于对宿主机的目录的操作 在容器里安装 Oracle JDK 8、配置 JDK 环境变量这里不多说，具体看：JDK 安装。 把容器中 /opt 目录下的 spring-boot-my-demo.jar 拷贝到容器的 root 目录下：cp /opt/spring-boot-my-demo.jar /root 再容器里新建脚本：vi /root/spring-boot-run.sh，脚本内容如下： #!/bin/bash source /etc/profile java -jar /root/spring-boot-my-demo.jar 在容器中对新建脚本增加执行权限：chmod u+x /root/spring-boot-run.sh 我们启动另外一个终端 查看我们刚刚运行的容器相关信息：docker ps -a 我们看到了我们刚刚运行的容器 ID（CONTAINER ID）为：a5d544d9b6f9，这个下面要用到 基于刚刚运行的容器创建新镜像：docker commit a5d544d9b6f9 youmeek/springboot:0.1 查看现在的镜像库：docker images，会发现多了一个 youmeek/springboot 新镜像，镜像 ID 为：7024f230fef9 运行新镜像，实例化为一个容器，并启动容器中刚刚写的脚本：docker run -d -p 38080:8080 --name=springBootJar --hostname=springBootJar 7024f230fef9 /root/spring-boot-run.sh -d：表示以“守护模式”执行 spring-boot-run.sh 脚本，此时 jar 中的 log 日志不会出现在输出终端上。 -p：表示宿主机与容器的端口映射，此时将容器内部的 8080 端口映射为宿主机的 38080 端口，这样就向外界暴露了 38080 端口，可通过 Docker 网桥来访问容器内部的 8080 端口了。 --name：表示给新实例容器取的名称，用一个有意义的名称命名即可 查看其实运行的容器：docker ps -a，可以知道我们的新容器 ID：fd21ac056343，名称为：springBootJar 查看这个容器的 jar 应用的 log 输出：docker logs -f fd21ac056343，可以看到 jar 启动后的 log 输出内容 通过浏览器访问容器中的应用：http://192.168.137.128:38080/youmeek/，可以看到 jar 应用的首页可以访问 Docker 基本命令 官网文档：https://docs.docker.com/engine/reference/run/ 版本信息 docker version，查看docker版本 docker info，显示docker系统的信息 镜像仓库 docker pull：从仓库下载镜像到本地 docker pull centos:latest：获取 CentOS 默认版本镜像 docker pull centos:7.3.1611：获取 CentOS 7 镜像，下载大小 70M 左右，下面的操作基于此镜像 docker pull centos:6.8：获取 CentOS 6 镜像 docker pull registry.cn-hangzhou.aliyuncs.com/chainone/centos7-jdk8：获取别人做好的阿里云镜像 docker push：将一个镜像 push 到 registry 仓库中 docker push myapache:v1 docker search：从 registry 仓库搜索镜像 docker search -s 3 centos，参数 -s 数字：表示筛选出收藏数（stars值）大于等于 3 的镜像 docker login：登录到一个镜像仓库。默认登录的是官网的仓库：https://hub.docker.com 登录阿里云仓库格式：sudo docker login --username=阿里云邮箱 比如我是这个：docker login --username=23333212@qq.com registry.cn-shenzhen.aliyuncs.com，你完整的登录地址你需要访问：https://cr.console.aliyun.com/#/imageList，在你自己创建的仓库中去查看那份详细操作指南上的地址 密码就是你首次访问：https://cr.console.aliyun.com/#/accelerator，弹窗出来让你设置的那个密码，如果忘记了重新设置下即可，重设地址：https://cr.console.aliyun.com/#/imageList，右上角有一个：修改docker登录密码。 本地镜像管理 docker stats：查看当前启动的容器各自占用的系统资源 bin docker stats --no-stream kafkadocker_kafka_1 kafkadocker_zookeeper_1：查看指定容器的占用资源情况 更加高级的监控方式有一个软件叫做：ctop（推荐使用）：https://github.com/bcicen/ctop CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 4532a9ee27b8 cloud-cadvisor 1.49% 53.28MiB / 3.702GiB 1.41% 13.5MB / 646MB 265MB / 0B 19 3895d5d50a5e kafkadocker_kafka_1 1.45% 1.24GiB / 3.702GiB 33.51% 145MB / 186MB 499MB / 724MB 128 1d1a6a7c48d8 kafkadocker_zookeeper_1 0.11% 70.85MiB / 3.702GiB 1.87% 55.8MB / 33.7MB 209MB / 1.22MB 23 docker images：显示本地所有的镜像列表 关注 REPOSITORY(名称)，TAG(标签)，IMAGE ID(镜像ID)三列 docker images centos：查看具体镜像情况 docker rmi：删除镜像，一般删除镜像前要先删除容器，不然如果镜像有被容器调用会报错 docker rmi 容器ID：删除具体某一个镜像 docker rmi 仓库:Tag：删除具体某一个镜像 docker rmi $(docker images -q)，删除所有镜像 docker rmi -f $(docker images -q)，强制删除所有镜像 docker rmi $(docker images | grep \"vmware\" | awk '{print $3}')，批量删除带有 vmware 名称的镜像 docker tag：为镜像打上标签 docker tag -f ubuntu:14.04 ubuntu:latest，-f 意思是强制覆盖 同一个IMAGE ID可能会有多个TAG（可能还在不同的仓库），首先你要根据这些 image names 来删除标签，当删除最后一个tag的时候就会自动删除镜像； docker rmi 仓库:Tag，取消标签（如果是镜像的最后一个标签，则会删除这个镜像） docker build：使用 Dockerfile 创建镜像（推荐） docker build . --rm -t runoob/ubuntu:v1，参数 -t，表示：-tag，打标签 多次 docker build 过程中是有依赖一个缓存的过程的，一般 build 过程都有好几个 step，Docker 非常聪明，会自己判断那些没有被修改过程的 step 采用缓存。如果想要避免使用缓存，可以使用这样命令 --no-cache：docker build --no-cache . --rm -t runoob/ubuntu:v1 docker history：显示生成一个镜像的历史命令，可以看出这个镜像的构建过程，包括：每一层镜像的 ID、指令 docker save：将一个镜像保存为一个 tar 包，带 layers 和 tag 信息（导出一个镜像） docker save 镜像ID -o /opt/test.tar docker load：从一个 tar 包创建一个镜像（导入一个镜像） docker load -i /opt/test.tar 容器生命周期管理 docker run，运行镜像 docker run -v /java_logs/:/opt/ -d -p 8080:80 --name=myDockerNameIsGitNavi --hostname=myDockerNameIsGitNavi -i -t 镜像ID /bin/bash -i -t 分别表示保证容器中的 STDIN 开启，并分配一个伪 tty 终端进行交互，这两个是合着用。 --name 是给容器起了一个名字（如果没有主动给名字，docker 会自动给你生成一个）容器的名称规则：大小写字母、数字、下划线、圆点、中横线，用正则表达式来表达就是：[a-zA-Z0-9_*-] -d 容器运行在后台。 -p 8080:80 表示端口映射，将宿主机的8080端口转发到容器内的80端口。（如果是 -P 参数，则表示随机映射应该端口，一般用在测试的时候） -v /java_logs/:/opt/ 表示目录挂载，/java_logs/ 是宿主机的目录，/opt/ 是容器目录 docker run --rm --name=myDockerNameIsGitNavi --hostname=myDockerNameIsGitNavi -i -t centos /bin/bash，--rm，表示退出即删除容器，一般用在做实验测试的时候 docker run --restart=always -i -t centos /bin/bash，--restart=always 表示停止后会自动重启 docker run --restart=on-failure:5 -i -t centos /bin/bash，--restart=on-failure:5 表示停止后会自动重启，最多重启 5 次 docker exec：对守护式的容器里面执行命令，方便对正在运行的容器进行维护、监控、管理 docker exec -i -t 容器ID /bin/bash，进入正在运行的 docker 容器，并启动终端交互 docker exec -d 容器ID touch /opt/test.txt，已守护式的方式进入 docker 容器，并创建一个文件 docker stop 容器ID，停止容器 docker stop $(docker ps -a -q)，停止所有容器 docker stop $(docker ps -a -q) ; docker rm $(docker ps -a -q)，停止所有容器，并删除所有容器 docker kill $(docker ps -q) ; docker rm $(docker ps -a -q)，停止所有容器，并删除所有容器 docker start 容器ID，重新启动已经停止的容器（重新启动，docker run 参数还是保留之前的） docker restart 容器ID，重启容器 docker rm，删除容器 docker rm 容器ID，删除指定容器（该容器必须是停止的） docker rm -f 容器ID，删除指定容器（该容器如果正在运行可以这样删除） docker rm $(docker ps -a -q)，删除所有容器 docker rm -f $(docker ps -a -q)，强制删除所有容器 docker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs docker rm 删除老的(一周前创建)容器 docker kill $(docker ps -q) ; docker rm $(docker ps -a -q) ; docker rmi $(docker images -q -a) 停止所有容器，删除所有容器，删除所有镜像 docker commit，把容器打成镜像 docker commit 容器ID gitnavi/docker-nodejs-test:0.1 gitnavi 是你注册的 https://store.docker.com/ 的名字，如果你没有的话，那需要先注册 docker-nodejs-test 是你为该镜像起的名字 0.1 是镜像的版本号，默认是 latest 版本 docker commit -m=\"这是一个描述信息\" --author=\"GitNavi\" 容器ID gitnavi/docker-nodejs-test:0.1 在提交镜像时指定更多的数据（包括标签）来详细描述所做的修改 docker diff 容器ID：显示容器文件系统的前后变化 --link 同一个宿主机下的不同容器的连接： docker run -it 镜像ID --link redis-name:myredis /bin/bash redis-name 是容器名称 myredis 是容器别名，其他容器连接它可以用这个别名来写入到自己的配置文件中 docker 网络模式 查看也有网络：docker network ls 创建网络：docker network create --subnet=172.19.0.0/16 net-redis-to-cluster 已有容器连接到某个网络（一个容器可以同时连上多个网络）：docker network connect net-redis-to-cluster my-redis-container 如果是内网提供服务的，可以直接创建一个网络，其服务使用该网络。然后另外一个需要调用该服务的，并且是对外网提供服务的可以使用 host 模式 --network XXXXXX 常见几种模式 bridge 默认模式，在 docker0 的网桥上创建新网络栈，确保独立的网络环境，实现网络隔离：docker run -it 镜像ID --network=bridge /bin/bash none 不适用网卡，不会有 IP，无法联网：docker run -it 镜像ID --network=none /bin/bash host 使用宿主机网络 IP、端口联网（在容器里面输入：ip a，看到的结果和在宿主机看到的一样）：docker run -it 镜像ID --network=host /bin/bash 自定义-使用自己命名的网络栈，但是需要手动配置网卡、IP 信息：docker run -it 镜像ID --network=自定义名称 /bin/bash 容器管理操作 docker ps：列出当前所有 正在运行 的容器 docker ps -a：列出所有的容器（包含历史，即运行过的容器） docker ps -l：列出最近一次启动的container docker ps -q：列出最近一次运行的container ID docker ps -a -l：列出最后一次运行的容器 docker ps -n x：显示最后 x 个容器，不管是正在运行或是已经停止的 docker top 容器ID：显示容器的进程信息 docker events：得到 docker 服务器的实时的事件 docker logs -f 容器ID：查看容器日志（如果一些容器不断重启，或是自动停止，可以这样看下） docker logs 容器ID，获取守护式容器的日志 docker logs -f 容器ID，不断监控容器日志，类似 tail -f docker logs -ft 容器ID，在 -f 的基础上又增加 -t 表示为每条日志加上时间戳，方便调试 docker logs --tail 10 容器ID，获取日志最后 10 行 docker logs --tail 0 -f 容器ID，跟踪某个容器的最新日志而不必读取日志文件 docker logs -f -t --since=\"2018-05-26\" --tail=200 容器ID 根据某个时间读取日志 docker logs -f -t --since=\"2018-05-26T11:13:40\" --tail=200 容器ID 根据某个时间读取日志 docker logs -f -t --since=\"2018-05-25T11:13:40\" --until \"2018-05-26T11:13:40\" --tail=200 容器ID 根据某个时间读取日志 docker logs --since 10m 容器ID 查看最近 10 分钟的日志 -f : 表示查看实时日志 -t : 显示时间戳 -since : 显示某个开始时间的所有日志 -tail=200 : 查看最后的 200 条日志 docker wait，阻塞到一个容器，直到容器停止运行 docker export，将容器整个文件系统导出为一个tar包，不带layers、tag等信息 docker port，显示容器的端口映射 docker inspect 容器ID：查看容器的全面信息，用 JSON 格式输出 docker inspect network名称：查看 network 信息，用 JSON 格式输出，包含使用该网络的容器有哪些 docker system df：类似于 Linux 上的 df 命令，用于查看 Docker 的磁盘使用情况 Images 镜像 Containers 容器 Local Volumes 数据卷 TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 6 6 1.049GB 0B (0%) Containers 7 4 10.25kB 0B (0%) Local Volumes 13 5 38.49GB 1.365MB (0%) Build Cache 0B 0B 获取容器中的 IP：docker inspect -f {{.NetworkSettings.IPAddress}} 容器ID 获取容器中的 IP：docker inspect -f {{.Volumes}} 容器ID 查看容器的挂载情况：docker inspect 容器ID | grep Mounts -A 10 下面为一个 docker inspect 后的结果示例： [ { \"Id\": \"e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b\", \"Created\": \"2018-01-18T03:47:16.138180181Z\", \"Path\": \"docker-entrypoint.sh\", \"Args\": [ \"--auth\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 19952, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2018-01-18T03:47:16.348568927Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:42aa46cfbd7a0d1101311defac39872b447b32295b40f9c99104ede5d02e9677\", \"ResolvConfPath\": \"/var/lib/docker/containers/e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b/hostname\", \"HostsPath\": \"/var/lib/docker/containers/e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b/hosts\", \"LogPath\": \"/var/lib/docker/containers/e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b/e1dff77b99d9c8489e0a0ce68a19ec5ffe18cc5d8b8ec17086f7f7bea29aa09b-json.log\", \"Name\": \"/cas-mongo\", \"RestartCount\": 0, \"Driver\": \"overlay\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": [ \"/data/mongo/db:/data/db\" ], \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": { \"27017/tcp\": [ { \"HostIp\": \"\", \"HostPort\": \"27017\" } ] }, \"RestartPolicy\": { \"Name\": \"always\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"shareable\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DiskQuota\": 0, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": 0, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0 }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay/0ab08b1f9c8f5f70cdcac2b01d9ee31de9e5a4955003567573635e8837931249/root\", \"MergedDir\": \"/var/lib/docker/overlay/4d6bb0d57f3f1b1dcf98c70b4bee4abf8dc110c7efa685ee5d84fe6f58c07b63/merged\", \"UpperDir\": \"/var/lib/docker/overlay/4d6bb0d57f3f1b1dcf98c70b4bee4abf8dc110c7efa685ee5d84fe6f58c07b63/upper\", \"WorkDir\": \"/var/lib/docker/overlay/4d6bb0d57f3f1b1dcf98c70b4bee4abf8dc110c7efa685ee5d84fe6f58c07b63/work\" }, \"Name\": \"overlay\" }, \"Mounts\": [ { \"Type\": \"volume\", \"Name\": \"6cd9721ff6a2768cd20e4a0678b176fa81a5de1c7d21fe6212b50c6854196db2\", \"Source\": \"/var/lib/docker/volumes/6cd9721ff6a2768cd20e4a0678b176fa81a5de1c7d21fe6212b50c6854196db2/_data\", \"Destination\": \"/data/configdb\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" }, { \"Type\": \"bind\", \"Source\": \"/data/mongo/db\", \"Destination\": \"/data/db\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" } ], \"Config\": { \"Hostname\": \"e1dff77b99d9\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": { \"27017/tcp\": {} }, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"GOSU_VERSION=1.7\", \"GPG_KEYS=0C49F3730359A14518585931BC711F9BA15703C6\", \"MONGO_PACKAGE=mongodb-org\", \"MONGO_REPO=repo.mongodb.org\", \"MONGO_MAJOR=3.4\", \"MONGO_VERSION=3.4.10\" ], \"Cmd\": [ \"--auth\" ], \"Image\": \"mongo:3.4\", \"Volumes\": { \"/data/configdb\": {}, \"/data/db\": {} }, \"WorkingDir\": \"\", \"Entrypoint\": [ \"docker-entrypoint.sh\" ], \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"7eabf418238f4d9f5fd5163fd4d173bbaea7764687a5cf40a9757d42b90ab2f9\", \"HairpinMode\": false, \"Link LocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": { \"27017/tcp\": [ { \"HostIp\": \"0.0.0.0\", \"HostPort\": \"27017\" } ] }, \"SandboxKey\": \"/var/run/docker/netns/7eabf418238f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"11c8d10a4be63b4ed710add6c440adf9d090b71918d4aaa837c46258e5425b80\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"ada97659acda146fc57e15a099e430a6e97de87f6d043b91d4c3582f6ab52d47\", \"EndpointID\": \"11c8d10a4be63b4ed710add6c440adf9d090b71918d4aaa837c46258e5425b80\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } } ] Docker 容器产生的 log 位置 Docker 运行一段时间，如果你的容器有大量的输出信息，则这个 log 文件会非常大，所以要考虑清理。 log 位置：/var/lib/docker/containers/容器ID值/容器ID值-json.log 可以考虑在停到容器的时候备份这个文件到其他位置，然后：echo > 容器ID值-json.log 当然，官网也提供了自动化的方案：https://docs.docker.com/config/containers/logging/json-file/ 修改 Docker 是配置文件：vim /etc/docker/daemon.json，（如果没有这个文件，自己新增）增加如下内容： { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"10m\", \"max-file\": \"5\" } } 如果你已经有该文件文件莱使用国内源，那修改方案应该是这样的： { \"registry-mirrors\": [\"https://ldhc17y9.mirror.aliyuncs.com\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"10m\", \"max-file\": \"5\" } } 删除 Docker 镜像中为 none 的镜像 Dockerfile 代码更新频繁，自然 docker build 构建同名镜像也频繁的很，产生了众多名为 none 的无用镜像 docker rmi $(docker images -f \"dangling=true\" -q) Docker daemon.json 可配置参数 https://docs.docker.com/engine/reference/commandline/dockerd/ Docker remote api 远程操作配置（保证在内网环境） 假设要被远程操作的服务器 IP：192.168.1.22 修改其配置文件：vim /lib/systemd/system/docker.service 修改默认值为：ExecStart=/usr/bin/dockerd 改为：ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 如果还需要连自己的 harbor 这类，完整配置：ExecStart=/usr/bin/dockerd --insecure-registry harbor.youmeek.com -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376 systemctl daemon-reload systemctl reload docker systemctl restart docker 验证： 在其他服务器上运行：docker -H 192.168.1.22:2376 images 能拿到和它本身看到的一样的数据表示可以了 Dockerfile 解释 该文件名就叫 Dockerfile，注意大小写，没有后缀，否则会报错。 主要由下面几个部分组成： 基础镜像信息 维护者/创建者信息 镜像操作指令 容器启动时执行执行 注释符号：# 这是一段注释说明 常用指令关键字： FROM，基础镜像信息 MAINTAINER，维护者/创建者信息 ADD，添加文件。如果添加的文件是类似 tar.gz 压缩包，会自动解压。 特别注意的是：ADD 文件到镜像的地址如果是目录，则需要最后保留斜杠，比如：ADD test.tar.gz /opt/shell/。不是斜杠结尾会认为是文件。 添加文件格式：ADD test.sh /opt/shell/test.sh 添加压缩包并解压格式：ADD test.tar.gz /opt/shell/，该压缩包会自动解压在 /opt/shell 目录下 COPY，类似 ADD，只是 COPY 只是复制文件，不会做类似解压压缩包这种行为。 COPY /opt/conf/ /etc/ 把宿主机的 /opt/conf 下文件复制到镜像的 /etc 目录下。 WORKDIR，设置工作目录，可以理解为类似 cd 命令，表示现在在某个目录路径，然后下面的 CMD、ENTRYPOINT 操作都是基于此目录 VOLUME，目录挂载 EXPOSE，暴露端口 USER，指定该镜像以什么用户去运行，也可以用这个来指定：docker run -u root。不指定默认是 root ENV，定义环境变量，该变量可以在后续的任何 RUN 指令中使用，使用方式：$HOME_DIR。在 docker run 的时候可以该方式来覆盖变量值 docker run -e “HOME_DIR=/opt” RUN，执行命令并创建新的镜像层，RUN 经常用于安装软件包 CMD，执行命令，并且一个 Dockerfile 只能有一条 CMD，有多条的情况下最后一条有效。在一种场景下 CMD 命令无效：docker run 的时候也指定了相同命令，则 docker run 命令优先级最高 ENTRYPOINT，配置容器启动时运行的命令，不会被 docker run 指令覆盖，并且 docker run 的指令可以作为参数传递到 ENTRYPOINT 中。要覆盖 ENTRYPOINT 命令也是有办法的：docker run --entrypoint 方式。Dockerfile 同时有 CMD 和 ENTRYPOINT 的时候，CMD 的指令是作为参数传递给 ENTRYPOINT 使用。 特别注意：RUN、CMD 和 ENTRYPOINT 这三个 Dockerfile 指令看上去很类似，很容易混淆。 最佳实战：来源 使用 RUN 指令安装应用和软件包，构建镜像。 如果 Docker 镜像的用途是运行应用程序或服务，比如运行一个 MySQL，应该优先使用 Exec 格式的 ENTRYPOINT 指令。CMD 可为 ENTRYPOINT 提供额外的默认参数，同时可利用 docker run 命令行替换默认参数。 如果想为容器设置默认的启动命令，可使用 CMD 指令。用户可在 docker run 命令行中替换此默认命令。 Dockerfile 部署 Spring Boot 应用 jar 名称：skb-user-0.0.1-SNAPSHOT.jar 打算用的宿主机端口：9096 Dockerfile 文件和 jar 文件存放在宿主机目录：/opt/zch Dockerfile 内容如下： FROM java:8-jre MAINTAINER gitnavi ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone ADD skb-user-0.0.1-SNAPSHOT.jar /usr/local/skb/user/ CMD [\"java\", \"-Xmx500m\", \"-jar\", \"/usr/local/skb/user/skb-user-0.0.1-SNAPSHOT.jar\", \"--spring.profiles.active=test\"] EXPOSE 9096 开始构建： cd /opt/zch docker build . --tag=\"skb/user:v1.0.1\" 因为 build 过程中会有多层镜像 step 过程，所以如果 build 过程中失败，那解决办法的思路是找到 step 失败的上一层，成功的 step 中镜像 ID。然后 docker run 该镜像 ID，手工操作，看报什么错误，然后就比较清晰得了解错误情况了。 docker run -d -p 9096:9096 -v /usr/local/logs/:/opt/ --name=skbUser --hostname=skbUser skb/user:v1.0.1 查看启动后容器列表：docker ps jar 应用的日志是输出在容器的 /opt 目录下，因为我们上面用了挂载，所在在我们宿主机的 /usr/local/logs 目录下可以看到输出的日志 防火墙开放端口： firewall-cmd --zone=public --add-port=9096/tcp --permanent firewall-cmd --reload 解释： # 是为了解决容器的时区和宿主机不一致问题 ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone Dockerfile 部署 Tomcat 应用 编写 Dockerfile FROM tomcat:8.0.46-jre8 MAINTAINER GitNavi ENV JAVA_OPTS=\"-Xms2g -Xmx2g -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=312M\" ENV CATALINA_HOME /usr/local/tomcat ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone RUN rm -rf /usr/local/tomcat/webapps/* ADD qiyeweixin.war /usr/local/tomcat/webapps/ EXPOSE 8080 CMD [\"catalina.sh\", \"run\"] 打包镜像：docker build -t harbor.gitnavi.com/demo/qiyeweixin:1.2.2 ./ 运行：docker run -d -p 8888:8080 --name=qiyeweixin --hostname=qiyeweixin -v /data/docker/logs/qiyeweixin:/data/logs/qiyeweixin harbor.gitnavi.com/demo/qiyeweixin:1.2.2 带 JVM 参数运行：docker run -d -p 8888:8080 -e JAVA_OPTS='-Xms7g -Xmx7g -XX:MetaspaceSize=128M -XX:MaxMetaspaceSize=512M' --name=qiyeweixin --hostname=qiyeweixin -v /data/docker/logs/qiyeweixin:/data/logs/qiyeweixin harbor.gitnavi.com/demo/qiyeweixin:1.2.2 虽然 Dockerfile 已经有 JVM 参数，并且也是有效的。但是如果 docker run 的时候又带了 JVM 参数，则会以 docker run 的参数为准 测试 JVM 是否有效方法，在代码里面书写，该值要接近 xmx 值： long maxMemory = Runtime.getRuntime().maxMemory(); logger.warn(\"-------------maxMemory=\" + ((double) maxMemory / (1024 * 1024))); Docker Compose Docker Compose 主要用于定义和运行多个 Docker 容器的工具，这样可以快速运行一套分布式系统 容器之间是有依赖关系，比如我一个 Java web 系统依赖 DB 容器、Redis 容器，必须这些依赖容器先运行起来。 一个文件：docker-compose.yml 一个命令：docker-compose up 官网安装说明：https://docs.docker.com/compose/install/#install-compose 安装方法： sudo curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 检查是否安装成功：docker-compose --version，输出：docker-compose version 1.18.0, build 8dd22a9 常用命令： 运行：docker-compose up -d 停止运行：docker-compose down 查看容器：docker-compose ps 删除停止的服务容器：docker-compose rm Docker Swarm Docker Swarm 是一个 Docker 集群管理工具 Harbor 镜像私有仓库 官网：http://vmware.github.io/harbor/ 资料 书籍：《第一本 Docker 书》 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Harbor-Install-And-Usage.html":{"url":"Linux-Tutorial/markdown-file/Harbor-Install-And-Usage.html","title":"Harbor 安装和配置","keywords":"","body":"Harbor 安装和配置 环境说明 CentOS 7.4 IP：192.168.0.105 需要访问的机子 hosts 需要映射（如果绑定真实域名就不需要这一步了）：192.168.0.105 harbor.gitnavi.com 直接用 IP 也是可以的，只是不用起来不美观 官方文档 安装指导：https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md 从中我们可以知道需要：Docker、Docker Compose 环境 硬件最低要求：2C + 4GB（推荐 8GB） 官网有推荐配置说明：hardware 下载：https://github.com/goharbor/harbor/releases 当前（201806）最新版本：v1.5.1 当前（201810）最新版本：v1.5.3 和 1.6.0 分 offline 和 online 版本，推荐使用 offline v1.5.1 下载地址：https://storage.googleapis.com/harbor-releases/release-1.5.0/harbor-offline-installer-v1.5.1.tgz v1.5.3 下载地址：https://storage.googleapis.com/harbor-releases/harbor-offline-installer-v1.5.3.tgz 安装 切换目录：cd /opt/setups 下载：wget https://storage.googleapis.com/harbor-releases/release-1.5.0/harbor-offline-installer-v1.5.1.tgz 解压：tar xvf harbor-offline-installer-v1.5.1.tgz 移动到 /usr 目录下：mv /opt/setups/harbor /usr/local 修改配置文件：vim /usr/local/harbor/harbor.cfg： _version = 1.5.0 # hostname 可以使用 ip、域名，不可以设置为 127.0.0.1 或 localhost hostname = harbor.gitnavi.com ui_url_protocol = http max_job_workers = 50 customize_crt = on ssl_cert = /data/cert/server.crt ssl_cert_key = /data/cert/server.key secretkey_path = /data admiral_url = NA log_rotate_count = 50 log_rotate_size = 200M http_proxy = https_proxy = no_proxy = 127.0.0.1,localhost,ui email_identity = email_server = smtp.mydomain.com email_server_port = 25 email_username = sample_admin@mydomain.com email_password = abc email_from = admin email_ssl = false email_insecure = false # 启动Harbor后，管理员UI登录的密码，默认是 Harbor12345，用户名默认是：admin harbor_admin_password = Harbor12345 auth_mode = db_auth ldap_url = ldaps://ldap.mydomain.com ldap_basedn = ou=people,dc=mydomain,dc=com ldap_uid = uid ldap_scope = 2 ldap_timeout = 5 ldap_verify_cert = true ldap_group_basedn = ou=group,dc=mydomain,dc=com ldap_group_filter = objectclass=group ldap_group_gid = cn ldap_group_scope = 2 self_registration = on token_expiration = 30 project_creation_restriction = everyone db_host = mysql db_password = root123 db_port = 3306 db_user = root redis_url = redis:6379 clair_db_host = postgres clair_db_password = password clair_db_port = 5432 clair_db_username = postgres clair_db = postgres uaa_endpoint = uaa.mydomain.org uaa_clientid = id uaa_clientsecret = secret uaa_verify_cert = true uaa_ca_cert = /path/to/ca.pem registry_storage_provider_name = filesystem registry_storage_provider_config = 安装成功会占用这些端口，所以请先做好准备，如果不想使用下面的端口需要修改：vim /usr/local/harbor/docker-compose.yml 80 6379 3306 5000 1514 后面重新启动 Harbor 也靠这个文件了：docker-compose -f /usr/local/harbor/docker-compose.yml restart 开始安装：sh /usr/local/harbor/install.sh，控制台输出如下（预计需要 5 ~ 10 分钟）： [Step 0]: checking installation environment ... Note: docker version: 17.12.0 Note: docker-compose version: 1.18.0 [Step 1]: loading Harbor images ... 52ef9064d2e4: Loading layer [==================================================>] 135.9MB/135.9MB 4a6862dbadda: Loading layer [==================================================>] 23.25MB/23.25MB 58b7d0c522b2: Loading layer [==================================================>] 24.4MB/24.4MB 9cd4bb748634: Loading layer [==================================================>] 7.168kB/7.168kB c81302a14908: Loading layer [==================================================>] 10.56MB/10.56MB 7848e9ba72a3: Loading layer [==================================================>] 24.39MB/24.39MB Loaded image: vmware/harbor-ui:v1.5.1 f1691b5a5198: Loading layer [==================================================>] 73.15MB/73.15MB a529013c99e4: Loading layer [==================================================>] 3.584kB/3.584kB d9b4853cff8b: Loading layer [==================================================>] 3.072kB/3.072kB 3d305073979e: Loading layer [==================================================>] 4.096kB/4.096kB c9e17074f54a: Loading layer [==================================================>] 3.584kB/3.584kB 956055840e30: Loading layer [==================================================>] 9.728kB/9.728kB Loaded image: vmware/harbor-log:v1.5.1 185db06a02d0: Loading layer [==================================================>] 23.25MB/23.25MB 835213979c70: Loading layer [==================================================>] 20.9MB/20.9MB f74eeb41c1c9: Loading layer [==================================================>] 20.9MB/20.9MB Loaded image: vmware/harbor-jobservice:v1.5.1 9bd5c7468774: Loading layer [==================================================>] 23.25MB/23.25MB 5fa6889b9a6d: Loading layer [==================================================>] 2.56kB/2.56kB bd3ac235b209: Loading layer [==================================================>] 2.56kB/2.56kB cb5d493833cc: Loading layer [==================================================>] 2.048kB/2.048kB 557669a074de: Loading layer [==================================================>] 22.8MB/22.8MB f02b4f30a9ac: Loading layer [==================================================>] 22.8MB/22.8MB Loaded image: vmware/registry-photon:v2.6.2-v1.5.1 5d3b562db23e: Loading layer [==================================================>] 23.25MB/23.25MB 8edca1b0e3b0: Loading layer [==================================================>] 12.16MB/12.16MB ce5f11ea46c0: Loading layer [==================================================>] 17.3MB/17.3MB 93750d7ec363: Loading layer [==================================================>] 15.87kB/15.87kB 36f81937e80d: Loading layer [==================================================>] 3.072kB/3.072kB 37e5df92b624: Loading layer [==================================================>] 29.46MB/29.46MB Loaded image: vmware/notary-server-photon:v0.5.1-v1.5.1 0a2f8f90bd3a: Loading layer [==================================================>] 401.3MB/401.3MB 41fca4deb6bf: Loading layer [==================================================>] 9.216kB/9.216kB f2e28262e760: Loading layer [==================================================>] 9.216kB/9.216kB 68677196e356: Loading layer [==================================================>] 7.68kB/7.68kB 2b006714574e: Loading layer [==================================================>] 1.536kB/1.536kB Loaded image: vmware/mariadb-photon:v1.5.1 a8c4992c632e: Loading layer [==================================================>] 156.3MB/156.3MB 0f37bf842677: Loading layer [==================================================>] 10.75MB/10.75MB 9f34c0cd38bf: Loading layer [==================================================>] 2.048kB/2.048kB 91ca17ca7e16: Loading layer [==================================================>] 48.13kB/48.13kB 5a7e0da65127: Loading layer [==================================================>] 10.8MB/10.8MB Loaded image: vmware/clair-photon:v2.0.1-v1.5.1 0e782fe069e7: Loading layer [==================================================>] 23.25MB/23.25MB 67fc1e2f7009: Loading layer [==================================================>] 15.36MB/15.36MB 8db2141aa82c: Loading layer [==================================================>] 15.36MB/15.36MB Loaded image: vmware/harbor-adminserver:v1.5.1 3f87a34f553c: Loading layer [==================================================>] 4.772MB/4.772MB Loaded image: vmware/nginx-photon:v1.5.1 Loaded image: vmware/photon:1.0 ad58f3ddcb1b: Loading layer [==================================================>] 10.95MB/10.95MB 9b50f12509bf: Loading layer [==================================================>] 17.3MB/17.3MB 2c21090fd212: Loading layer [==================================================>] 15.87kB/15.87kB 38bec864f23e: Loading layer [==================================================>] 3.072kB/3.072kB 6e81ea7b0fa6: Loading layer [==================================================>] 28.24MB/28.24MB Loaded image: vmware/notary-signer-photon:v0.5.1-v1.5.1 897a26fa09cb: Loading layer [==================================================>] 95.02MB/95.02MB 16e3a10a21ba: Loading layer [==================================================>] 6.656kB/6.656kB 85ecac164331: Loading layer [==================================================>] 2.048kB/2.048kB 37a2fb188706: Loading layer [==================================================>] 7.68kB/7.68kB Loaded image: vmware/postgresql-photon:v1.5.1 bed9f52be1d1: Loading layer [==================================================>] 11.78kB/11.78kB d731f2986f6e: Loading layer [==================================================>] 2.56kB/2.56kB c3fde9a69f96: Loading layer [==================================================>] 3.072kB/3.072kB Loaded image: vmware/harbor-db:v1.5.1 7844feb13ef3: Loading layer [==================================================>] 78.68MB/78.68MB de0fd8aae388: Loading layer [==================================================>] 3.072kB/3.072kB 3f79efb720fd: Loading layer [==================================================>] 59.9kB/59.9kB 1c02f801c2e8: Loading layer [==================================================>] 61.95kB/61.95kB Loaded image: vmware/redis-photon:v1.5.1 454c81edbd3b: Loading layer [==================================================>] 135.2MB/135.2MB e99db1275091: Loading layer [==================================================>] 395.4MB/395.4MB 051e4ee23882: Loading layer [==================================================>] 9.216kB/9.216kB 6cca4437b6f6: Loading layer [==================================================>] 9.216kB/9.216kB 1d48fc08c8bc: Loading layer [==================================================>] 7.68kB/7.68kB 0419724fd942: Loading layer [==================================================>] 1.536kB/1.536kB 543c0c1ee18d: Loading layer [==================================================>] 655.2MB/655.2MB 4190aa7e89b8: Loading layer [==================================================>] 103.9kB/103.9kB Loaded image: vmware/harbor-migrator:v1.5.0 [Step 2]: preparing environment ... Generated and saved secret to file: /data/secretkey Generated configuration file: ./common/config/nginx/nginx.conf Generated configuration file: ./common/config/adminserver/env Generated configuration file: ./common/config/ui/env Generated configuration file: ./common/config/registry/config.yml Generated configuration file: ./common/config/db/env Generated configuration file: ./common/config/jobservice/env Generated configuration file: ./common/config/jobservice/config.yml Generated configuration file: ./common/config/log/logrotate.conf Generated configuration file: ./common/config/jobservice/config.yml Generated configuration file: ./common/config/ui/app.conf Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt The configuration files are ready, please use docker-compose to start the service. Creating harbor-log ... done [Step 3]: checking existing instance of Harbor ... Creating registry ... done Creating harbor-ui ... done Creating network \"harbor_harbor\" with the default driver Creating nginx ... done Creating registry ... Creating harbor-adminserver ... Creating harbor-db ... Creating redis ... Creating harbor-ui ... Creating harbor-jobservice ... Creating nginx ... ✔ ----Harbor has been installed and started successfully.---- Now you should be able to visit the admin portal at http://harbor.gitnavi.com. For more details, please visit https://github.com/vmware/harbor . 安装成功后，可以访问：http://harbor.gitnavi.com 默认用户名：admin 默认密码：Harbor12345 docker 客户端默认是使用 https 访问 docker registry，我们默认在安装 Harbor 的时候配置文件用的时候 http，所以其他 docker 客户端需要修改 vim /lib/systemd/system/docker.service 修改默认值为：ExecStart=/usr/bin/dockerd 改为：ExecStart=/usr/bin/dockerd --insecure-registry harbor.gitnavi.com systemctl daemon-reload systemctl reload docker systemctl restart docker 访问：http://harbor.gitnavi.com/harbor/projects，创建一个项目，比如：youmeek，等下需要用到。 这里用 admin 用户，不再另外创建用了，但是实际使用最好新建用户。 docker login -u admin -p Harbor12345 harbor.gitnavi.com 给本地的一个 maven 镜像打 tag：docker tag maven:3.3-jdk-8 harbor.gitnavi.com/youmeek/harbor-maven:3.3-jdk-8 push 到仓库：docker push 182.61.19.178/demo/springboot-jenkins-docker:3 harbor.cfg 默认值 ## Configuration file of Harbor #This attribute is for migrator to detect the version of the .cfg file, DO NOT MODIFY! _version = 1.5.0 #The IP address or hostname to access admin UI and registry service. #DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients. hostname = reg.mydomain.com #The protocol for accessing the UI and token/notification service, by default it is http. #It can be set to https if ssl is enabled on nginx. ui_url_protocol = http #Maximum number of job workers in job service max_job_workers = 50 #Determine whether or not to generate certificate for the registry's token. #If the value is on, the prepare script creates new root cert and private key #for generating token to access the registry. If the value is off the default key/cert will be used. #This flag also controls the creation of the notary signer's cert. customize_crt = on #The path of cert and key files for nginx, they are applied only the protocol is set to https ssl_cert = /data/cert/server.crt ssl_cert_key = /data/cert/server.key #The path of secretkey storage secretkey_path = /data #Admiral's url, comment this attribute, or set its value to NA when Harbor is standalone admiral_url = NA #Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated. log_rotate_count = 50 #Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes. #If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G #are all valid. log_rotate_size = 200M #Config http proxy for Clair, e.g. http://my.proxy.com:3128 #Clair doesn't need to connect to harbor ui container via http proxy. http_proxy = https_proxy = no_proxy = 127.0.0.1,localhost,ui #NOTES: The properties between BEGIN INITIAL PROPERTIES and END INITIAL PROPERTIES #only take effect in the first boot, the subsequent changes of these properties #should be performed on web ui #************************BEGIN INITIAL PROPERTIES************************ #Email account settings for sending out password resetting emails. #Email server uses the given username and password to authenticate on TLS connections to host and act as identity. #Identity left blank to act as username. email_identity = email_server = smtp.mydomain.com email_server_port = 25 email_username = sample_admin@mydomain.com email_password = abc email_from = admin email_ssl = false email_insecure = false ##The initial password of Harbor admin, only works for the first time when Harbor starts. #It has no effect after the first launch of Harbor. #Change the admin password from UI after launching Harbor. harbor_admin_password = Harbor12345 ##By default the auth mode is db_auth, i.e. the credentials are stored in a local database. #Set it to ldap_auth if you want to verify a user's credentials against an LDAP server. auth_mode = db_auth #The url for an ldap endpoint. ldap_url = ldaps://ldap.mydomain.com #A user's DN who has the permission to search the LDAP/AD server. #If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd. #ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com #the password of the ldap_searchdn #ldap_search_pwd = password #The base DN from which to look up a user in LDAP/AD ldap_basedn = ou=people,dc=mydomain,dc=com #Search filter for LDAP/AD, make sure the syntax of the filter is correct. #ldap_filter = (objectClass=person) # The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/AD ldap_uid = uid #the scope to search for users, 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREE ldap_scope = 2 #Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds. ldap_timeout = 5 #Verify certificate from LDAP server ldap_verify_cert = true #The base dn from which to lookup a group in LDAP/AD ldap_group_basedn = ou=group,dc=mydomain,dc=com #filter to search LDAP/AD group ldap_group_filter = objectclass=group #The attribute used to name a LDAP/AD group, it could be cn, name ldap_group_gid = cn #The scope to search for ldap groups. 0-LDAP_SCOPE_BASE, 1-LDAP_SCOPE_ONELEVEL, 2-LDAP_SCOPE_SUBTREE ldap_group_scope = 2 #Turn on or off the self-registration feature self_registration = on #The expiration time (in minute) of token created by token service, default is 30 minutes token_expiration = 30 #The flag to control what users have permission to create projects #The default value \"everyone\" allows everyone to creates a project. #Set to \"adminonly\" so that only admin user can create project. project_creation_restriction = everyone #************************END INITIAL PROPERTIES************************ #######Harbor DB configuration section####### #The address of the Harbor database. Only need to change when using external db. db_host = mysql #The password for the root user of Harbor DB. Change this before any production use. db_password = root123 #The port of Harbor database host db_port = 3306 #The user name of Harbor database db_user = root ##### End of Harbor DB configuration####### #The redis server address. Only needed in HA installation. #address:port[,weight,password,db_index] redis_url = redis:6379 ##########Clair DB configuration############ #Clair DB host address. Only change it when using an exteral DB. clair_db_host = postgres #The password of the Clair's postgres database. Only effective when Harbor is deployed with Clair. #Please update it before deployment. Subsequent update will cause Clair's API server and Harbor unable to access Clair's database. clair_db_password = password #Clair DB connect port clair_db_port = 5432 #Clair DB username clair_db_username = postgres #Clair default database clair_db = postgres ##########End of Clair DB configuration############ #The following attributes only need to be set when auth mode is uaa_auth uaa_endpoint = uaa.mydomain.org uaa_clientid = id uaa_clientsecret = secret uaa_verify_cert = true uaa_ca_cert = /path/to/ca.pem ### Docker Registry setting ### #registry_storage_provider can be: filesystem, s3, gcs, azure, etc. registry_storage_provider_name = filesystem #registry_storage_provider_config is a comma separated \"key: value\" pairs, e.g. \"key1: value, key2: value2\". #Refer to https://docs.docker.com/registry/configuration/#storage for all available configuration. registry_storage_provider_config = 资料 https://www.ilanni.com/?p=13492 https://blog.csdn.net/aixiaoyang168/article/details/73549898 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/LDAP-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/LDAP-Install-And-Settings.html","title":"LDAP 安装和使用","keywords":"","body":"LDAP 安装和配置 LDAP 基本概念 https://segmentfault.com/a/1190000002607140 http://www.itdadao.com/articles/c15a1348510p0.html http://blog.csdn.net/reblue520/article/details/51804162 LDAP 服务器端安装 环境：CentOS 7.3 x64（为了方便，已经禁用了防火墙） 常见服务端： OpenLDAP 这里选择：OpenLDAP，安装（最新的是 2.4.40）：yum install -y openldap openldap-clients openldap-servers migrationtools 配置： cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG chown ldap. /var/lib/ldap/DB_CONFIG 启动： systemctl start slapd systemctl enable slapd 查看占用端口（默认用的是 389）：netstat -tlnp | grep slapd 设置OpenLDAP管理员密码 输入命令：slappasswd，重复输入两次明文密码后（我是：123456），我得到一个加密后密码（后面会用到）：{SSHA}YK8qBtlmEpjUiVEPyfmNNDALjBaUTasc 新建临时配置目录： mkdir /root/my_ldif ; cd /root/my_ldif vim chrootpw.ldif，添加如下内容： dn: olcDatabase={0}config,cn=config changetype: modify add: olcRootPW olcRootPW: {SSHA}YK8qBtlmEpjUiVEPyfmNNDALjBaUTasc 添加刚刚写的配置（过程比较慢）：ldapadd -Y EXTERNAL -H ldapi:/// -f chrootpw.ldif 导入默认的基础配置（过程比较慢）：for i in /etc/openldap/schema/*.ldif; do ldapadd -Y EXTERNAL -H ldapi:/// -f $i; done 修改默认的domain 输入命令：slappasswd，重复输入两次明文密码后（我是：111111），我得到一个加密后密码（后面会用到）：{SSHA}rNLkIMYKvYhbBjxLzSbjVsJnZSkrfC3w cd /root/my_ldif ; vim chdomain.ldif，添加如下内容（cn，dc，dc，olcRootPW 几个值需要你自己改）： dn: olcDatabase={1}monitor,cn=config changetype: modify replace: olcAccess olcAccess: {0}to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" read by dn.base=\"cn=gitnavi,dc=youmeek,dc=com\" read by * none dn: olcDatabase={2}hdb,cn=config changetype: modify replace: olcSuffix olcSuffix: dc=youmeek,dc=com dn: olcDatabase={2}hdb,cn=config changetype: modify replace: olcRootDN olcRootDN: cn=gitnavi,dc=youmeek,dc=com dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcRootPW olcRootPW: {SSHA}rNLkIMYKvYhbBjxLzSbjVsJnZSkrfC3w dn: olcDatabase={2}hdb,cn=config changetype: modify add: olcAccess olcAccess: {0}to attrs=userPassword,shadowLastChange by dn=\"cn=gitnavi,dc=youmeek,dc=com\" write by anonymous auth by self write by * none olcAccess: {1}to dn.base=\"\" by * read olcAccess: {2}to * by dn=\"cn=gitnavi,dc=youmeek,dc=com\" write by * read 添加配置：ldapadd -Y EXTERNAL -H ldapi:/// -f chdomain.ldif 添加一个基本的目录 cd /root/my_ldif ; vim basedomain.ldif，添加如下内容（cn，dc，dc 几个值需要你自己改）： dn: dc=youmeek,dc=com objectClass: top objectClass: dcObject objectclass: organization o: youmeek dot Com dc: youmeek dn: cn=gitnavi,dc=youmeek,dc=com objectClass: organizationalRole cn: gitnavi description: Directory Manager dn: ou=People,dc=youmeek,dc=com objectClass: organizationalUnit ou: People dn: ou=Group,dc=youmeek,dc=com objectClass: organizationalUnit ou: Group 添加配置：ldapadd -x -D cn=gitnavi,dc=youmeek,dc=com -W -f basedomain.ldif，会提示让你输入配置 domain 的密码，我是：111111 简单的配置到此就好了 测试连接 重启下服务：systemctl restart slapd 本机测试，输入命令：ldapsearch -LLL -W -x -D \"cn=gitnavi,dc=youmeek,dc=com\" -H ldap://localhost -b \"dc=youmeek,dc=com\"，输入 domain 密码，可以查询到相应信息 局域网客户端连接测试，下载 Ldap Admin（下载地址看文章下面），具体连接信息看下图： LDAP 客户端 常见客户端： LdapAdmin LdapBrowser phpLDAPadmin Softerra LDAP Administrator 资料 https://superlc320.gitbooks.io/samba-ldap-centos7/ldap_+_centos_7.html http://yhz61010.iteye.com/blog/2352672 https://kyligence.gitbooks.io/kap-manual/zh-cn/security/ldap.cn.html http://gaowenlong.blog.51cto.com/451336/1887408 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Alfresco-Install-And-Usage.html":{"url":"Linux-Tutorial/markdown-file/Alfresco-Install-And-Usage.html","title":"Alfresco 安装和使用","keywords":"","body":"Alfresco 使用 Alfresco 介绍 官网：https://www.alfresco.com 开源社区版下载：https://www.alfresco.com/alfresco-community-download 官网文档说明：http://docs.alfresco.com/ 开源社区版本安装说明：http://docs.alfresco.com/community/concepts/master-ch-install.html Linux 版本安装说明：http://docs.alfresco.com/community/tasks/simpleinstall-community-lin.html 我的目的：用来管理公司发布的文档，比如：Word、Excel、记事本等这类，该系统支持在线预览文件。 环境说明 CentOS 7.3 带有图形桌面（需要有） 下载下来的软件：alfresco-community-installer-201704-linux-x64_3.bin，大小：822M 安装包存放目录：/opt 开始安装： 使用 Linux 桌面方式进入系统，打开终端： 切换目录：cd /opt 赋予权限：chmod +x alfresco-community-installer-201704-linux-x64_3.bin ./alfresco-community-installer-201704-linux-x64_3.bin 然后会出现如下图这种步骤引导安装界面，带有简体中文版，选择 简单 的基本流程安装方式，默认都是下一步下一步。其实选择 高级 也基本上是下一步下一步的，只是可以看到更多细节。 在设置管理员密码上，我是：123456（默认用户名是：admin） 安装完成后，可以选择开始启动程序。建议不勾选，我们来使用下面命令。 程序默认是安装在：/opt/alfresco-community 所以我们需要打开终端： 切换目录：cd /opt/alfresco-community 启动程序：./alfresco.sh start 停止程序：./alfresco.sh stop 启动完成后，用浏览器打开：http://127.0.0.1:8080/share（如果你 Linux 防火墙关闭了，也可以直接用其他机子直接访问，把这台机子当做服务器用），可以看到如下内容： Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:04:16 "},"Linux-Tutorial/markdown-file/Thrift-Install-And-Usage.html":{"url":"Linux-Tutorial/markdown-file/Thrift-Install-And-Usage.html","title":"Apache Thrift 安装和使用","keywords":"","body":"Apache Thrift 安装和配置 官网下载：https://thrift.apache.org/download Apache 综合下载地（可以下载到旧版本）：http://archive.apache.org/dist/thrift/ 一般我们建议下载 tar.gz 自己编译安装 安装所需的依赖等条件说明：https://thrift.apache.org/docs/install/ 源码安装方式说明：https://thrift.apache.org/docs/BuildingFromSource 现在最新的版本是：0.10.0，而我选的版本是：0.9.3，因为我目前各个语言大家用的最多的还是 0.9 系列的版本 源码包安装方法 下载 0.9.3 的源码包：http://archive.apache.org/dist/thrift/0.9.3/thrift-0.9.3.tar.gz 安装依赖：yum install -y autoconf automake libtool cmake ncurses-devel openssl-devel lzo-devel zlib-devel gcc gcc-c++ 安装总流程： tar zxvf thrift-0.9.3.tar.gz cd thrift-0.9.3 ./configure --without-tests make make install 安装成功后，看下版本：thrift --version Docker 方式直接使用 拉取镜像：docker pull thrift:0.9.3 现在假设我们有一个 service.thrift 的配置文件，放在 /opt 目录下：/opt/service.thrift 现在我们要根据这个配置文件生成 java 类文件，把这些类 java 类生成到目录：/opt/thrift/data 这个需要可以使用下面命令： docker run -v \"$PWD:/opt\" thrift:0.9.3 thrift -o /opt/thrift/data --gen java /opt/service.thrift 如果你没有 service.thrift 文件也无所谓，我们现在就来创建一个（里面的内容懂不懂都无所谓）： namespace java com.gitnavi.service struct User { 1: i32 id 2: string username } service UserService { User getUser() } Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Node-Install-And-Usage.html":{"url":"Linux-Tutorial/markdown-file/Node-Install-And-Usage.html","title":"Node.js 安装和使用","keywords":"","body":"Node 安装和配置 Node 安装 官网：https://nodejs.org 官网下载：https://nodejs.org/zh-cn/download/ 历史版本下载：https://nodejs.org/zh-cn/download/releases/ 此时（20171212） Maven 最新版本为：8.9.3 (includes npm 5.5.1) 官网安装教程：https://nodejs.org/en/download/package-manager/ 官网 CentOS 系统下的安装教程：https://nodejs.org/en/download/package-manager/#enterprise-linux-and-fedora 官网文档复制过来就是： 如果你是要安装 node 8 系列，下载这个 yum 源 curl --silent --location https://rpm.nodesource.com/setup_8.x | sudo bash - 如果你是要安装 node 9 系列，下载这个 yum 源 curl --silent --location https://rpm.nodesource.com/setup_9.x | sudo bash - 然后通过 yum 开始安装（软件大小：51M 左右） sudo yum -y install nodejs 验证：node -v 注意:因为网络原因，最好先把脚本下载到本地，再用代理进行安装 nrm 快速切换 NPM 源 安装：npm install -g nrm 列表源：nrm ls 使用源：nrm use taobao 更多使用方法：https://segmentfault.com/a/1190000000473869 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/YApi-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/YApi-Install-And-Settings.html","title":"YApi 安装和配置","keywords":"","body":"YApi 安装和配置 部署的环境 系统：CentOS 7.4 硬件要求：1 GB RAM minimum ip：http://192.168.1.121 docker version：17.12.1-ce, build 7390fc6 docker-compose version：1.18.0, build 8dd22a9 建议部署成 http 站点，因 chrome 浏览器安全限制，部署成 https 会导致测试功能在请求 http 站点时文件上传功能异常。--来源 Docker 快速部署 一个好心人的维护：https://github.com/branchzero/yapi-docker 使用方法： work path：mkdir -p /opt/git-data clone：cd /opt/git-data && git clone https://github.com/branchzero/yapi-docker.git permission：chmod -R 777 /opt/git-data run command：cd /opt/git-data/yapi-docker && docker-compose up -d open chrome：http://192.168.1.121:3000 初始化管理员账号名：admin@admin.com，密码：ymfe.org YApi 介绍 官网：https://yapi.ymfe.org/index.html Github：https://github.com/YMFE/yapi 官网在线演示：http://yapi.demo.qunar.com/ 使用手册：https://yapi.ymfe.org/usage.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Kafka-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Kafka-Install-And-Settings.html","title":"Kafka 安装和配置","keywords":"","body":"Kafka 安装和配置 对于版本 由于 Kafka 经常会被连接到各个地方去，所以对于 Kafka 的版本，一般不能用太新的，要看你用在什么地方。 Flink 的要求 Spark 的要求 Spring 的要求 消息系统的好处 解耦（各个业务系统各自为政，有各自新需求，各自系统自行修改，只通过消息来通信） 大系统层面的扩展性（不用改旧业务系统代码，增加新系统，接收新消息） 异步通信（一个消息，多个业务系统来消费。某些场景可以堆积到一定程度再去消费） 缓冲（解耦某些需要长时间处理业务） Kafka 介绍 A distributed streaming platform 官网：https://kafka.apache.org/ Github：https://github.com/apache/kafka 主要是由 Java 和 Scala 开发 官网下载：https://kafka.apache.org/downloads 当前最新稳定版本（201803）：1.0.1 官网 quickstart：https://kafka.apache.org/quickstart 运行的机子不要小于 2G 内存 Kafka 流行的主要原因： 支持常见的发布订阅功能 分布式 高吞吐量（听说：普通单机也支持每秒 100000 条消息的传输） 磁盘数据持久化，消费者 down 后，重新 up 的时候可以继续接收前面未接收到的消息 支持流数据处理，常见于大数据 核心概念： Producer：生产者（业务系统），负责发布消息到 broker Consumer：消费者（业务系统），向 broker 读取消息的客户端 Broker：可以理解为：存放消息的管道（kafka 软件节点本身） Topic：可以理解为：消息主题、消息标签、消息通道、消息队列（物理上不同 Topic 的消息分开存储，根据 Partition 参数决定一个 Topic 的消息保存于一个或多个 broker 上。作为使用者，不用关心 Topic 实际物理存储地方。） Partition：是物理上的概念，每个 Topic 包含一个或多个 Partition。一般有几个 Broker，填写分区最好是等于大于节点值。分区目的主要是数据分片，解决水平扩展、高吞吐量。当 Producer 生产消息的时候，消息会被算法计算后分配到对应的分区，Consumer 读取的时候算法也会帮我们找到消息所在分区，这是内部实现的，应用层面不用管。 Replication-factor：副本。假设有 3 个 Broker 的情况下，当副本为 3 的时候每个 Partition 会在每个 Broker 都会存有一份，目的主要是容错。 其中有一个 Leader。 如果你只有一个 Broker，但是创建 Topic 的时候指定 Replication-factor 为 3，则会报错 Consumer Group：每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）一般一个业务系统集群指定同一个一个 group id，然后一个业务系统集群只能一个节点来消费同一个消息。 Consumer Group 信息存储在 zookeeper 中，需要通过 zookeeper 的客户端来查看和设置 如果某 Consumer Group 中 consumer 数量少于 partition 数量，则至少有一个 consumer 会消费多个 partition 的数据 如果 consumer 的数量与 partition 数量相同，则正好一个 consumer 消费一个 partition 的数据 如果 consumer 的数量多于 partition 的数量时，会有部分 consumer 无法消费该 topic 下任何一条消息。 具体实验可以看这篇文章：Kafka深度解析 Record：消息数据本身，由一个 key、value、timestamp 组成 业界常用的 docker 镜像： wurstmeister/kafka-docker（不断更新，优先） Spring 项目选用依赖包的时候，对于版本之间的关系可以看这里：http://projects.spring.io/spring-kafka/ 目前（201803） spring boot 2.0 以上基础框架版本，kafka 版本 1.0.x，推荐使用：spring-kafka 2.1.4.RELEASE spring boot 2.0 以下基础框架版本，kafka 版本 0.11.0.x, 1.0.x，推荐使用：spring-kafka 1.3.3.RELEASE 官网 quickstart 指导：https://kafka.apache.org/quickstart 常用命令： wurstmeister/kafka-docker 容器中 kafka home：cd /opt/kafka 假设我的 zookeeper 地址：10.135.157.34:2181，如果你有多个节点用逗号隔开 列出所有 topic：bin/kafka-topics.sh --list --zookeeper 10.135.157.34:2181 创建 topic：bin/kafka-topics.sh --create --topic kafka-test-topic-1 --partitions 3 --replication-factor 1 --zookeeper 10.135.157.34:2181 创建名为 kafka-test-topic-1 的 topic，3个分区分别存放数据，数据备份总共 2 份 查看特定 topic 的详情：bin/kafka-topics.sh --describe --topic kafka-test-topic-1 --zookeeper 10.135.157.34:2181 删除 topic：bin/kafka-topics.sh --delete --topic kafka-test-topic-1 --zookeeper 10.135.157.34:2181 更多命令可以看：http://orchome.com/454 假设 topic 详情的返回信息如下： PartitionCount:6：分区为 6 个 ReplicationFactor:3：副本为 3 个 Partition: 0 Leader: 3：Partition 下标为 0 的主节点是 broker.id=3 当 Leader down 掉之后，其他节点会选举中一个新 Leader Replicas: 3,1,2：在 Partition: 0 下共有 3 个副本，broker.id 分别为 3,1,2 Isr: 3,1,2：在 Partition: 0 下目前存活的 broker.id 分别为 3,1,2 Topic:kafka-all PartitionCount:6 ReplicationFactor:3 Configs: Topic: kafka-all Partition: 0 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: kafka-all Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: kafka-all Partition: 2 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: kafka-all Partition: 3 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 Topic: kafka-all Partition: 4 Leader: 1 Replicas: 1,3,2 Isr: 1,3,2 Topic: kafka-all Partition: 5 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Docker 单个实例部署（1.0.1） 目前 latest 用的时候 kafka 1.0.1，要指定版本可以去作者 github 看下 tag 目录，切换不同 tag，然后看下 Dockerfile 里面的 kafka 版本号 我的服务器外网 ip：182.61.19.177，hostname 为：instance-3v0pbt5d 在我的开发机上上配置 host： 182.61.19.177 instance-3v0pbt5d 部署 kafka： 目前 latest 用的时候 kafka 1.0.1，要指定版本可以去作者 github 看下 tag 目录，切换不同 tag，然后看下 Dockerfile 里面的 kafka 版本号 新建文件：vim docker-compose.yml 这里的 kafka 对外网暴露端口是 9094，内网端口是 9092 version: '3.2' services: zookeeper: image: wurstmeister/zookeeper ports: - \"2181:2181\" kafka: image: wurstmeister/kafka:latest ports: - target: 9094 published: 9094 protocol: tcp mode: host environment: HOSTNAME_COMMAND: \"docker info | grep ^Name: | cut -d' ' -f 2\" KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT KAFKA_ADVERTISED_PROTOCOL_NAME: OUTSIDE KAFKA_ADVERTISED_PORT: 9094 KAFKA_PROTOCOL_NAME: INSIDE KAFKA_PORT: 9092 KAFKA_LOG_DIRS: /data/docker/kafka/logs KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true' KAFKA_LOG_RETENTION_HOURS: 168 volumes: - /var/run/docker.sock:/var/run/docker.sock - /data/docker/kafka/logs:/data/docker/kafka/logs 启动：docker-compose up -d 停止：docker-compose stop 测试： 进入 kafka 容器：docker exec -it kafkadocker_kafka_1 /bin/bash 根据官网 Dockerfile 说明，kafka home 应该是：cd /opt/kafka 创建 topic 命令：bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic my-topic-test 查看 topic 命令：bin/kafka-topics.sh --list --zookeeper zookeeper:2181 删除 topic：bin/kafka-topics.sh --delete --topic my-topic-test --zookeeper zookeeper:2181 给 topic 发送消息命令：bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic-test，然后在出现交互输入框的时候输入你要发送的内容 再开一个终端，进入 kafka 容器，接受消息：bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic-test --from-beginning 其中 --from-beginning 参数表示在启动该客户端的时候接受前面 kafka 的所有记录。不加这个参数，则旧数据不会收到，生产者新生产的消息才会接收到。 此时发送的终端输入一个内容回车，接受消息的终端就可以收到。 Docker 多机多实例部署（外网无法访问） 三台机子： 内网 ip：172.24.165.129，外网 ip：47.91.22.116 内网 ip：172.24.165.130，外网 ip：47.91.22.124 内网 ip：172.24.165.131，外网 ip：47.74.6.138 修改三台机子 hostname： 节点 1：hostnamectl --static set-hostname youmeekhost1 节点 2：hostnamectl --static set-hostname youmeekhost2 节点 3：hostnamectl --static set-hostname youmeekhost3 三台机子的 hosts 都修改为如下内容：vim /etc/hosts 172.24.165.129 youmeekhost1 172.24.165.130 youmeekhost2 172.24.165.131 youmeekhost3 开发机设置 hosts： 47.91.22.116 youmeekhost1 47.91.22.124 youmeekhost2 47.74.6.138 youmeekhost3 Zookeeper 集群 节点 1： docker run -d --name=zookeeper1 --net=host --restart=always \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -v /etc/hosts:/etc/hosts \\ -e ZOO_MY_ID=1 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ zookeeper:latest 节点 2： docker run -d --name=zookeeper2 --net=host --restart=always \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -v /etc/hosts:/etc/hosts \\ -e ZOO_MY_ID=2 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ zookeeper:latest 节点 3： docker run -d --name=zookeeper3 --net=host --restart=always \\ -v /data/docker/zookeeper/data:/data \\ -v /data/docker/zookeeper/log:/datalog \\ -v /etc/hosts:/etc/hosts \\ -e ZOO_MY_ID=3 \\ -e \"ZOO_SERVERS=server.1=youmeekhost1:2888:3888 server.2=youmeekhost2:2888:3888 server.3=youmeekhost3:2888:3888\" \\ zookeeper:latest 先安装 nc 再来校验 zookeeper 集群情况 环境：CentOS 7.4 官网下载：https://nmap.org/download.html，找到 rpm 包 当前时间（201803）最新版本下载：wget https://nmap.org/dist/ncat-7.60-1.x86_64.rpm 安装并 ln：sudo rpm -i ncat-7.60-1.x86_64.rpm && ln -s /usr/bin/ncat /usr/bin/nc 检验：nc --version zookeeper 集群测试 节点 1 执行命令：echo stat | nc youmeekhost1 2181，能得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.31.154.16:35336[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x0 Mode: follower Node count: 4 节点 2 执行命令：echo stat | nc youmeekhost2 2181，能得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.31.154.17:55236[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x100000000 Mode: leader Node count: 4 节点 3 执行命令：echo stat | nc youmeekhost3 2181，能得到如下信息： Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMT Clients: /172.31.65.88:41840[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x100000000 Mode: follower Node count: 4 Kafka 集群 节点 1 执行： docker run -d --net=host --name=kafka1 \\ --restart=always \\ --env KAFKA_BROKER_ID=1 \\ --env KAFKA_ZOOKEEPER_CONNECT=youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181 \\ --env KAFKA_LOG_DIRS=/data/docker/kafka/logs \\ --env HOSTNAME_COMMAND=\"docker info | grep ^Name: | cut -d' ' -f 2\" \\ --env KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT \\ --env KAFKA_ADVERTISED_PROTOCOL_NAME=OUTSIDE \\ --env KAFKA_ADVERTISED_PORT=9094 \\ --env KAFKA_PROTOCOL_NAME=INSIDE \\ --env KAFKA_PORT=9092 \\ --env KAFKA_AUTO_CREATE_TOPICS_ENABLE=true \\ --env KAFKA_LOG_RETENTION_HOURS=168 \\ --env KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime \\ -v /data/docker/kafka/logs:/data/docker/kafka/logs \\ -v /etc/hosts:/etc/hosts \\ wurstmeister/kafka:latest 节点 2 执行： docker run -d --net=host --name=kafka2 \\ --restart=always \\ --env KAFKA_BROKER_ID=2 \\ --env KAFKA_ZOOKEEPER_CONNECT=youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181 \\ --env KAFKA_LOG_DIRS=/data/docker/kafka/logs \\ --env HOSTNAME_COMMAND=\"docker info | grep ^Name: | cut -d' ' -f 2\" \\ --env KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT \\ --env KAFKA_ADVERTISED_PROTOCOL_NAME=OUTSIDE \\ --env KAFKA_ADVERTISED_PORT=9094 \\ --env KAFKA_PROTOCOL_NAME=INSIDE \\ --env KAFKA_PORT=9092 \\ --env KAFKA_AUTO_CREATE_TOPICS_ENABLE=true \\ --env KAFKA_LOG_RETENTION_HOURS=168 \\ --env KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime \\ -v /data/docker/kafka/logs:/data/docker/kafka/logs \\ -v /etc/hosts:/etc/hosts \\ wurstmeister/kafka:latest 节点 3 执行： docker run -d --net=host --name=kafka3 \\ --restart=always \\ --env KAFKA_BROKER_ID=3 \\ --env KAFKA_ZOOKEEPER_CONNECT=youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181 \\ --env KAFKA_LOG_DIRS=/data/docker/kafka/logs \\ --env HOSTNAME_COMMAND=\"docker info | grep ^Name: | cut -d' ' -f 2\" \\ --env KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT \\ --env KAFKA_ADVERTISED_PROTOCOL_NAME=OUTSIDE \\ --env KAFKA_ADVERTISED_PORT=9094 \\ --env KAFKA_PROTOCOL_NAME=INSIDE \\ --env KAFKA_PORT=9092 \\ --env KAFKA_AUTO_CREATE_TOPICS_ENABLE=true \\ --env KAFKA_LOG_RETENTION_HOURS=168 \\ --env KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime \\ -v /data/docker/kafka/logs:/data/docker/kafka/logs \\ -v /etc/hosts:/etc/hosts \\ wurstmeister/kafka:latest Kafka 集群测试 在 kafka1 上测试： 进入 kafka1 容器：docker exec -it kafka1 /bin/bash 根据官网 Dockerfile 说明，kafka home 应该是：cd /opt/kafka 创建 topic 命令：bin/kafka-topics.sh --create --zookeeper youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181 --replication-factor 3 --partitions 3 --topic my-topic-test 查看 topic 命令：bin/kafka-topics.sh --list --zookeeper youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181 给 topic 发送消息命令：bin/kafka-console-producer.sh --broker-list youmeekhost1:9092 --topic my-topic-test，然后在出现交互输入框的时候输入你要发送的内容 在 kafka2 上测试： 进入 kafka2 容器：docker exec -it kafka2 /bin/bash 接受消息：cd /opt/kafka && bin/kafka-console-consumer.sh --bootstrap-server youmeekhost2:9092 --topic my-topic-test --from-beginning 在 kafka3 上测试： 进入 kafka3 容器：docker exec -it kafka3 /bin/bash 接受消息：cd /opt/kafka && bin/kafka-console-consumer.sh --bootstrap-server youmeekhost3:9092 --topic my-topic-test --from-beginning 如果 kafka1 输入的消息，kafka2 和 kafka3 能收到，则表示已经成功。 Kafka 认证配置 可以参考：Kafka的SASL/PLAIN认证配置说明 Kafka 单纯监控 KafkaOffsetMonitor Github 官网：https://github.com/quantifind/KafkaOffsetMonitor README 带了下载地址和运行命令 只是已经很久不更新了 部署 kafka-manager Github 官网：https://github.com/yahoo/kafka-manager 注意官网说明的版本支持 节点 1（没成功）：docker run -d --name=kafka-manager1 --restart=always -p 9000:9000 -e ZK_HOSTS=\"youmeekhost1:2181,youmeekhost2:2181,youmeekhost3:2181\" sheepkiller/kafka-manager:latest 源码类安装可以看：Kafka监控工具—Kafka Manager Kafka manager 是一款管理 + 监控的工具，比较重 Kafka 1.0.1 源码安装（也支持 1.0.2、0.11.0.3、0.10.2.2） 测试环境：2G 内存足够 一台机子：CentOS 7.4，根据文章最开头，已经修改了 hosts 确保本机安装有 JDK8（JDK 版本不能随便挑选） 先用上面的 docker 方式部署一个 zookeeper，我这里的 zookeeper IP 地址为：172.16.0.2 如果该 zookeeper 前面已经用过了，最好重新删除，重新 run，因为 zookeeper 上保留的旧的 topic 配置 官网下载：https://kafka.apache.org/downloads 当前（201803）最新版本为：1.0.1，同时推荐 Scala 版本为 2.11，这里要特别注意：kafka_2.11-1.0.1.tgz 中的 2.11 指的是 Scala 版本 找到：Binary downloads 下面的链接 下载：wget http://mirrors.shu.edu.cn/apache/kafka/1.0.1/kafka_2.11-1.0.1.tgz 解压：tar zxvf kafka_2.11-1.0.1.tgz，假设当前目录为：/usr/local/kafka_2.11-1.0.1 为了方便，修改目录名字：mv /usr/local/kafka_2.11-1.0.1 /usr/local/kafka 创建 log 输出目录：mkdir -p /data/kafka/logs 修改 kafka-server 的配置文件：vim /usr/local/kafka/config/server.properties 找到下面两个参数内容，修改成如下： # 唯一ID（kafka 集群环境下，该值必须唯一，默认从 0 开始），和 zookeeper 的配置文件中的 myid 类似道理（单节点多 broker 的情况下该参数必改） broker.id=1 # 监听地址（单节点多 broker 的情况下该参数必改） listeners=PLAINTEXT://0.0.0.0:9092 # 向 Zookeeper 注册的地址。这里可以直接填写外网IP地址，但是不建议这样做，而是通过配置 hosts 的方式来设置。不然填写外网 IP 地址会导致所有流量都走外网（单节点多 broker 的情况下该参数必改） advertised.listeners=PLAINTEXT://youmeekhost:9092 # zookeeper，存储了 broker 的元信息 zookeeper.connect=youmeekhost:2181 # 日志数据目录，可以通过逗号来指定多个目录（单节点多 broker 的情况下该参数必改） log.dirs=/data/kafka/logs # 创建新 topic 的时候默认 1 个分区。需要特别注意的是：已经创建好的 topic 的 partition 的个数只可以被增加，不能被减少。 # 如果对消息有高吞吐量的要求，可以增加分区数来分摊压力 num.partitions=1 # 允许删除topic delete.topic.enable=false # 允许自动创建topic（默认是 true） auto.create.topics.enable=true # 磁盘IO不足的时候，可以适当调大该值 ( 当内存足够时 ) #log.flush.interval.messages=10000 #log.flush.interval.ms=1000 # kafka 数据保留时间 默认 168 小时 == 7 天 log.retention.hours=168 # 其余都使用默认配置，但是顺便解释下： # borker 进行网络处理的线程数 num.network.threads=3 # borker 进行 I/O 处理的线程数 num.io.threads=8 # 发送缓冲区 buffer 大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能 socket.send.buffer.bytes=102400 # 接收缓冲区大小，当数据到达一定大小后在序列化到磁盘 socket.receive.buffer.bytes=102400 # 这个参数是向 kafka 请求消息或者向 kafka 发送消息的请请求的最大数，这个值不能超过 java 的堆栈大小 socket.request.max.bytes=104857600 启动 kafka 服务（必须制定配置文件）：cd /usr/local/kafka && bin/kafka-server-start.sh config/server.properties 后台方式运行 kafka 服务：cd /usr/local/kafka && bin/kafka-server-start.sh -daemon config/server.properties 停止 kafka 服务：cd /usr/local/kafka && bin/kafka-server-stop.sh 再开一个终端测试： 创建 topic 命令：cd /usr/local/kafka && bin/kafka-topics.sh --create --zookeeper youmeekhost:2181 --replication-factor 1 --partitions 1 --topic my-topic-test 查看 topic 命令：cd /usr/local/kafka && bin/kafka-topics.sh --list --zookeeper youmeekhost:2181 删除 topic：cd /usr/local/kafka && bin/kafka-topics.sh --delete --topic my-topic-test --zookeeper youmeekhost:2181 给 topic 发送消息命令：cd /usr/local/kafka && bin/kafka-console-producer.sh --broker-list youmeekhost:9092 --topic my-topic-test，然后在出现交互输入框的时候输入你要发送的内容 再开一个终端，进入 kafka 容器，接受消息：cd /usr/local/kafka && bin/kafka-console-consumer.sh --bootstrap-server youmeekhost:9092 --topic my-topic-test --from-beginning 此时发送的终端输入一个内容回车，接受消息的终端就可以收到。 Spring Boot 依赖： org.springframework.kafka spring-kafka 1.3.3.RELEASE org.apache.kafka kafka-clients 1.0.1 org.apache.kafka kafka-streams 1.0.1 项目配置文件：bootstrap-servers 地址：instance-3v0pbt5d:9092（这里端口是 9092 别弄错了） kafka 1.0.1 默认配置文件内容 # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 #listeners=PLAINTEXT://:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for \"listeners\" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). #advertised.listeners=PLAINTEXT://your.host.name:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma seperated list of directories under which to store log files log.dirs=/tmp/kafka-logs # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics \"__consumer_offsets\" and \"__transaction_state\" # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log unless the remaining # segments drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=localhost:2181 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 其他资料 管理Kafka的Consumer-Group信息 Kafka--Consumer消费者 http://www.ituring.com.cn/article/499268 http://orchome.com/kafka/index https://www.jianshu.com/p/263164fdcac7 https://www.cnblogs.com/wangxiaoqiangs/p/7831990.html http://www.bijishequ.com/detail/536308 http://lanxinglan.cn/2017/10/18/%E5%9C%A8Docker%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2Kafka/ https://www.cnblogs.com/ding2016/p/8282907.html http://blog.csdn.net/fuyuwei2015/article/details/73379055 https://segmentfault.com/a/1190000012990954 http://www.54tianzhisheng.cn/2018/01/04/Kafka/ https://renwole.com/archives/442 http://www.bijishequ.com/detail/542646?p=85 http://blog.csdn.net/zhbr_f1/article/details/73732299 http://wangzs.leanote.com/post/kafka-manager%E5%AE%89%E8%A3%85 https://cloud.tencent.com/developer/article/1013313 http://blog.csdn.net/boling_cavalry/article/details/78309050 https://www.jianshu.com/p/d77149efa59f http://www.bijishequ.com/detail/536308 http://blog.51cto.com/13323775/2063420 http://lanxinglan.cn/2017/10/18/%E5%9C%A8Docker%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%83%A8%E7%BD%B2Kafka/ http://www.cnblogs.com/huxi2b/p/7929690.html http://blog.csdn.net/HG_Harvey/article/details/79198496 http://blog.csdn.net/vtopqx/article/details/78638996 http://www.weduoo.com/archives/2047 https://blog.52itstyle.com/archives/2358/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Hadoop-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Hadoop-Install-And-Settings.html","title":"Hadoop 安装和配置","keywords":"","body":"Hadoop 安装和配置 Hadoop 说明 Hadoop 官网：https://hadoop.apache.org/ Hadoop 官网下载：https://hadoop.apache.org/releases.html 基础环境 学习机器 2C4G（生产最少 8G）： 172.16.0.17 172.16.0.43 172.16.0.180 操作系统：CentOS 7.5 root 用户 所有机子必备：Java：1.8 确保：echo $JAVA_HOME 能查看到路径，并记下来路径 Hadoop:2.6.5 关闭所有机子的防火墙：systemctl stop firewalld.service 集群环境设置 Hadoop 集群具体来说包含两个集群：HDFS 集群和 YARN 集群，两者逻辑上分离，但物理上常在一起 HDFS 集群：负责海量数据的存储，集群中的角色主要有 NameNode / DataNode YARN 集群：负责海量数据运算时的资源调度，集群中的角色主要有 ResourceManager /NodeManager HDFS 采用 master/worker 架构。一个 HDFS 集群是由一个 Namenode 和一定数目的 Datanodes 组成。Namenode 是一个中心服务器，负责管理文件系统的命名空间 (namespace) 以及客户端对文件的访问。集群中的 Datanode 一般是一个节点一个，负责管理它所在节点上的存储。 分别给三台机子设置 hostname hostnamectl --static set-hostname linux01 hostnamectl --static set-hostname linux02 hostnamectl --static set-hostname linux03 修改 hosts 就按这个来，其他多余的别加，不然可能也会有影响 vim /etc/hosts 172.16.0.17 linux01 172.16.0.43 linux02 172.16.0.180 linux03 对 linux01 设置免密： 生产密钥对 ssh-keygen -t rsa 公钥内容写入 authorized_keys cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys 测试： ssh localhost 将公钥复制到两台 slave 如果你是采用 pem 登录的，可以看这个：SSH 免密登录 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@172.16.0.43，根据提示输入 linux02 机器的 root 密码，成功会有相应提示 ssh-copy-id -i ~/.ssh/id_rsa.pub -p 22 root@172.16.0.180，根据提示输入 linux03 机器的 root 密码，成功会有相应提示 在 linux01 上测试： ssh linux02 ssh linux03 Hadoop 安装 关于版本这件事，主要看你的技术生态圈。如果你的其他技术，比如 Spark，Flink 等不支持最新版，则就只能向下考虑。 我这里技术栈，目前只能到：2.6.5，所以下面的内容都是基于 2.6.5 版本 官网说明：https://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/ClusterSetup.html 分别在三台机子上都创建目录： mkdir -p /data/hadoop/hdfs/name /data/hadoop/hdfs/data /data/hadoop/hdfs/tmp 下载 Hadoop：http://apache.claz.org/hadoop/common/hadoop-2.6.5/ 现在 linux01 机子上安装 cd /usr/local && wget http://apache.claz.org/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz tar zxvf hadoop-2.6.5.tar.gz，有 191M 左右 给三台机子都先设置 HADOOP_HOME 会 ansible playbook 会方便点：Ansible 安装和配置 vim /etc/profile export HADOOP_HOME=/usr/local/hadoop-2.6.5 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 修改 linux01 配置 修改 JAVA_HOME vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh 把 25 行的 export JAVA_HOME=${JAVA_HOME} 都改为 export JAVA_HOME=/usr/local/jdk1.8.0_191 vim $HADOOP_HOME/etc/hadoop/yarn-env.sh 文件开头加一行 export JAVA_HOME=/usr/local/jdk1.8.0_191 hadoop.tmp.dir == 指定hadoop运行时产生文件的存储目录 vim $HADOOP_HOME/etc/hadoop/core-site.xml，改为： hadoop.tmp.dir file:/data/hadoop/hdfs/tmp io.file.buffer.size 131072 fs.default.name hdfs://linux01:9000 --> fs.defaultFS hdfs://linux01:9000 hadoop.proxyuser.root.hosts * hadoop.proxyuser.root.groups * 配置包括副本数量 最大值是 datanode 的个数 数据存放目录 vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml dfs.replication 2 dfs.namenode.name.dir file:/data/hadoop/hdfs/name true dfs.datanode.data.dir file:/data/hadoop/hdfs/data true dfs.webhdfs.enabled true dfs.permissions false 设置 YARN 新创建：vim $HADOOP_HOME/etc/hadoop/mapred-site.xml mapreduce.framework.name yarn mapreduce.map.memory.mb 4096 mapreduce.reduce.memory.mb 8192 mapreduce.map.java.opts -Xmx3072m mapreduce.reduce.java.opts -Xmx6144m yarn.resourcemanager.hostname == 指定YARN的老大（ResourceManager）的地址 yarn.nodemanager.aux-services == NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序默认值：\"\" 32G 内存的情况下配置： vim $HADOOP_HOME/etc/hadoop/yarn-site.xml yarn.resourcemanager.hostname linux01 yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.vmem-pmem-ratio 2.1 yarn.nodemanager.resource.memory-mb 20480 yarn.scheduler.minimum-allocation-mb 2048 配置 slave 相关信息 vim $HADOOP_HOME/etc/hadoop/slaves 把默认的配置里面的 localhost 删除，换成： linux02 linux03 scp -r /usr/local/hadoop-2.6.5 root@linux02:/usr/local/ scp -r /usr/local/hadoop-2.6.5 root@linux03:/usr/local/ linux01 机子运行 格式化 HDFS hdfs namenode -format 输出结果： [root@linux01 hadoop-2.6.5]# hdfs namenode -format 18/12/17 17:47:17 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = localhost/127.0.0.1 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 2.6.5 STARTUP_MSG: classpath = /usr/local/hadoop-2.6.5/etc/hadoop:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-auth-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-common-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/common/hadoop-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/hdfs/hadoop-hdfs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-api-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-registry-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-client-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar:/usr/local/hadoop-2.6.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.5-tests.jar:/usr/local/hadoop-2.6.5/contrib/capacity-scheduler/*.jar STARTUP_MSG: build = https://github.com/apache/hadoop.git -r e8c9fe0b4c252caf2ebf1464220599650f119997; compiled by 'sjlee' on 2016-10-02T23:43Z STARTUP_MSG: java = 1.8.0_191 ************************************************************/ 18/12/17 17:47:17 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT] 18/12/17 17:47:17 INFO namenode.NameNode: createNameNode [-format] Formatting using clusterid: CID-beba43b4-0881-48b4-8eda-5c3bca046398 18/12/17 17:47:17 INFO namenode.FSNamesystem: No KeyProvider found. 18/12/17 17:47:17 INFO namenode.FSNamesystem: fsLock is fair:true 18/12/17 17:47:17 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000 18/12/17 17:47:17 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true 18/12/17 17:47:17 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000 18/12/17 17:47:17 INFO blockmanagement.BlockManager: The block deletion will start around 2018 Dec 17 17:47:17 18/12/17 17:47:17 INFO util.GSet: Computing capacity for map BlocksMap 18/12/17 17:47:17 INFO util.GSet: VM type = 64-bit 18/12/17 17:47:17 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB 18/12/17 17:47:17 INFO util.GSet: capacity = 2^21 = 2097152 entries 18/12/17 17:47:17 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false 18/12/17 17:47:17 INFO blockmanagement.BlockManager: defaultReplication = 2 18/12/17 17:47:17 INFO blockmanagement.BlockManager: maxReplication = 512 18/12/17 17:47:17 INFO blockmanagement.BlockManager: minReplication = 1 18/12/17 17:47:17 INFO blockmanagement.BlockManager: maxReplicationStreams = 2 18/12/17 17:47:17 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000 18/12/17 17:47:17 INFO blockmanagement.BlockManager: encryptDataTransfer = false 18/12/17 17:47:17 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 1000 18/12/17 17:47:17 INFO namenode.FSNamesystem: fsOwner = root (auth:SIMPLE) 18/12/17 17:47:17 INFO namenode.FSNamesystem: supergroup = supergroup 18/12/17 17:47:17 INFO namenode.FSNamesystem: isPermissionEnabled = false 18/12/17 17:47:17 INFO namenode.FSNamesystem: HA Enabled: false 18/12/17 17:47:17 INFO namenode.FSNamesystem: Append Enabled: true 18/12/17 17:47:17 INFO util.GSet: Computing capacity for map INodeMap 18/12/17 17:47:17 INFO util.GSet: VM type = 64-bit 18/12/17 17:47:17 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB 18/12/17 17:47:17 INFO util.GSet: capacity = 2^20 = 1048576 entries 18/12/17 17:47:17 INFO namenode.NameNode: Caching file names occuring more than 10 times 18/12/17 17:47:17 INFO util.GSet: Computing capacity for map cachedBlocks 18/12/17 17:47:17 INFO util.GSet: VM type = 64-bit 18/12/17 17:47:17 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB 18/12/17 17:47:17 INFO util.GSet: capacity = 2^18 = 262144 entries 18/12/17 17:47:17 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033 18/12/17 17:47:17 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0 18/12/17 17:47:17 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 30000 18/12/17 17:47:17 INFO namenode.FSNamesystem: Retry cache on namenode is enabled 18/12/17 17:47:17 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis 18/12/17 17:47:17 INFO util.GSet: Computing capacity for map NameNodeRetryCache 18/12/17 17:47:17 INFO util.GSet: VM type = 64-bit 18/12/17 17:47:17 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB 18/12/17 17:47:17 INFO util.GSet: capacity = 2^15 = 32768 entries 18/12/17 17:47:17 INFO namenode.NNConf: ACLs enabled? false 18/12/17 17:47:17 INFO namenode.NNConf: XAttrs enabled? true 18/12/17 17:47:17 INFO namenode.NNConf: Maximum size of an xattr: 16384 18/12/17 17:47:17 INFO namenode.FSImage: Allocated new BlockPoolId: BP-233285725-127.0.0.1-1545040037972 18/12/17 17:47:18 INFO common.Storage: Storage directory /data/hadoop/hdfs/name has been successfully formatted. 18/12/17 17:47:18 INFO namenode.FSImageFormatProtobuf: Saving image file /data/hadoop/hdfs/name/current/fsimage.ckpt_0000000000000000000 using no compression 18/12/17 17:47:18 INFO namenode.FSImageFormatProtobuf: Image file /data/hadoop/hdfs/name/current/fsimage.ckpt_0000000000000000000 of size 321 bytes saved in 0 seconds. 18/12/17 17:47:18 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0 18/12/17 17:47:18 INFO util.ExitUtil: Exiting with status 0 18/12/17 17:47:18 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1 ************************************************************/ HDFS 启动 启动：start-dfs.sh，根据提示一路 yes 这个命令效果： 主节点会启动任务：NameNode 和 SecondaryNameNode 从节点会启动任务：DataNode 主节点查看：jps，可以看到： 21922 Jps 21603 NameNode 21787 SecondaryNameNode 从节点查看：jps 可以看到： 19728 DataNode 19819 Jps 查看运行更多情况：hdfs dfsadmin -report Configured Capacity: 0 (0 B) Present Capacity: 0 (0 B) DFS Remaining: 0 (0 B) DFS Used: 0 (0 B) DFS Used%: NaN% Under replicated blocks: 0 Blocks with corrupt replicas: 0 Missing blocks: 0 如果需要停止：stop-dfs.sh 查看 log：cd $HADOOP_HOME/logs YARN 运行 start-yarn.sh 然后 jps 你会看到一个：ResourceManager 从节点你会看到：NodeManager 停止：stop-yarn.sh 端口情况 主节点当前运行的所有端口：netstat -tpnl | grep java 会用到端口（为了方便展示，整理下顺序）： tcp 0 0 172.16.0.17:9000 0.0.0.0:* LISTEN 22932/java >> NameNode tcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 22932/java >> NameNode tcp 0 0 0.0.0.0:50090 0.0.0.0:* LISTEN 23125/java >> SecondaryNameNode tcp6 0 0 172.16.0.17:8030 :::* LISTEN 23462/java >> ResourceManager tcp6 0 0 172.16.0.17:8031 :::* LISTEN 23462/java >> ResourceManager tcp6 0 0 172.16.0.17:8032 :::* LISTEN 23462/java >> ResourceManager tcp6 0 0 172.16.0.17:8033 :::* LISTEN 23462/java >> ResourceManager tcp6 0 0 172.16.0.17:8088 :::* LISTEN 23462/java >> ResourceManager 从节点当前运行的所有端口：netstat -tpnl | grep java 会用到端口（为了方便展示，整理下顺序）： tcp 0 0 0.0.0.0:50010 0.0.0.0:* LISTEN 14545/java >> DataNode tcp 0 0 0.0.0.0:50020 0.0.0.0:* LISTEN 14545/java >> DataNode tcp 0 0 0.0.0.0:50075 0.0.0.0:* LISTEN 14545/java >> DataNode tcp6 0 0 :::8040 :::* LISTEN 14698/java >> NodeManager tcp6 0 0 :::8042 :::* LISTEN 14698/java >> NodeManager tcp6 0 0 :::13562 :::* LISTEN 14698/java >> NodeManager tcp6 0 0 :::37481 :::* LISTEN 14698/java >> NodeManager 管理界面 查看 HDFS NameNode 管理界面：http://linux01:50070 访问 YARN ResourceManager 管理界面：http://linux01:8088 访问 NodeManager-1 管理界面：http://linux02:8042 访问 NodeManager-2 管理界面：http://linux03:8042 运行作业 在主节点上操作 运行一个 Mapreduce 作业试试： 计算 π：hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar pi 5 10 运行一个文件相关作业： 由于运行 hadoop 时指定的输入文件只能是 HDFS 文件系统中的文件，所以我们必须将要进行 wordcount 的文件从本地文件系统拷贝到 HDFS 文件系统中。 查看目前根目录结构：hadoop fs -ls / 查看目前根目录结构，另外写法：hadoop fs -ls hdfs://linux-05:9000/ 或者列出目录以及下面的文件：hadoop fs -ls -R / 更多命令可以看：hadoop HDFS常用文件操作命令 创建目录：hadoop fs -mkdir -p /tmp/zch/wordcount_input_dir 上传文件：hadoop fs -put /opt/input.txt /tmp/zch/wordcount_input_dir 查看上传的目录下是否有文件：hadoop fs -ls /tmp/zch/wordcount_input_dir 向 yarn 提交作业，计算单词个数：hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /tmp/zch/wordcount_input_dir /tmp/zch/wordcount_output_dir 查看计算结果输出的目录：hadoop fs -ls /tmp/zch/wordcount_output_dir 查看计算结果输出内容：hadoop fs -cat /tmp/zch/wordcount_output_dir/part-r-00000 查看正在运行的 Hadoop 任务：yarn application -list 关闭 Hadoop 任务进程：yarn application -kill 你的ApplicationId 资料 如何正确的为 MapReduce 配置内存分配 https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/ http://www.cnblogs.com/Leo_wl/p/7426496.html https://blog.csdn.net/bingduanlbd/article/details/51892750 https://blog.csdn.net/whdxjbw/article/details/81050597 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Showdoc-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Showdoc-Install-And-Settings.html","title":"Showdoc 安装和配置","keywords":"","body":"Showdoc 安装和配置 Showdoc 介绍 官网：https://www.showdoc.cc/web/#/ Github：https://github.com/star7th/showdoc 当前（201804）最新版：2.0.7 Docker 下安装 Showdoc 场景： 我的 git 数据目录地址 /opt/git-data clone 官网源码：git clone --depth=1 https://github.com/star7th/showdoc.git 进入源码目录：cd /opt/git-data/showdoc 构建 docker 镜像：docker build -t showdoc ./ 运行镜像：docker run -d --name showdoc -p 4999:80 showdoc 浏览器输入：http://47.106.127.131:4999/install，进行程序的初始化，其实就是让你选择语言和几个提示 注意： 容器中的数据存放目录为：/var/www/html，其中 数据库文件：/var/www/html/Sqlite/sqlite.db.php 图片：/var/www/html/Public/Uploads 后续需要升级系统，更换服务器需要迁移这两个目录下的文件。我尝试挂载这两个目录，但是发现出现各种问题解决不了，所以就不管了。 网站首页地址：http://47.106.127.131:4999/web/#/ 没有什么系统管理员用户的概念，需要自己注册 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/WordPress-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/WordPress-Install-And-Settings.html","title":"WordPress 安装和配置","keywords":"","body":"WordPress 安装和配置 环境 腾讯云 CentOS 7.4 1C + 1G（最低配置） IP：193.112.211.201 推荐按此文章进行安装的时候可以把该 IP 替换成你的，方便直接复制 更新系统可更新软件 yum clean all yum -y update 安装 Apache yum install -y httpd systemctl start httpd.service systemctl enable httpd.service 访问（如果出现 Apache 欢迎页面即表示成功）：http://193.112.211.201 安装 MySQL 先检查是否已经安装了 Mariadb 检查：rpm -qa | grep mariadb 卸载：rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 MySQL 5.5 安装和配置（内存 1G 推荐） MySQL 5.5 MySQL 5.6 安装和配置（如果内存没有大于 2G，请不要使用） MySQL 5.6 MySQL 5.7（推荐） wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall -y mysql57-community-release-el7-8.noarch.rpm yum install mysql-community-server systemctl enable mysqld.service systemctl restart mysqld.service MySQL 5.7 配置 默认 MySQL 5.7 安装完有一个随机密码生成，位置在：/var/log/mysqld.log，里面有这样一句话：A temporary password is generated for root@localhost: 随机密码 如果初次要连上去需要填写该密码 我们也可以选择重置密码： systemctl stop mysqld.service /usr/sbin/mysqld --skip-grant-tables --user=mysql 在启动一个终端：mysql -u root mysql UPDATE user SET authentication_string=PASSWORD('新密码') where USER='root';FLUSH PRIVILEGES; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '上一步的新密码' WITH GRANT OPTION; systemctl restart mysqld.service 试一下：mysql -h localhost -u root -p 然后输入密码，在 MySQL 终端输入：select 1; 如果报：You must reset your password using ALTER USER statement before executing this statement，解决办法： set global validate_password_policy=0; #密码强度设为最低等级 set global validate_password_length=6; #密码允许最小长度为6 set password = password('新密码'); FLUSH PRIVILEGES; YUM 安装的 MySQL 默认配置文件在：vim /etc/my.cnf，默认有如下信息，会自己配置的可以改下。 # For advice on how to change settings please see # http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html [mysqld] # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove leading # to turn on a very important data integrity option: logging # changes to the binary log between backups. # log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0 log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid 安装 PHP 7 CentOS 7 默认是 PHP 5.4，版本太低 安装过程：https://www.tecmint.com/install-php-7-in-centos-7/ 默认配置文件位置：vim /etc/php.ini 测试 PHP 安装结果，新建文件：vim /var/www/html/info.php 浏览器访问（出现 PHP 环境信息表示安装成功）：http://193.112.211.201/info.php 测试后删除刚刚文件：rm -rf /var/www/html/info.php 安装 WordPress 寻找官网最新版本下载地址（201806 是 4.9.4）：https://cn.wordpress.org/ cd /var/www/html/ wget https://cn.wordpress.org/wordpress-4.9.4-zh_CN.zip unzip wordpress-4.9.4-zh_CN.zip rm -rf wordpress-4.9.4-zh_CN.zip cd /var/www/html/wordpress && mv * ../ rm -rf /var/www/html/wordpress/ chmod -R 777 /var/www/html/ 修改 Apache 配置文件：vim /etc/httpd/conf/httpd.conf 旧值： #ServerName www.example.com:80 改为： ServerName www.youmeek.com:80 ---------------------- 旧值： AllowOverride None 改为： AllowOverride All ---------------------- 旧值： DirectoryIndex index.html 改为： DirectoryIndex index.html index.htm Default.html Default.htm index.php Default.php index.html.var 重启 Apache systemctl restart httpd.service systemctl enable httpd.service 创建数据库 SQL 语句：CREATE DATABASE wordpress DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 如果有数据则直接导入：/usr/bin/mysql -u root --password=123456 DATABASE_Name WordPress 在线配置引导 浏览器访问：http://193.112.211.201/wp-admin/setup-config.php DNS 解析 我是托管到 DNSPOD，重新指向到新 IP 地址即可 资料 https://blog.csdn.net/qq_35723367/article/details/79544001 https://zhuanlan.zhihu.com/p/36744507 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/GoAccess-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/GoAccess-Install-And-Settings.html","title":"GoAccess 安装和配置","keywords":"","body":"GoAccess 安装和配置 官网资料 一般用于 Apache, Nginx 的 Log 分析 官网：https://goaccess.io/ 官网下载（201807 最新版本 1.2）：https://goaccess.io/download 官网 Github：https://github.com/allinurl/goaccess 国内中文站：https://goaccess.cc/ 安装（CentOS 7.4） 注意，如果是在 CentOS 6 下安装会碰到一些问题，可以参考：https://www.jianshu.com/p/7cacc1d20588 安装依赖包 yum install -y ncurses-devel wget http://geolite.maxmind.com/download/geoip/api/c/GeoIP.tar.gz tar -zxvf GeoIP.tar.gz cd GeoIP-1.4.8/ ./configure make && make install 安装 GoAccess wget http://tar.goaccess.io/goaccess-1.2.tar.gz tar -xzvf goaccess-1.2.tar.gz cd goaccess-1.2/ ./configure --enable-utf8 --enable-geoip=legacy make && make install 配置 假设你 nginx 安装在：/usr/local/nginx 假设你 nginx 的 log 输出到：/var/log/nginx 修改 vim /usr/local/nginx/conf/nginx.conf 指定 nginx 的日志格式 http { charset utf8; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" \"$request_time\"'; access_log /var/log/nginx/access.log main; error_log /var/log/nginx/error.log; } 停止 nginx：/usr/local/nginx/sbin/nginx -s stop 备份旧的 nginx log 文件：mv /var/log/nginx/access.log /var/log/nginx/access.log.20180702back 启动 nginx：/usr/local/nginx/sbin/nginx 创建 GoAccess 配置文件：vim /etc/goaccess_log_conf_nginx.conf time-format %T date-format %d/%b/%Y log_format %h - %^ [%d:%t %^] \"%r\" %s %b \"%R\" \"%u\" \"%^\" %^ %^ %^ %T 使用 在终端上展示数据 goaccess -a -d -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf 手动生成当前统计页面 goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html 更多参数用法： 时间分布图上：按小时展示数据： goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=min 时间分布图上：按分钟展示数据： goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=hour 不显示指定的面板 goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=min \\ --ignore-panel=VISITORS \\ --ignore-panel=REQUESTS \\ --ignore-panel=REQUESTS_STATIC \\ --ignore-panel=NOT_FOUND \\ --ignore-panel=HOSTS \\ --ignore-panel=OS \\ --ignore-panel=BROWSERS \\ --ignore-panel=VIRTUAL_HOSTS \\ --ignore-panel=REFERRERS \\ --ignore-panel=REFERRING_SITES \\ --ignore-panel=KEYPHRASES \\ --ignore-panel=STATUS_CODES \\ --ignore-panel=REMOTE_USER \\ --ignore-panel=GEO_LOCATION 我一般只留下几个面板（排除掉不想看的面板，因为使用 --enable-panel 参数无法达到这个目的） goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=min \\ --ignore-panel=VISITORS \\ --ignore-panel=REQUESTS_STATIC \\ --ignore-panel=NOT_FOUND \\ --ignore-panel=OS \\ --ignore-panel=VIRTUAL_HOSTS \\ --ignore-panel=REFERRERS \\ --ignore-panel=KEYPHRASES \\ --ignore-panel=REMOTE_USER \\ --ignore-panel=GEO_LOCATION 方便执行命令创建脚本 vim goaccess_report_by_min.sh goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=min \\ --ignore-panel=VISITORS \\ --ignore-panel=REQUESTS_STATIC \\ --ignore-panel=NOT_FOUND \\ --ignore-panel=OS \\ --ignore-panel=VIRTUAL_HOSTS \\ --ignore-panel=REFERRERS \\ --ignore-panel=KEYPHRASES \\ --ignore-panel=REMOTE_USER \\ --ignore-panel=GEO_LOCATION vim goaccess_report_by_hour.sh goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --ignore-crawlers --hour-spec=hour \\ --ignore-panel=VISITORS \\ --ignore-panel=REQUESTS_STATIC \\ --ignore-panel=NOT_FOUND \\ --ignore-panel=OS \\ --ignore-panel=VIRTUAL_HOSTS \\ --ignore-panel=REFERRERS \\ --ignore-panel=KEYPHRASES \\ --ignore-panel=REMOTE_USER \\ --ignore-panel=GEO_LOCATION 实时生成统计页面 我个人看法是：一般没必要浪费这个性能，需要的时候执行下脚本就行了。 官网文档：https://goaccess.io/man#examples，查询关键字：REAL TIME HTML OUTPUT goaccess -f /var/log/nginx/access.log -p /etc/goaccess_log_conf_nginx.conf -o /usr/local/nginx/report/index.html --real-time-html --daemonize 资料 https://www.fanhaobai.com/2017/06/go-access.html https://www.imydl.tech/lnmp/32.html Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Portainer-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Portainer-Install-And-Settings.html","title":"Portainer 安装和配置","keywords":"","body":"Portainer 安装和配置、优化 介绍 官网：https://portainer.io/ 官网 GitHub：https://github.com/portainer/portainer 官网文档：https://portainer.readthedocs.io/en/stable/ 安装 创建文件夹：mkdir -p /data/docker/portainer 赋权：chmod -R 777 /data/docker/portainer 创建文件：vim docker-compose.yml version: '3' services: portainer: container_name: portainer image: portainer/portainer volumes: - /data/docker/portainer:/data - /var/run/docker.sock:/var/run/docker.sock ports: - \"9000:9000\" 启动：docker-compose up -d 该容器占用内存非常非常小，只有 5 M 左右。 对本地监控配置 因为 Portainer 镜像构建的时候已经配置了：/var/run/docker.sock:/var/run/docker.sock，所以对于跟 Portainer 同一台机子的其他容器都可以被直接监控 浏览器访问访问：http://192.168.1.2:9000 第一次启动会让你创建用户名和密码。第二步就是配置管理哪里的 docker 容器，我这里选择：local 远程监控配置 以下方法为了方便，没有做任何安全措施，请用于内网 关掉防火墙 修改远程 Docker 配置：vim /usr/lib/systemd/system/docker.service 旧值： ExecStart=/usr/bin/dockerd 新值： ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 重启 Docker：systemctl daemon-reload && systemctl reload docker && systemctl restart docker Portainer 启动选择 Remote，填写远程 IP 和端口，比如：192.168.1.3:2375 资料 Portainer 容器管理 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Grafana-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Grafana-Install-And-Settings.html","title":"Grafana 安装和配置","keywords":"","body":"Grafana 安装和配置 对于版本 支持的 Elasticsearch 版本 Grafana 安装 CentOS 7.4 rpm 文件包大小 53M 所需内存：300M 左右 官网下载：https://grafana.com/grafana/download?platform=linux 官网指导：http://docs.grafana.org/installation/rpm/ sudo yum install -y initscripts fontconfig urw-fonts wget https://dl.grafana.com/oss/release/grafana-5.4.0-1.x86_64.rpm sudo yum localinstall -y grafana-5.4.0-1.x86_64.rpm 启动 Grafana 服务（默认是不启动的） sudo systemctl start grafana-server sudo systemctl status grafana-server 将 Grafana 服务设置为开机启动：sudo systemctl enable grafana-server 开放端口：firewall-cmd --add-port=3000/tcp --permanent 重新加载防火墙配置：firewall-cmd --reload 访问：http://192.168.0.105:3000 默认管理账号；admin，密码：admin，登录后需要修改密码 配置 官网指导：http://docs.grafana.org/installation/configuration/ 安装包默认安装后的一些路径 二进制文件：/usr/sbin/grafana-server init.d 脚本：/etc/init.d/grafana-server 配置文件：/etc/grafana/grafana.ini 日志文件：/var/log/grafana/grafana.log 插件目录是：/var/lib/grafana/plugins 默认配置的 sqlite3 数据库：/var/lib/grafana/grafana.db 最重要的配置文件：vim /etc/grafana/grafana.ini 可以修改用户名和密码 端口 数据路径 数据库配置 第三方认证 Session 有效期 添加数据源：http://192.168.0.105:3000/datasources/new 添加组织：http://192.168.0.105:3000/admin/orgs 添加用户：http://192.168.0.105:3000/org/users 添加插件：http://192.168.0.105:3000/plugins 个性化设置：http://192.168.0.105:3000/org 软件变量：http://192.168.0.105:3000/admin/settings 数据源 Elasticsearch 使用： https://cloud.tencent.com/info/68052367407c3bf21cc10c0263027f3f.html http://docs.grafana.org/features/datasources/elasticsearch/#using-elasticsearch-in-grafana <> <> <> <> 其他资料 https://blog.csdn.net/BianChengNinHao/article/details/80985302 <> <> <> <> <> <> <> <> <> Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"Linux-Tutorial/markdown-file/Ansible-Install-And-Settings.html":{"url":"Linux-Tutorial/markdown-file/Ansible-Install-And-Settings.html","title":"Ansible 安装和配置","keywords":"","body":"Ansible 安装和配置 Ansible 说明 Ansible 官网：https://www.ansible.com/ Ansible 官网 Github：https://github.com/ansible/ansible Ansible 官网文档：https://docs.ansible.com// 简单讲：它的作用就是把写 shell 这件事变成标准化、模块化。方便更好的自动化运维 安装 官网说明：https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html CentOS：sudo yum install -y ansible 查看版本：ansible --version 配置基本概念 Ansible 基本配置文件顺序 Ansible 执行的时候会按照以下顺序查找配置项，所以修改的时候要特别注意改的是哪个文件 ANSIBLE_CONFIG (环境变量) ansible.cfg (脚本所在当前目录下) ~/.ansible.cfg (用户家目录下，默认没有) /etc/ansible/ansible.cfg（安装后会自动生成） 配置远程主机地址 (Ansible 称这些地址为 Inventory) 假设我有 3 台机子： 192.168.0.223 192.168.0.70 192.168.0.103 官网对此的配置说明：https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#hosts-and-groups 给这三台机子设置免密登录的情况（一般推荐方式） 编辑 Ansible 配置文件：vim /etc/ansible/hosts 添加如下内容 [hadoop-host] 192.168.0.223 192.168.0.70 192.168.0.103 其中 [hadoop-host] 表示这些主机代表的一个组名 如果不设置免密，直接采用账号密码（容易泄露信息） 编辑 Ansible 配置文件：vim /etc/ansible/hosts 添加如下内容 [hadoop-host] hadoop-master ansible_host=192.168.0.223 ansible_user=root ansible_ssh_pass=123456 hadoop-node1 ansible_host=192.168.0.70 ansible_user=root ansible_ssh_pass=123456 hadoop-node2 ansible_host=192.168.0.103 ansible_user=root ansible_ssh_pass=123456 简单使用（ad hoc方式） ad hoc 官网：https://docs.ansible.com/ansible/latest/user_guide/intro_adhoc.html 运行 Ansible 运行 Ansible 的 ping 命令，看看配置正确时输出如下： sudo ansible --private-key ~/.ssh/id_rsa all -m ping 让远程所有主机都执行 ps 命令，输出如下 ansible all -a 'ps' 让远程所有 hadoop-host 组的主机都执行 ps 命令，输出如下 ansible hadoop-host -a 'ps' Playbook 脚本方式 官网：https://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html 一些语法：https://docs.ansible.com/ansible/latest/modules/command_module.html playbook（剧本），顾名思义，就是需要定义一个脚本或者说配置文件，然后定义好要做什么。之后 ansible 就会根据 playbook 脚本对远程主机进行操作 简单脚本 下面脚本让所有远程主机执行 whoami 命令，并把结果（当前用户名）输出到 /opt/whoami.txt 文件 创建脚本文件：vim /opt/simple-playbook.yml - hosts: all tasks: - name: whoami shell: 'whoami > /opt/whoami.txt' 执行命令：ansible-playbook /opt/simple-playbook.yml，结果如下，并且 opt 下也有文件生成 PLAY [all] ************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************** ok: [192.168.0.223] ok: [192.168.0.103] ok: [192.168.0.70] TASK [whoami] *********************************************************************************************************************** changed: [192.168.0.103] changed: [192.168.0.223] changed: [192.168.0.70] PLAY RECAP ************************************************************************************************************************** 192.168.0.103 : ok=2 changed=1 unreachable=0 failed=0 192.168.0.223 : ok=2 changed=1 unreachable=0 failed=0 192.168.0.70 : ok=2 changed=1 unreachable=0 failed=0 平时用来测试 创建脚本文件：vim /opt/test-playbook.yml - hosts: hadoop-test remote_user: root vars: java_install_folder: /usr/local tasks: # 按行的方式写入 - name: Set JAVA_HOME 1 lineinfile: dest=/etc/profile line=\"JAVA_HOME={{ java_install_folder }}/jdk1.8.0_181\" # 按块的方式写入，#{mark} 会被自动替换成：begin 和 end 字符来包裹整块内容（我这里自己定义了词语） - name: Set JAVA_HOME 2 blockinfile: path: /etc/profile marker: \"#{mark} JDK ENV\" marker_begin: \"开始\" marker_end: \"结束\" block: | export JAVA_HOME={{ java_install_folder }}/jdk1.8.0_181 export PATH=$PATH:$JAVA_HOME/bin 执行命令：ansible-playbook /opt/test-playbook.yml 更多 playbook 实战 禁用防火墙（CentOS 7.x） 创建脚本文件：vim /opt/disable-firewalld-playbook.yml - hosts: all remote_user: root tasks: - name: Disable SELinux at next reboot selinux: state: disabled - name: disable firewalld command: \"{{ item }}\" with_items: - systemctl stop firewalld - systemctl disable firewalld - setenforce 0 基础环境（CentOS 7.x） 创建脚本文件：vim /opt/install-basic-playbook.yml - hosts: all remote_user: root tasks: - name: Disable SELinux at next reboot selinux: state: disabled - name: disable firewalld command: \"{{ item }}\" with_items: - systemctl stop firewalld - systemctl disable firewalld - setenforce 0 - name: install-basic command: \"{{ item }}\" with_items: - yum install -y zip unzip lrzsz git epel-release wget htop deltarpm - name: install-vim shell: \"{{ item }}\" with_items: - yum install -y vim - curl https://raw.githubusercontent.com/wklken/vim-for-server/master/vimrc > ~/.vimrc - name: install-docker shell: \"{{ item }}\" with_items: - yum install -y yum-utils device-mapper-persistent-data lvm2 - yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo - yum makecache fast - yum install -y docker-ce - systemctl start docker.service - docker run hello-world - name: install-docker-compose shell: \"{{ item }}\" with_items: - curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose - chmod +x /usr/local/bin/docker-compose - docker-compose --version - systemctl restart docker.service - systemctl enable docker.service 执行命令：ansible-playbook /opt/install-basic-playbook.yml 修改 hosts 创建脚本文件：vim /opt/hosts-playbook.yml - hosts: all remote_user: root tasks: - name: update hosts blockinfile: path: /etc/hosts block: | 192.168.0.223 linux01 192.168.0.223 linux02 192.168.0.223 linux03 192.168.0.223 linux04 192.168.0.223 linux05 执行命令：ansible-playbook /opt/hosts-playbook.yml 部署 JDK 创建脚本文件：vim /opt/jdk8-playbook.yml - hosts: hadoop-host remote_user: root vars: java_install_folder: /usr/local tasks: - name: copy jdk copy: src=/opt/jdk-8u181-linux-x64.tar.gz dest={{ java_install_folder }} - name: tar jdk shell: chdir={{ java_install_folder }} tar zxf jdk-8u181-linux-x64.tar.gz - name: set JAVA_HOME blockinfile: path: /etc/profile marker: \"#{mark} JDK ENV\" block: | JAVA_HOME={{ java_install_folder }}/jdk1.8.0_181 JRE_HOME=$JAVA_HOME/jre PATH=$PATH:$JAVA_HOME/bin CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export JAVA_HOME export JRE_HOME export PATH export CLASSPATH - name: source profile shell: source /etc/profile 执行命令：ansible-playbook /opt/jdk8-playbook.yml 部署 Hadoop 集群 创建脚本文件：vim /opt/hadoop-playbook.yml 刚学 Ansible，不好动配置文件，所以就只保留环境部分的设置，其他部分自行手工~ - hosts: hadoop-host remote_user: root tasks: - name: Creates directory file: path: /data/hadoop/hdfs/name state: directory - name: Creates directory file: path: /data/hadoop/hdfs/data state: directory - name: Creates directory file: path: /data/hadoop/hdfs/tmp state: directory - name: set HADOOP_HOME blockinfile: path: /etc/profile marker: \"#{mark} HADOOP ENV\" block: | HADOOP_HOME=/usr/local/hadoop PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin export HADOOP_HOME export PATH - name: source profile shell: source /etc/profile 执行命令：ansible-playbook /opt/hadoop-playbook.yml 资料 ANSIBLE模块 - shell和command区别 https://www.the5fire.com/ansible-guide-cn.html https://www.jianshu.com/p/62388a4fcbc6 http://showme.codes/2017-06-12/ansible-introduce/ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:39:06 "},"advanced-java/":{"url":"advanced-java/","title":"分布式","keywords":"","body":"互联网 Java 工程师进阶知识完全扫盲 本系列知识出自中华石杉，内容涵盖高并发、分布式、高可用、微服务等领域知识。我对这部分知识做了一个系统的整理，方便学习查阅。配合《大型网站技术架构——李智慧》、《Redis 设计与实现——黄健宏》食用，效果更佳。 学习之前，先来看看 Issues 讨论区的技术面试官是怎么说的吧。本项目也欢迎各位开发者朋友来分享自己的一些想法和实践经验。 高并发架构 消息队列 为什么使用消息队列？消息队列有什么优点和缺点？Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么优点和缺点？ 如何保证消息队列的高可用？ 如何保证消息不被重复消费？（如何保证消息消费的幂等性） 如何保证消息的可靠性传输？（如何处理消息丢失的问题） 如何保证消息的顺序性？ 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。 搜索引擎 es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？ es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？ es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？ es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？ 缓存 在项目中缓存是如何使用的？缓存如果使用不当会造成什么后果？ Redis 和 Memcached 有什么区别？Redis 的线程模型是什么？为什么单线程的 Redis 比多线程的 Memcached 效率要高得多？ Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？ Redis 的过期策略都有哪些？手写一下 LRU 代码实现？ 如何保证 Redis 高并发、高可用？Redis 的主从复制原理能介绍一下么？Redis 的哨兵原理能介绍一下么？ Redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ Redis 集群模式的工作原理能说一下么？在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？如何动态增加和删除一个节点？ 了解什么是 redis 的雪崩、穿透和击穿？Redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 Redis 的穿透？ 如何保证缓存与数据库的双写一致性？ Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？ 生产环境中的 Redis 是怎么部署的？ 分库分表 为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？ 如何设计可以动态扩容缩容的分库分表方案？ 分库分表之后，id 主键如何处理？ 读写分离 如何实现 MySQL 的读写分离？MySQL 主从复制原理是啥？如何解决 MySQL 主从同步的延时问题？ 高并发系统 如何设计一个高并发系统？ 分布式系统 面试连环炮 系统拆分 为什么要进行系统拆分？如何进行系统拆分？拆分后不用 Dubbo 可以吗？ 分布式服务框架 说一下 Dubbo 的工作原理？注册中心挂了可以继续通信吗？ Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ Dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ Dubbo 的 spi 思想是什么？ 如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 分布式服务接口请求的顺序性如何保证？ 如何自己设计一个类似 Dubbo 的 RPC 框架？ 分布式锁 Zookeeper 都有哪些应用场景？ 使用 Redis 如何设计分布式锁？使用 Zookeeper 来设计分布式锁可以吗？以上两种分布式锁的实现方式哪种效率比较高？ 分布式事务 分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？ 分布式会话 集群部署时的分布式 Session 如何实现？ 高可用架构 Hystrix 介绍 电商网站详情页系统架构 Hystrix 线程池技术实现资源隔离 Hystrix 信号量机制实现资源隔离 Hystrix 隔离策略细粒度控制 深入 Hystrix 执行时内部原理 基于 request cache 请求缓存技术优化批量商品数据查询接口 基于本地缓存的 fallback 降级机制 深入 Hystrix 断路器执行原理 深入 Hystrix 线程池隔离与接口限流 基于 timeout 机制为服务接口调用超时提供安全保护 高可用系统 如何设计一个高可用系统？ 限流 如何限流？在工作中是怎么做的？说一下具体的实现？ 熔断 如何进行熔断？ 熔断框架都有哪些？具体实现原理知道吗？ 降级 如何进行降级？ 微服务架构 微服务架构整个章节内容属额外新增，后续抽空更新，也欢迎读者们参与补充完善 关于微服务架构的描述 Spring Cloud 微服务架构 什么是微服务？微服务之间是如何独立通讯的？ Spring Cloud 和 Dubbo 有哪些区别？ Spring Boot 和 Spring Cloud，谈谈你对它们的理解？ 什么是服务熔断？什么是服务降级？ 微服务的优缺点分别是什么？说一下你在项目开发中碰到的坑？ 你所知道的微服务技术栈都有哪些？ Eureka 和 Zookeeper 都可以提供服务注册与发现的功能，它们有什么区别？ ...... Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:50:06 "},"advanced-java/docs/high-concurrency/mq-interview.html":{"url":"advanced-java/docs/high-concurrency/mq-interview.html","title":"消息队列","keywords":"","body":"消息队列面试场景 面试官：你好。 候选人：你好。 （面试官在你的简历上面看到了，呦，有个亮点，你在项目里用过 MQ，比如说你用过 ActiveMQ） 面试官：你在系统里用过消息队列吗？（面试官在随和的语气中展开了面试） 候选人：用过的（此时感觉没啥） 面试官：那你说一下你们在项目里是怎么用消息队列的？ 候选人：巴拉巴拉，“我们啥啥系统发送个啥啥消息到队列，别的系统来消费啥啥的。比如我们有个订单系统，订单系统每次下一个新的订单的时候，就会发送一条消息到 ActiveMQ 里面去，后台有个库存系统负责获取消息然后更新库存。” （部分同学在这里会进入一个误区，就是你仅仅就是知道以及回答你们是怎么用这个消息队列的，用这个消息队列来干了个什么事情？） 面试官：那你们为什么使用消息队列啊？你的订单系统不发送消息到 MQ，直接订单系统调用库存系统一个接口，咔嚓一下，直接就调用成功，库存不也就更新了。 候选人：额。。。（楞了一下，为什么？我没怎么仔细想过啊，老大让用就用了），硬着头皮胡言乱语了几句。 （面试官此时听你楞了一下，然后听你胡言乱语了几句，开始心里觉得有点儿那什么了，怀疑你之前就压根儿没思考过这问题） 面试官：那你说说用消息队列都有什么优点和缺点？ （面试官此时心里想的是，你的 MQ 在项目里为啥要用，你没怎么考虑过，那我稍微简单点儿，我问问你消息队列你之前有没有考虑过如果用的话，优点和缺点分别是啥？） 候选人：这个。。。（确实平时没怎么考虑过这个问题啊。。。胡言乱语了） （面试官此时心里已经更觉得你这哥儿们不行，平时都没什么思考） 面试官：Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别？ （面试官问你这个问题，就是说，绕过比较虚的话题，直接看看你对各种 MQ 中间件是否了解，是否做过功课，是否做过调研） 候选人：我们就用过 ActiveMQ，所以别的没用过。。。区别，也不太清楚。。。 （面试官此时更是觉得你这哥儿们平时就是瞎用，根本就没什么思考，觉得不行） 面试官：那你们是如何保证消息队列的高可用啊？ 候选人：这个。。。我平时就是简单走 API 调用一下，不太清楚消息队列怎么部署的。。。 面试官：如何保证消息不被重复消费啊？如何保证消费的时候是幂等的啊？ 候选人：啥？（MQ 不就是写入&消费就可以了，哪来这么多问题） 面试官：如何保证消息的可靠性传输啊？要是消息丢失了怎么办啊？ 候选人：我们没怎么丢过消息啊。。。 面试官：那如何保证消息的顺序性？ 候选人：顺序性？什么意思？我为什么要保证消息的顺序性？它不是本来就有顺序吗？ 面试官：如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 候选人：不是，我这平时没遇到过这些问题啊，就是简单用用，知道 MQ 的一些功能。 面试官：如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。 候选人：。。。。。我还是走吧。。。。 这其实是面试官的一种面试风格，就是说面试官的问题不是发散的，而是从一个小点慢慢铺开。比如说面试官可能会跟你聊聊高并发话题，就这个话题里面跟你聊聊缓存、MQ 等等东西，由浅入深，一步步深挖。 其实上面是一个非常典型的关于消息队列的技术考察过程，好的面试官一定是从你做过的某一个点切入，然后层层展开深入考察，一个接一个问，直到把这个技术点刨根问底，问到最底层。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/why-mq.html":{"url":"advanced-java/docs/high-concurrency/why-mq.html","title":"为什么使用消息队列？消息队列有什么优点和缺点？Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么优点和缺点？","keywords":"","body":"面试题 为什么使用消息队列？ 消息队列有什么优点和缺点？ Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？ 面试官心理分析 其实面试官主要是想看看： 第一，你知不知道你们系统里为什么要用消息队列这个东西？ 不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。 没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&坏处？ 你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？ 你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。 如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。 面试题剖析 为什么使用消息队列 其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。 先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。 解耦 看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃...... 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。 异步 再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ 削峰 每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 消息队列有什么优缺点 优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低 系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题 A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？ 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/how-to-ensure-high-availability-of-message-queues.html":{"url":"advanced-java/docs/high-concurrency/how-to-ensure-high-availability-of-message-queues.html","title":"如何保证消息队列的高可用？","keywords":"","body":"面试题 如何保证消息队列的高可用？ 面试官心理分析 如果有人问到你 MQ 的知识，高可用是必问的。上一讲提到，MQ 会导致系统可用性降低。所以只要你用了 MQ，接下来问的一些要点肯定就是围绕着 MQ 的那些缺点怎么来解决了。 要是你傻乎乎的就干用了一个 MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的感觉就是，只会简单使用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个 20k 薪资以内的普通小弟还凑合，要是做薪资 20k+ 的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。 面试题剖析 这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。 所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。 RabbitMQ 的高可用性 RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。 RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式 单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。 普通集群模式（无高可用性） 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。 而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 镜像集群模式（高可用性） 这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？ Kafka 的高可用性 Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。 这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。 实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。 Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。 比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。 Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。 这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。 写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为） 消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。 看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.html":{"url":"advanced-java/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.html","title":"如何保证消息不被重复消费？（如何保证消息消费的幂等性）","keywords":"","body":"面试题 如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？ 面试官心理分析 其实这是很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？这个是 MQ 领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。 面试题剖析 回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 举个栗子。 有这么个场景。数据 1/2/3 依次进入 kafka，kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.html":{"url":"advanced-java/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.html","title":"如何保证消息的可靠性传输？（如何处理消息丢失的问题）","keywords":"","body":"面试题 如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？ 面试官心理分析 这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。 如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。 面试题剖析 数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。 RabbitMQ 生产者弄丢了数据 生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。 此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。 // 开启事务 channel.txSelect try { // 这里发送消息 } catch (Exception e) { channel.txRollback // 这里再次重发这条消息 } // 提交事务 channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。 所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据 就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化 这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2 就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。 注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。 所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据 RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。 这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。 Kafka 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据 这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？ 如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/how-to-ensure-the-order-of-messages.html":{"url":"advanced-java/docs/high-concurrency/how-to-ensure-the-order-of-messages.html","title":"如何保证消息的顺序性？","keywords":"","body":"面试题 如何保证消息的顺序性？ 面试官心理分析 其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。 面试题剖析 我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -> mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案 RabbitMQ 拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/mq-time-delay-and-expired-failure.html":{"url":"advanced-java/docs/high-concurrency/mq-time-delay-and-expired-failure.html","title":"如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？","keywords":"","body":"面试题 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？ 面试官心理分析 你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？ 所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。 面试题剖析 关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。 大量消息在 mq 里积压了几个小时了还没解决 几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。 一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。 一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下： 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。 mq 中的消息过期失效了 假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。 这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。 假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。 mq 都快写满了 如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/mq-design.html":{"url":"advanced-java/docs/high-concurrency/mq-design.html","title":"如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。","keywords":"","body":"面试题 如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。 面试官心理分析 其实聊到这个问题，一般面试官要考察两块： 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。 说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？ 面试题剖析 其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。 比如说这个消息队列系统，我们从以下几个角度来考虑一下： 首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -> topic -> partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。 其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -> leader & follower -> broker 挂了重新选举 leader 即可对外服务。 能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。 mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/es-introduction.html":{"url":"advanced-java/docs/high-concurrency/es-introduction.html","title":"搜索引擎","keywords":"","body":"lucene 和 es 的前世今生 lucene 是最先进、功能最强大的搜索库。如果直接基于 lucene 开发，非常复杂，即便写一些简单的功能，也要写大量的 Java 代码，需要深入理解原理。 elasticsearch 基于 lucene，隐藏了 lucene 的复杂性，提供了简单易用的 restful api / Java api 接口（另外还有其他语言的 api 接口）。 分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式，支持 PB 级数据 es 的核心概念 Near Realtime 近实时，有两层意思： 从写入数据到数据可以被搜索到有一个小延迟（大概是 1s） 基于 es 执行搜索和分析可以达到秒级 Cluster 集群 集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。 Node 节点 Node 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。 Document & field 文档是 es 中最小的数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 json 数据结构来表示。每个 index 下的 type，都可以存储多条 document。一个 document 里面有多个 field，每个 field 就是一个数据字段。 { \"product_id\": \"1\", \"product_name\": \"iPhone X\", \"product_desc\": \"苹果手机\", \"category_id\": \"2\", \"category_name\": \"电子产品\" } Index 索引包含了一堆有相似结构的文档数据，比如商品索引。一个索引包含很多 document，一个索引就代表了一类相似或者相同的 ducument。 Type 类型，每个索引里可以有一个或者多个 type，type 是 index 的一个逻辑分类，比如商品 index 下有多个 type：日化商品 type、电器商品 type、生鲜商品 type。每个 type 下的 document 的 field 可能不太一样。 shard 单台机器无法存储大量数据，es 可以将一个索引中的数据切分为多个 shard，分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。 replica 任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 replica 副本。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5个 replica shard，最小的高可用配置，是 2 台服务器。 这么说吧，shard 分为 primary shard 和 replica shard。而 primary shard 一般简称为 shard，而 replica shard 一般简称为 replica。 es 核心概念 vs. db 核心概念 es db index 数据库 type 数据表 docuemnt 一行数据 以上是一个简单的类比。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/es-architecture.html":{"url":"advanced-java/docs/high-concurrency/es-architecture.html","title":"es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？","keywords":"","body":"面试题 es 的分布式架构原理能说一下么（es 是如何实现分布式的啊）？ 面试官心理分析 在搜索这块，lucene 是最流行的搜索库。几年前业内一般都问，你了解 lucene 吗？你知道倒排索引的原理吗？现在早已经 out 了，因为现在很多项目都是直接用基于 lucene 的分布式搜索引擎—— ElasticSearch，简称为 es。 而现在分布式搜索基本已经成为大部分互联网行业的 Java 系统的标配，其中尤为流行的就是 es，前几年 es 没火的时候，大家一般用 solr。但是这两年基本大部分企业和项目都开始转向 es 了。 所以互联网面试，肯定会跟你聊聊分布式搜索引擎，也就一定会聊聊 es，如果你确实不知道，那你真的就 out 了。 如果面试官问你第一个问题，确实一般都会问你 es 的分布式架构设计能介绍一下么？就看看你对分布式搜索引擎架构的一个基本理解。 面试题剖析 ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 es 进程实例，组成了一个 es 集群。 es 中存储数据的基本单位是索引，比如说你现在要在 es 中存储一些订单数据，你就应该在 es 中创建一个索引 order_idx，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。 index -> type -> mapping -> document -> field。 这样吧，为了做个更直白的介绍，我在这里做个类比。但是切记，不要划等号，类比只是为了便于理解。 index 相当于 mysql 里的一张表。而 type 没法跟 mysql 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，比如一件衣服、一双鞋子；有些订单是虚拟商品的订单，比如游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。 所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。 很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（注意，mapping types 这个概念在 ElasticSearch 7.X 已被完全移除，详细说明可以参考官方文档），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。 你搞一个索引，这个索引可以拆分成多个 shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。 接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard，负责写入数据，但是还有几个 replica shard。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。 通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。 es 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。 如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。 说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。 其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/es-write-query-search.html":{"url":"advanced-java/docs/high-concurrency/es-write-query-search.html","title":"es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？","keywords":"","body":"面试题 es 写入数据的工作原理是什么啊？es 查询数据的工作原理是什么啊？底层的 lucene 介绍一下呗？倒排索引了解吗？ 面试官心理分析 问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的是...... 对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了。要是出点什么问题，你啥都不知道，那还能指望你什么呢？ 面试题剖析 es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es 读数据过程 可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node。 coordinate node 返回 document 给客户端。 es 搜索数据过程 es 最强大的是做全文检索，就是比如你有三条数据： java真好玩儿啊 java好难学啊 j2ee特别牛 你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。 客户端发送请求到一个 coordinate node。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除/更新数据底层原理 如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。 底层 lucene 简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引 在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1,2,3,4,5 2 地图 1,2,3,4,5 3 之父 1,2,4,5 4 跳槽 1,4 5 Facebook 1,2,3,4,5 6 加盟 2,3,5 7 创始人 3 8 拉斯 3,5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/es-optimizing-query-performance.html":{"url":"advanced-java/docs/high-concurrency/es-optimizing-query-performance.html","title":"es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？","keywords":"","body":"面试题 es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？ 面试官心理分析 这个问题是肯定要问的，说白了，就是看你有没有实际干过 es，因为啥？其实 es 性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s，坑爹了。第一次搜索的时候，是 5~10s，后面反而就快了，可能就几百毫秒。 你就很懵，每个用户第一次访问都会比较慢，比较卡么？所以你要是没玩儿过 es，或者就是自己玩玩儿 demo，被问到这个问题容易懵逼，显示出你对 es 确实玩儿的不怎么样？ 面试题剖析 说实话，es 性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。 性能优化的杀手锏——filesystem cache 你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。 es 的搜索引擎严重依赖于底层的 filesystem cache，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。 性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒、5秒、10秒。但如果是走 filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。 这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G。每台机器给 es jvm heap 是 32G，那么剩下来留给 filesystem cache 的就是每台机器才 32G，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。 归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。 比如说你现在有一行数据。id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。 hbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。 数据预热 假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。 其实可以做数据预热。 举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。 冷热分离 es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。 你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。 document 模型设计 对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。 document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。 分页性能优化 es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。 分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。 我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。 有什么解决方案吗？ 不允许深度分页（默认深度分页性能很差） 跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。 类似于 app 里的推荐商品不断下拉出来一页一页的 类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api，关于如何使用，自行上网搜索。 scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。 除了用 scroll api，你也可以用 search_after 来做，search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/es-production-cluster.html":{"url":"advanced-java/docs/high-concurrency/es-production-cluster.html","title":"es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？","keywords":"","body":"面试题 es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？ 面试官心理分析 这个问题，包括后面的 redis 什么的，谈到 es、redis、mysql 分库分表等等技术，面试必问！就是你生产环境咋部署的？说白了，这个问题没啥技术含量，就是看你有没有在真正的生产环境里干过这事儿！ 有些同学可能是没在生产环境中干过的，没实际去拿线上机器部署过 es 集群，也没实际玩儿过，也没往 es 集群里面导入过几千万甚至是几亿的数据量，可能你就不太清楚这里面的一些生产项目中的细节。 如果你是自己就玩儿过 demo，没碰过真实的 es 集群，那你可能此时会懵。别懵，你一定要云淡风轻的回答出来这个问题，表示你确实干过这事儿。 面试题剖析 其实这个问题没啥，如果你确实干过 es，那你肯定了解你们生产 es 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？你肯定知道！ 但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。 es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。 大概就这么说一下就行了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/why-cache.html":{"url":"advanced-java/docs/high-concurrency/why-cache.html","title":"缓存","keywords":"","body":"面试题 项目中缓存是如何使用的？为什么要用缓存？缓存使用不当会造成什么后果？ 面试官心理分析 这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬。 只要问到缓存，上来第一个问题，肯定是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？ 这就是看看你对缓存这个东西背后有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答，那面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。 面试题剖析 项目中缓存是如何使用的？ 这个，需要结合自己项目的业务来。 为什么要用缓存？ 用缓存，主要有两个用途：高性能、高并发。 高性能 假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？ 缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。 就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。 高并发 mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。 所以要是你有个系统，高峰期一秒钟过来的请求有 1万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。 缓存是走内存的，内存天然就支撑高并发。 用了缓存之后会有什么不良后果？ 常见的缓存问题有以下几个： 缓存与数据库双写不一致 缓存雪崩、缓存穿透 缓存并发竞争 后面再详细说明。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/redis-single-thread-model.html":{"url":"advanced-java/docs/high-concurrency/redis-single-thread-model.html","title":"Redis 和 Memcached 有什么区别？Redis 的线程模型是什么？为什么单线程的 Redis 比多线程的 Memcached 效率要高得多？","keywords":"","body":"面试题 redis 和 memcached 有什么区别？redis 的线程模型是什么？为什么 redis 单线程却能支撑高并发？ 面试官心理分析 这个是问 redis 的时候，最基本的问题吧，redis 最基本的一个内部原理和特点，就是 redis 实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿 redis 的时候，出了问题岂不是什么都不知道？ 还有可能面试官会问问你 redis 和 memcached 的区别，但是 memcached 是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是 redis，没什么公司用 memcached 了。 面试题剖析 redis 和 memcached 有啥区别？ redis 支持复杂的数据结构 redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 redis 原生支持集群模式 在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比 由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。 redis 的线程模型 redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。 为啥 redis 单线程模型也能效率这么高？ 纯内存操作 核心是基于非阻塞的 IO 多路复用机制 单线程反而避免了多线程的频繁上下文切换问题 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/redis-data-types.html":{"url":"advanced-java/docs/high-concurrency/redis-data-types.html","title":"Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？","keywords":"","body":"面试题 redis 都有哪些数据类型？分别在哪些场景下使用比较合适？ 面试官心理分析 除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。 其实问这个问题，主要有两个原因： 看看你到底有没有全面的了解 redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作； 看看你在实际项目里都怎么玩儿过 redis。 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。 面试题剖析 redis 主要有以下几种数据类型： string hash list set sorted set string 这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。 set college szu hash 这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。 hset person name bingo hset person age 20 hset person id 1 hget person name person = { \"name\": \"bingo\", \"age\": 20, \"id\": 1 } list list 是有序列表，这个可以玩儿出很多花样。 比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 # 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。 lrange mylist 0 -1 比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。 lpush mylist 1 lpush mylist 2 lpush mylist 3 4 5 # 1 rpop mylist set set 是无序集合，自动去重。 直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 redis 进行全局的 set 去重。 可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。 把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。 #-------操作一个set------- # 添加元素 sadd mySet 1 # 查看全部元素 smembers mySet # 判断是否包含某个值 sismember mySet 3 # 删除某个/些元素 srem mySet 1 srem mySet 2 4 # 查看元素个数 scard mySet # 随机删除一个元素 spop mySet #-------操作多个set------- # 将一个set的元素移动到另外一个set smove yourSet mySet 2 # 求两set的交集 sinter yourSet mySet # 求两set的并集 sunion yourSet mySet # 求在yourSet中而不在mySet中的元素 sdiff yourSet mySet sorted set sorted set 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。 zadd board 85 zhangsan zadd board 72 lisi zadd board 96 wangwu zadd board 63 zhaoliu # 获取排名前三的用户（默认是升序，所以需要 rev 改为降序） zrevrange board 0 3 # 获取某用户的排名 zrank board zhaoliu Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/redis-expiration-policies-and-lru.html":{"url":"advanced-java/docs/high-concurrency/redis-expiration-policies-and-lru.html","title":"Redis 的过期策略都有哪些？手写一下 LRU 代码实现？","keywords":"","body":"面试题 redis 的过期策略都有哪些？内存淘汰机制都有哪些？手写一下 LRU 代码实现？ 面试官心理分析 如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？ 常见的有两个问题： 往 redis 写入的数据怎么没了？ 可能有同学会遇到，在生产环境的 redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 redis 你就没用对啊。redis 是缓存，你给当存储了是吧？ 啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。redis 主要是基于内存来进行高性能、高并发的读写操作的。 那既然内存是有限的，比如 redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。 数据明明过期了，怎么还占用着内存？ 这是由 redis 的过期策略来决定。 面试题剖析 redis 过期策略 redis 过期策略是：定期删除+惰性删除。 所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制。 内存淘汰机制 redis 内存淘汰机制有以下几个： noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 手写一个 LRU 算法 你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。 不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。 class LRUCache extends LinkedHashMap { private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) { // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; } @Override protected boolean removeEldestEntry(Map.Entry eldest) { // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() > CACHE_SIZE; } } Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/how-to-ensure-high-concurrency-and-high-availability-of-redis.html":{"url":"advanced-java/docs/high-concurrency/how-to-ensure-high-concurrency-and-high-availability-of-redis.html","title":"如何保证 Redis 高并发、高可用？Redis 的主从复制原理能介绍一下么？Redis 的哨兵原理能介绍一下么？","keywords":"","body":"面试题 如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？ 面试官心理分析 其实问这个问题，主要是考考你，redis 单机能承载多高并发？如果单机扛不住如何扩容扛更多的并发？redis 会不会挂？既然 redis 会挂那怎么保证 redis 是高可用的？ 其实针对的都是项目中你肯定要考虑的一些问题，如果你没考虑过，那确实你对生产系统中的问题思考太少。 面试题剖析 如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。 由于此节内容较多，因此，会分为两个小节进行讲解。 redis 主从架构 redis 基于哨兵实现高可用 redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/redis-persistence.html":{"url":"advanced-java/docs/high-concurrency/redis-persistence.html","title":"Redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？","keywords":"","body":"面试题 redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？ 面试官心理分析 redis 如果仅仅只是将数据缓存在内存里面，如果 redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。 如果 redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。 这个其实一样，针对的都是 redis 的生产环境可能遇到的一些问题，就是 redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？ 面试题剖析 持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 redis 整个挂了，然后 redis 就不可用了，你要做的事情就是让 redis 变得可用，尽快变得可用。 重启 redis，尽快让它对外提供服务，如果没做数据备份，这时候 redis 启动了，也不可用啊，数据都没了。 很可能说，大量的请求过来，缓存全部无法命中，在 redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了... 如果你把 redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。 redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 通过 RDB 或 AOF，都可以将 redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/redis-cluster.html":{"url":"advanced-java/docs/high-concurrency/redis-cluster.html","title":"Redis 集群模式的工作原理能说一下么？在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？如何动态增加和删除一个节点？","keywords":"","body":"面试题 redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？ 面试官心理分析 在前几年，redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis，或者 twemproxy，都有。有一些 redis 中间件，你读写 redis 中间件，redis 中间件负责将你的数据分布式存储在多台机器上的 redis 实例中。 这两年，redis 不断在发展，redis 也不断有新的版本，现在的 redis 集群模式，可以做到在多台机器上，部署多个 redis 实例，每个实例存储一部分的数据，同时每个 redis 主实例可以挂 redis 从实例，自动确保说，如果 redis 主实例挂了，会自动切换到 redis 从实例上来。 现在 redis 的新版本，大家都是用 redis cluster 的，也就是 redis 原生支持的 redis 集群模式，那么面试官肯定会就 redis cluster 对你来个几连炮。要是你没用过 redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 redis cluster 吧。 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 redis 主从架构的高可用性。 redis cluster，主要是针对海量数据+高并发+高可用的场景。redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node。这样整个 redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。 面试题剖析 redis cluster 介绍 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制 基本通信原理 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。 redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。 10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议 gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。 redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。 ping 消息深入 ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 分布式寻址算法 hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） redis cluster 的 hash slot 算法 hash 算法 来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性 hash 算法 一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。 来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 redis cluster 的 hash slot 算法 redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。 任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。 redis cluster 的高可用与主备切换原理 redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机 如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤 对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举 每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。 与哨兵比较 整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/redis-caching-avalanche-and-caching-penetration.html":{"url":"advanced-java/docs/high-concurrency/redis-caching-avalanche-and-caching-penetration.html","title":"了解什么是 redis 的雪崩、穿透和击穿？Redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 Redis 的穿透？","keywords":"","body":"面试题 了解什么是 redis 的雪崩、穿透和击穿？redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 redis 的穿透？ 面试官心理分析 其实这是问到缓存必问的，因为缓存雪崩和穿透，是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。 面试题剖析 缓存雪崩 对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。 这就是缓存雪崩。 大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。 缓存雪崩的事前事中事后的解决方案如下。 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死。 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 redis。如果 ehcache 和 redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 redis 中。 限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空白的值。 好处： 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。 缓存穿透 对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。 黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。 举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。 解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。 缓存击穿 缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 解决方式也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/redis-consistence.html":{"url":"advanced-java/docs/high-concurrency/redis-consistence.html","title":"如何保证缓存与数据库的双写一致性？","keywords":"","body":"面试题 如何保证缓存与数据库的双写一致性？ 面试官心理分析 你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 面试题剖析 一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 Cache Aside Pattern 最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 最初级的缓存不一致问题及解决方案 问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了... 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。 解决方案如下： 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题： 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每隔库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致*读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。 如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。 我们来实际粗略测算一下。 如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。 经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。 读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。 比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/redis-cas.html":{"url":"advanced-java/docs/high-concurrency/redis-cas.html","title":"Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？","keywords":"","body":"面试题 redis 的并发竞争问题是什么？如何解决这个问题？了解 redis 事务的 CAS 方案吗？ 面试官心理分析 这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。 而且 redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。 面试题剖析 某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。 你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。 每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/redis-production-environment.html":{"url":"advanced-java/docs/high-concurrency/redis-production-environment.html","title":"生产环境中的 Redis 是怎么部署的？","keywords":"","body":"面试题 生产环境中的 redis 是怎么部署的？ 面试官心理分析 看看你了解不了解你们公司的 redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 redis 给几个 G 的内存？设置了哪些参数？压测后你们 redis 集群承载多少 QPS？ 兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。 面试题剖析 redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/database-shard.html":{"url":"advanced-java/docs/high-concurrency/database-shard.html","title":"分库分表","keywords":"","body":"面试题 为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 面试官心理分析 其实这块肯定是扯到高并发了，因为分库分表一定是为了支撑高并发、数据量大两个问题的。而且现在说实话，尤其是互联网类的公司面试，基本上都会来这么一下，分库分表如此普遍的技术问题，不问实在是不行，而如果你不知道那也实在是说不过去！ 面试题剖析 为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？） 说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。 我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10。天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢...... 再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 分表 比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库 分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧。 # 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？ 这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： cobar TDDL atlas sharding-jdbc mycat cobar 阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 cobar 集群，cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 atlas 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 sharding-jdbc 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 mycat 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结 综上，现在其实建议考量的，就是 sharding-jdbc 和 mycat，这两个都可以去考虑使用。 sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 sharding-jdbc 的依赖； mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？ 水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段 hash 一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/database-shard-method.html":{"url":"advanced-java/docs/high-concurrency/database-shard-method.html","title":"现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？","keywords":"","body":"面试题 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？ 面试官心理分析 你看看，你现在已经明白为啥要分库分表了，你也知道常用的分库分表中间件了，你也设计好你们如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，你接下来该怎么把你那个单库单表的系统给迁移到分库分表上去？ 所以这都是一环扣一环的，就是看你有没有全流程经历过这个过程。 面试题剖析 这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。 停机迁移方案 我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。 接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。 但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。 双写迁移方案 这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。 导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/database-shard-dynamic-expand.html":{"url":"advanced-java/docs/high-concurrency/database-shard-dynamic-expand.html","title":"如何设计可以动态扩容缩容的分库分表方案？","keywords":"","body":"面试题 如何设计可以动态扩容缩容的分库分表方案？ 面试官心理分析 对于分库分表来说，主要是面对以下问题： 选择一个数据库中间件，调研、学习、测试； 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表； 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写； 完成单库单表到分库分表的迁移，双写方案； 线上系统开始基于分库分表对外提供服务； 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？ 这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。 那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。 这都是玩儿分库分表线上必须经历的事儿。 面试题剖析 停机扩容（不推荐） 这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。 如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。 优化后的方案 一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。 我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。 每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载32 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库。 1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。 每秒 5 万的写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。 一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。 orderId id % 32 (库) id / 32 % 32 (表) 259 3 8 1189 5 5 352 0 11 4593 17 15 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表。 这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 这里对步骤做一个总结： 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-concurrency/database-shard-global-id-generate.html":{"url":"advanced-java/docs/high-concurrency/database-shard-global-id-generate.html","title":"分库分表之后，id 主键如何处理？","keywords":"","body":"面试题 分库分表之后，id 主键如何处理？ 面试官心理分析 其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个全局唯一的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。 面试题剖析 基于数据库的实现方案 数据库自增 id 这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 设置数据库 sequence 或者表自增字段步长 可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。 比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。 适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。 UUID 好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。 适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。 UUID.randomUUID().toString().replace(“-”, “”) -> sfsdf23423rr234sfdaf 获取系统当前时间 这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。 snowflake 算法 snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 public class IdWorker { private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) { // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId > maxWorkerId || workerId maxDatacenterId || datacenterId 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。 利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/mysql-read-write-separation.html":{"url":"advanced-java/docs/high-concurrency/mysql-read-write-separation.html","title":"读写分离","keywords":"","body":"面试题 你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？ 面试官心理分析 高并发这个阶段，肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是 app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？ 面试题剖析 如何实现 MySQL 的读写分离？ 其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 MySQL 主从复制原理的是啥？ 主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 MySQL 主从同步延时问题（精华） 以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。 是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。 我们通过 MySQL 命令： show status 查看 Seconds_Behind_Master，可以看到从库复制主库的数据落后了几 ms。 一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-concurrency/high-concurrency-design.html":{"url":"advanced-java/docs/high-concurrency/high-concurrency-design.html","title":"如何设计一个高并发系统？","keywords":"","body":"面试题 如何设计一个高并发系统？ 面试官心理分析 说实话，如果面试官问你这个题目，那么你必须要使出全身吃奶劲了。为啥？因为你没看到现在很多公司招聘的 JD 里都是说啥，有高并发就经验者优先。 如果你确实有真才实学，在互联网公司里干过高并发系统，那你确实拿 offer 基本如探囊取物，没啥问题。面试官也绝对不会这样来问你，否则他就是蠢。 假设你在某知名电商公司干过高并发系统，用户上亿，一天流量几十亿，高峰期并发量上万，甚至是十万。那么人家一定会仔细盘问你的系统架构，你们系统啥架构？怎么部署的？部署了多少台机器？缓存咋用的？MQ 咋用的？数据库咋用的？就是深挖你到底是如何扛住高并发的。 因为真正干过高并发的人一定知道，脱离了业务的系统架构都是在纸上谈兵，真正在复杂业务场景而且还高并发的时候，那系统架构一定不是那么简单的，用个 redis，用 mq 就能搞定？当然不是，真实的系统架构搭配上业务之后，会比这种简单的所谓“高并发架构”要复杂很多倍。 如果有面试官问你个问题说，如何设计一个高并发系统？那么不好意思，一定是因为你实际上没干过高并发系统。面试官看你简历就没啥出彩的，感觉就不咋地，所以就会问问你，如何设计一个高并发系统？其实说白了本质就是看看你有没有自己研究过，有没有一定的知识积累。 最好的当然是招聘个真正干过高并发的哥儿们咯，但是这种哥儿们人数稀缺，不好招。所以可能次一点的就是招一个自己研究过的哥儿们，总比招一个啥也不会的哥儿们好吧！ 所以这个时候你必须得做一把个人秀了，秀出你所有关于高并发的知识！ 面试题剖析 其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？ 我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。 当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。 所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。 那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题： 可以分为以下 6 点： 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch 系统拆分 将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。 缓存 缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。 MQ MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。 分库分表 分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。 读写分离 读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。 ElasticSearch Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。 上面的 6 点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。 说句实话，毕竟你真正厉害的一点，不是在于弄明白一些技术，或者大概知道一个高并发系统应该长什么样？其实实际上在真正的复杂的业务系统里，做高并发要远远比上面提到的点要复杂几十倍到上百倍。你需要考虑：哪些需要分库分表，哪些不需要分库分表，单库单表跟分库分表如何 join，哪些数据要放到缓存里去，放哪些数据才可以扛住高并发的请求，你需要完成对一个复杂业务系统的分析之后，然后逐步逐步的加入高并发的系统架构的改造，这个过程是无比复杂的，一旦做过一次，并且做好了，你在这个市场上就会非常的吃香。 其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，你了解了，也只能是次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/distributed-system-interview.html":{"url":"advanced-java/docs/distributed-system/distributed-system-interview.html","title":"面试连环炮","keywords":"","body":"分布式系统面试连环炮 有一些同学，之前呢主要是做传统行业，或者外包项目，一直是在那种小的公司，技术一直都搞的比较简单。他们有共同的一个问题，就是都没怎么搞过分布式系统，现在互联网公司，一般都是做分布式的系统，大家都不是做底层的分布式系统、分布式存储系统 hadoop hdfs、分布式计算系统 hadoop mapreduce / spark、分布式流式计算系统 storm。 分布式业务系统，就是把原来用 Java 开发的一个大块系统，给拆分成多个子系统，多个子系统之间互相调用，形成一个大系统的整体。假设原来你做了一个 OA 系统，里面包含了权限模块、员工模块、请假模块、财务模块，一个工程，里面包含了一堆模块，模块与模块之间会互相去调用，1 台机器部署。现在如果你把这个系统给拆开，权限系统、员工系统、请假系统、财务系统 4 个系统，4 个工程，分别在 4 台机器上部署。一个请求过来，完成这个请求，这个员工系统，调用权限系统，调用请假系统，调用财务系统，4 个系统分别完成了一部分的事情，最后 4 个系统都干完了以后，才认为是这个请求已经完成了。 这两年开始兴起和流行 Spring Cloud，刚流行，还没开始普及，目前普及的是 dubbo，因此这里也主要讲 dubbo。 面试官可能会问你以下问题。 为什么要进行系统拆分？ 为什么要进行系统拆分？如何进行系统拆分？拆分后不用dubbo可以吗？dubbo和thrift有什么区别呢？分布式服务框架 说一下的 dubbo 的工作原理？注册中心挂了可以继续通信吗？ dubbo 支持哪些序列化协议？说一下 hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ dubbo 负载均衡策略和高可用策略都有哪些？动态代理策略呢？ dubbo 的 spi 思想是什么？ 如何基于 dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 分布式服务接口请求的顺序性如何保证？ 如何自己设计一个类似 dubbo 的 rpc 框架？ 分布式锁 使用 redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？ 分布式事务 分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？ 分布式会话 集群部署时的分布式 session 如何实现？ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:10 "},"advanced-java/docs/distributed-system/why-dubbo.html":{"url":"advanced-java/docs/distributed-system/why-dubbo.html","title":"为什么要进行系统拆分？如何进行系统拆分？拆分后不用 Dubbo 可以吗？","keywords":"","body":"面试题 为什么要进行系统拆分？如何进行系统拆分？拆分后不用 dubbo 可以吗？ 面试官心理分析 从这个问题开始就进行分布式系统环节了，现在出去面试分布式都成标配了，没有哪个公司不问问你分布式的事儿。你要是不会分布式的东西，简直这简历没法看，没人会让你去面试。 其实为啥会这样呢？这就是因为整个大行业技术发展的原因。 早些年，印象中在 2010 年初的时候，整个 IT 行业，很少有人谈分布式，更不用说微服务，虽然很多 BAT 等大型公司，因为系统的复杂性，很早就是分布式架构，大量的服务，只不过微服务大多基于自己搞的一套框架来实现而已。 但是确实，那个年代，大家很重视 ssh2，很多中小型公司几乎大部分都是玩儿 struts2、spring、hibernate，稍晚一些，才进入了 spring mvc、spring、mybatis 的组合。那个时候整个行业的技术水平就是那样，当年 oracle 很火，oracle 管理员很吃香，oracle 性能优化啥的都是 IT 男的大杀招啊。连大数据都没人提，当年 OCP、OCM 等认证培训机构，火的不行。 但是确实随着时代的发展，慢慢的，很多公司开始接受分布式系统架构了，这里面尤为对行业有至关重要影响的，是阿里的 dubbo，某种程度上而言，阿里在这里推动了行业技术的前进。 正是因为有阿里的 dubbo，很多中小型公司才可以基于 dubbo，来把系统拆分成很多的服务，每个人负责一个服务，大家的代码都没有冲突，服务可以自治，自己选用什么技术都可以，每次发布如果就改动一个服务那就上线一个服务好了，不用所有人一起联调，每次发布都是几十万行代码，甚至几百万行代码了。 直到今日，很高兴看到分布式系统都成行业面试标配了，任何一个普通的程序员都该掌握这个东西，其实这是行业的进步，也是所有 IT 码农的技术进步。所以既然分布式都成标配了，那么面试官当然会问了，因为很多公司现在都是分布式、微服务的架构，那面试官当然得考察考察你了。 面试题剖析 为什么要将系统进行拆分？ 网上查查，答案极度零散和复杂，很琐碎，原因一大坨。但是我这里给大家直观的感受： 要是不拆分，一个大系统几十万行代码，20 个人维护一份代码，简直是悲剧啊。代码经常改着改着就冲突了，各种代码冲突和合并要处理，非常耗费时间；经常我改动了我的代码，你调用了我的，导致你的代码也得重新测试，麻烦的要死；然后每次发布都是几十万行代码的系统一起发布，大家得一起提心吊胆准备上线，几十万行代码的上线，可能每次上线都要做很多的检查，很多异常问题的处理，简直是又麻烦又痛苦；而且如果我现在打算把技术升级到最新的 spring 版本，还不行，因为这可能导致你的代码报错，我不敢随意乱改技术。 假设一个系统是 20 万行代码，其中 A 在里面改了 1000 行代码，但是此时发布的时候是这个 20 万行代码的大系统一块儿发布。就意味着 20 万上代码在线上就可能出现各种变化，20 个人，每个人都要紧张地等在电脑面前，上线之后，检查日志，看自己负责的那一块儿有没有什么问题。 A 就检查了自己负责的 1 万行代码对应的功能，确保 ok 就闪人了；结果不巧的是，A 上线的时候不小心修改了线上机器的某个配置，导致另外 B 和 C 负责的 2 万行代码对应的一些功能，出错了。 几十个人负责维护一个几十万行代码的单块应用，每次上线，准备几个礼拜，上线 -> 部署 -> 检查自己负责的功能。 拆分了以后，整个世界清爽了，几十万行代码的系统，拆分成 20 个服务，平均每个服务就 1~2 万行代码，每个服务部署到单独的机器上。20 个工程，20 个 git 代码仓库，20 个开发人员，每个人维护自己的那个服务就可以了，是自己独立的代码，跟别人没关系。再也没有代码冲突了，爽。每次就测试我自己的代码就可以了，爽。每次就发布我自己的一个小服务就可以了，爽。技术上想怎么升级就怎么升级，保持接口不变就可以了，真爽。 所以简单来说，一句话总结，如果是那种代码量多达几十万行的中大型项目，团队里有几十个人，那么如果不拆分系统，开发效率极其低下，问题很多。但是拆分系统之后，每个人就负责自己的一小部分就好了，可以随便玩儿随便弄。分布式系统拆分之后，可以大幅度提升复杂系统大型团队的开发效率。 但是同时，也要提醒的一点是，系统拆分成分布式系统之后，大量的分布式系统面临的问题也是接踵而来，所以后面的问题都是在围绕分布式系统带来的复杂技术挑战在说。 如何进行系统拆分？ 这个问题说大可以很大，可以扯到领域驱动模型设计上去，说小了也很小，我不太想给大家太过于学术的说法，因为你也不可能背这个答案，过去了直接说吧。还是说的简单一点，大家自己到时候知道怎么回答就行了。 系统拆分为分布式系统，拆成多个服务，拆成微服务的架构，是需要拆很多轮的。并不是说上来一个架构师一次就给拆好了，而以后都不用拆。 第一轮；团队继续扩大，拆好的某个服务，刚开始是 1 个人维护 1 万行代码，后来业务系统越来越复杂，这个服务是 10 万行代码，5 个人；第二轮，1个服务 -> 5个服务，每个服务 2 万行代码，每人负责一个服务。 如果是多人维护一个服务，最理想的情况下，几十个人，1 个人负责 1 个或 2~3 个服务；某个服务工作量变大了，代码量越来越多，某个同学，负责一个服务，代码量变成了 10 万行了，他自己不堪重负，他现在一个人拆开，5 个服务，1 个人顶着，负责 5 个人，接着招人，2 个人，给那个同学带着，3 个人负责 5 个服务，其中 2 个人每个人负责 2 个服务，1 个人负责 1 个服务。 个人建议，一个服务的代码不要太多，1 万行左右，两三万撑死了吧。 大部分的系统，是要进行多轮拆分的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。 但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了供应商管理系统、采购单管理系统，订单系统又拆分成了购物车系统、价格系统、订单管理系统。 扯深了实在很深，所以这里先给大家举个例子，你自己感受一下，核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆。你根据自己负责系统的例子，来考虑一下就好了。 拆分后不用 dubbo 可以吗？ 当然可以了，大不了最次，就是各个系统之间，直接基于 spring mvc，就纯 http 接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为 http 接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了 5 台机器，你怎么把请求均匀地甩给那 5 台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。 所以 dubbo 说白了，是一种 rpc 框架，就是说本地就是进行接口调用，但是 dubbo 会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡、服务实例上下线自动感知、超时重试等等乱七八糟的问题。那你就不用自己做了，用 dubbo 就可以了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/distributed-system/dubbo-operating-principle.html":{"url":"advanced-java/docs/distributed-system/dubbo-operating-principle.html","title":"说一下 Dubbo 的工作原理？注册中心挂了可以继续通信吗？","keywords":"","body":"面试题 说一下的 dubbo 的工作原理？注册中心挂了可以继续通信吗？说说一次 rpc 请求的流程？ 面试官心理分析 MQ、ES、Redis、Dubbo，上来先问你一些思考性的问题、原理，比如 kafka 高可用架构原理、es 分布式架构原理、redis 线程模型原理、Dubbo 工作原理；之后就是生产环境里可能会碰到的一些问题，因为每种技术引入之后生产环境都可能会碰到一些问题；再来点综合的，就是系统设计，比如让你设计一个 MQ、设计一个搜索引擎、设计一个缓存、设计一个 rpc 框架等等。 那既然开始聊分布式系统了，自然重点先聊聊 dubbo 了，毕竟 dubbo 是目前事实上大部分公司的分布式系统的 rpc 框架标准，基于 dubbo 也可以构建一整套的微服务架构。但是需要自己大量开发。 当然去年开始 spring cloud 非常火，现在大量的公司开始转向 spring cloud 了，spring cloud 人家毕竟是微服务架构的全家桶式的这么一个东西。但是因为很多公司还在用 dubbo，所以 dubbo 肯定会是目前面试的重点，何况人家 dubbo 现在重启开源社区维护了，捐献给了 apache，未来应该也还是有一定市场和地位的。 既然聊 dubbo，那肯定是先从 dubbo 原理开始聊了，你先说说 dubbo 支撑 rpc 分布式调用的架构啥的，然后说说一次 rpc 请求 dubbo 是怎么给你完成的，对吧。 面试题剖析 dubbo 工作原理 第一层：service 层，接口层，给服务提供者和消费者来实现的 第二层：config 层，配置层，主要是对 dubbo 进行各种配置的 第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信 第四层：registry 层，服务注册层，负责服务的注册与发现 第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控 第七层：protocal 层，远程调用层，封装 rpc 调用 第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步 第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口 第十层：serialize 层，数据序列化层 工作流程 第一步：provider 向注册中心去注册 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务 第三步：consumer 调用 provider 第四步：consumer 和 provider 都异步通知监控中心 注册中心挂了可以继续通信吗？ 可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/dubbo-serialization-protocol.html":{"url":"advanced-java/docs/distributed-system/dubbo-serialization-protocol.html","title":"Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？","keywords":"","body":"面试题 dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ 面试官心理分析 上一个问题，说说 dubbo 的基本工作原理，那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。 接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时 RPC 的时候怎么走的？ 面试题剖析 序列化，就是把数据结构或者是一些对象，转换为二进制串的过程，而反序列化是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。 dubbo 支持不同的通信协议 dubbo 协议 默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。 为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。 长连接，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。 而短连接，每次要发送请求之前，需要先重新建立一次连接。 rmi 协议 走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hessian 协议 走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。 http 协议 走 json 序列化。 webservice 走 SOAP 文本序列化。 dubbo 支持的序列化协议 dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。 说一下 Hessian 的数据结构 Hessian 的对象序列化机制有 8 种原始类型： 原始二进制数据 boolean 64-bit date（64 位毫秒值的日期） 64-bit double 32-bit int 64-bit long null UTF-8 编码的 string 另外还包括 3 种递归类型： list for lists and arrays map for maps and dictionaries object for objects 还有一种特殊的类型： ref：用来表示对共享对象的引用。 为什么 PB 的效率是最高的？ 可能有一些同学比较习惯于 JSON or XML 数据存储格式，对于 Protocol Buffer 还比较陌生。Protocol Buffer 其实是 Google 出品的一种轻量并且高效的结构化数据存储格式，性能比 JSON、XML 要高很多。 其实 PB 之所以性能如此好，主要得益于两个：第一，它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍；第二，它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:10 "},"advanced-java/docs/distributed-system/dubbo-load-balancing.html":{"url":"advanced-java/docs/distributed-system/dubbo-load-balancing.html","title":"Dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？","keywords":"","body":"面试题 dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？ 面试官心理分析 继续深问吧，这些都是用 dubbo 必须知道的一些东西，你得知道基本原理，知道序列化是什么协议，还得知道具体用 dubbo 的时候，如何负载均衡，如何高可用，如何动态代理。 说白了，就是看你对 dubbo 熟悉不熟悉： dubbo 工作原理：服务注册、注册中心、消费者、代理通信、负载均衡； 网络通信、序列化：dubbo 协议、长连接、NIO、hessian 序列化协议； 负载均衡策略、集群容错策略、动态代理策略：dubbo 跑起来的时候一些功能是如何运转的？怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ dubbo SPI 机制：你了解不了解 dubbo 的 SPI 机制？如何基于 SPI 机制对 dubbo 进行扩展？ 面试题剖析 dubbo 负载均衡策略 random loadbalance 默认情况下，dubbo 是 random load balance ，即随机调用实现负载均衡，可以对 provider 不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。 roundrobin loadbalance 这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。 举个栗子。 跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。 这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。 leastactive loadbalance 这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求。 consistanthash loadbalance 一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。 dubbo 集群容错策略 failover cluster 模式 失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器） failfast cluster模式 一次调用失败就立即失败，常见于写操作。（调用失败就立即失败） failsafe cluster 模式 出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。 failback cluster 模式 失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。 forking cluster 模式 并行调用多个 provider，只要一个成功就立即返回。 broadcacst cluster 逐个调用所有的 provider。 dubbo动态代理策略 默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/distributed-system/dubbo-spi.html":{"url":"advanced-java/docs/distributed-system/dubbo-spi.html","title":"Dubbo 的 spi 思想是什么？","keywords":"","body":"面试题 dubbo 的 spi 思想是什么？ 面试官心理分析 继续深入问呗，前面一些基础性的东西问完了，确定你应该都 ok，了解 dubbo 的一些基本东西，那么问个稍微难一点点的问题，就是 spi，先问问你 spi 是啥？然后问问你 dubbo 的 spi 是怎么实现的？ 其实就是看看你对 dubbo 的掌握如何。 面试题剖析 spi 是啥？ spi，简单来说，就是 service provider interface，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。 举个栗子。 你有一个接口 A。A1/A2/A3 分别是接口A的不同实现。你通过配置 接口 A = 实现 A2，那么在系统实际运行的时候，会加载你的配置，用实现 A2 实例化一个对象来提供服务。 spi 机制一般用在哪儿？插件扩展的场景，比如说你开发了一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面，从而扩展某个功能，这个时候 spi 思想就用上了。 Java spi 思想的体现 spi 经典的思想体现，大家平时都在用，比如说 jdbc。 Java 定义了一套 jdbc 的接口，但是 Java 并没有提供 jdbc 的实现类。 但是实际上项目跑的时候，要使用 jdbc 接口的哪些实现类呢？一般来说，我们要根据自己使用的数据库，比如 mysql，你就将 mysql-jdbc-connector.jar 引入进来；oracle，你就将 oracle-jdbc-connector.jar 引入进来。 在系统跑的时候，碰到你使用 jdbc 的接口，他会在底层使用你引入的那个 jar 中提供的实现类。 dubbo 的 spi 思想 dubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。 Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol 接口，在系统运行的时候，，dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。 它会去找一个你配置的 Protocol，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。 上面那行代码就是 dubbo 里大量使用的，就是对很多组件，都是保留一个接口和多个实现，然后在系统运行的时候动态根据配置去找到对应的实现类。如果你没配置，那就走默认的实现好了，没问题。 @SPI(\"dubbo\") public interface Protocol { int getDefaultPort(); @Adaptive Exporter export(Invoker invoker) throws RpcException; @Adaptive Invoker refer(Class type, URL url) throws RpcException; void destroy(); } 在 dubbo 自己的 jar 里，在/META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol文件中： dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol http=com.alibaba.dubbo.rpc.protocol.http.HttpProtocol hessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol 所以说，这就看到了 dubbo 的 spi 机制默认是怎么玩儿的了，其实就是 Protocol 接口，@SPI(\"dubbo\") 说的是，通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol。 如果想要动态替换掉默认的实现类，需要使用 @Adaptive 接口，Protocol 接口中，有两个方法加了 @Adaptive 注解，就是说那俩接口会被代理实现。 啥意思呢？ 比如这个 Protocol 接口搞了俩 @Adaptive 注解标注了方法，在运行的时候会针对 Protocol 生成代理类，这个代理类的那俩方法里面会有代理代码，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，你也可以自己指定，你如果指定了别的 key，那么就会获取别的实现类的实例了。 如何自己扩展 dubbo 中的组件 下面来说说怎么来自己扩展 dubbo 中的组件。 自己写个工程，要是那种可以打成 jar 包的，里面的 src/main/resources 目录下，搞一个 META-INF/services，里面放个文件叫：com.alibaba.dubbo.rpc.Protocol，文件里搞一个my=com.bingo.MyProtocol。自己把 jar 弄到 nexus 私服里去。 然后自己搞一个 dubbo provider 工程，在这个工程里面依赖你自己搞的那个 jar，然后在 spring 配置文件里给个配置： provider 启动的时候，就会加载到我们 jar 包里的my=com.bingo.MyProtocol 这行配置里，接着会根据你的配置使用你定义好的 MyProtocol 了，这个就是简单说明一下，你通过上述方式，可以替换掉大量的 dubbo 内部的组件，就是扔个你自己的 jar 包，然后配置一下即可。 dubbo 里面提供了大量的类似上面的扩展点，就是说，你如果要扩展一个东西，只要自己写个 jar，让你的 consumer 或者是 provider 工程，依赖你的那个 jar，在你的 jar 里指定目录下配置好接口名称对应的文件，里面通过 key=实现类。 然后对于对应的组件，类似 用你的那个 key 对应的实现类来实现某个接口，你可以自己去扩展 dubbo 的各种功能，提供你自己的实现。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/dubbo-service-management.html":{"url":"advanced-java/docs/distributed-system/dubbo-service-management.html","title":"如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？","keywords":"","body":"面试题 如何基于 dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 面试官心理分析 服务治理，这个问题如果问你，其实就是看看你有没有服务治理的思想，因为这个是做过复杂微服务的人肯定会遇到的一个问题。 服务降级，这个是涉及到复杂分布式系统中必备的一个话题，因为分布式系统互相来回调用，任何一个系统故障了，你不降级，直接就全盘崩溃？那就太坑爹了吧。 失败重试，分布式系统中网络请求如此频繁，要是因为网络问题不小心失败了一次，是不是要重试？ 超时重试，跟上面一样，如果不小心网络慢一点，超时了，如何重试？ 面试题剖析 服务治理 1. 调用链路自动生成 一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。 那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。 2. 服务访问压力以及时长统计 需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少； 第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。 这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。 3. 其它 服务分层（避免循环依赖） 调用链路失败监控和报警 服务鉴权 每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%） 服务降级 比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。 举个栗子，我们有接口 HelloService。HelloServiceImpl 有该接口的具体实现。 public interface HelloService { void sayHello(); } public class HelloServiceImpl implements HelloService { public void sayHello() { System.out.println(\"hello world......\"); } } 我们调用接口失败的时候，可以通过 mock 统一返回 null。 mock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+Mock” 后缀。然后在 Mock 类里实现自己的降级逻辑。 public class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑 } } 失败重试和超时重试 所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下： 举个栗子。 某个服务的接口，要耗费 5s，你这边不能干等着，你这边配置了 timeout 之后，我等待 2s，还没返回，我直接就撤了，不能干等你。 可以结合你们公司具体的场景来说说你是怎么设置这些参数的： timeout：一般设置为 200ms，我们认为不能超过 200ms 还没返回。 retries：设置 retries，一般是在读请求的时候，比如你要查询个数据，你可以设置个 retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/distributed-system-idempotency.html":{"url":"advanced-java/docs/distributed-system/distributed-system-idempotency.html","title":"分布式服务接口的幂等性如何设计（比如不能重复扣款）？","keywords":"","body":"面试题 分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 面试官心理分析 从这个问题开始，面试官就已经进入了实际的生产问题的面试了。 一个分布式系统中的某个接口，该如何保证幂等性？这个事儿其实是你做分布式系统的时候必须要考虑的一个生产环境的技术问题。啥意思呢？ 你看，假如你有个服务提供一些接口供外部调用，这个服务部署在了 5 台机器上，接着有个接口就是付款接口。然后人家用户在前端上操作的时候，不知道为啥，总之就是一个订单不小心发起了两次支付请求，然后这俩请求分散在了这个服务部署的不同的机器上，好了，结果一个订单扣款扣两次。 或者是订单系统调用支付系统进行支付，结果不小心因为网络超时了，然后订单系统走了前面我们看到的那个重试机制，咔嚓给你重试了一把，好，支付系统收到一个支付请求两次，而且因为负载均衡算法落在了不同的机器上，尴尬了。。。 所以你肯定得知道这事儿，否则你做出来的分布式系统恐怕容易埋坑。 面试题剖析 这个不是技术问题，这个没有通用的一个方法，这个应该结合业务来保证幂等性。 所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款、不能多插入一条数据、不能将统计值多加了 1。这就是幂等性。 其实保证幂等性主要是三点： 对于每个请求必须有一个唯一的标识，举个栗子：订单支付请求，肯定得包含订单 id，一个订单 id 最多支付一次，对吧。 每次处理完请求之后，必须有一个记录标识这个请求处理过了。常见的方案是在 mysql 中记录个状态啥的，比如支付之前记录一条这个订单的支付流水。 每次接收请求需要进行判断，判断之前是否处理过。比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId 已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。 实际运作过程中，你要结合自己的业务来，比如说利用 redis，用 orderId 作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。 要求是支付一个订单，必须插入一条支付流水，order_id 建一个唯一键 unique key。你在支付一个订单之前，先插入一条支付流水，order_id 就已经进去了。你就可以写一个标识到 redis 里面去，set order_id payed，下一次重复请求过来了，先查 redis 的 order_id 对应的 value，如果是 payed 就说明已经支付过了，你就别重复支付了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/distributed-system/distributed-system-request-sequence.html":{"url":"advanced-java/docs/distributed-system/distributed-system-request-sequence.html","title":"分布式服务接口请求的顺序性如何保证？","keywords":"","body":"面试题 分布式服务接口请求的顺序性如何保证？ 面试官心理分析 其实分布式系统接口的调用顺序，也是个问题，一般来说是不用保证顺序的。但是有时候可能确实是需要严格的顺序保证。给大家举个例子，你服务 A 调用服务 B，先插入再删除。好，结果俩请求过去了，落在不同机器上，可能插入请求因为某些原因执行慢了一些，导致删除请求先执行了，此时因为没数据所以啥效果也没有；结果这个时候插入请求过来了，好，数据插入进去了，那就尴尬了。 本来应该是 “先插入 -> 再删除”，这条数据应该没了，结果现在 “先删除 -> 再插入”，数据还存在，最后你死都想不明白是怎么回事。 所以这都是分布式系统一些很常见的问题。 面试题剖析 首先，一般来说，个人建议是，你们从业务逻辑上设计的这个系统最好是不需要这种顺序性的保证，因为一旦引入顺序性保障，比如使用分布式锁，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大等问题。 下面我给个我们用过的方案吧，简单来说，首先你得用 dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。 但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案......曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？ 最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是什么，避免这种问题的产生。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/dubbo-rpc-design.html":{"url":"advanced-java/docs/distributed-system/dubbo-rpc-design.html","title":"如何自己设计一个类似 Dubbo 的 RPC 框架？","keywords":"","body":"面试题 如何自己设计一个类似 Dubbo 的 RPC 框架？ 面试官心理分析 说实话，就这问题，其实就跟问你如何自己设计一个 MQ 一样的道理，就考两个： 你有没有对某个 rpc 框架原理有非常深入的理解。 你能不能从整体上来思考一下，如何设计一个 rpc 框架，考考你的系统设计能力。 面试题剖析 其实问到你这问题，你起码不能认怂，因为是知识的扫盲，那我不可能给你深入讲解什么 kafka 源码剖析，dubbo 源码剖析，何况我就算讲了，你要真的消化理解和吸收，起码个把月以后了。 所以我给大家一个建议，遇到这类问题，起码从你了解的类似框架的原理入手，自己说说参照 dubbo 的原理，你来设计一下，举个例子，dubbo 不是有那么多分层么？而且每个分层是干啥的，你大概是不是知道？那就按照这个思路大致说一下吧，起码你不能懵逼，要比那些上来就懵，啥也说不出来的人要好一些。 举个栗子，我给大家说个最简单的回答思路： 上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信心，可以用 zookeeper 来做，对吧。 然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。 接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。 然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。 接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。 服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。 这就是一个最最基本的 rpc 框架的思路，先不说你有多牛逼的技术功底，哪怕这个最简单的思路你先给出来行不行？ Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/distributed-system/zookeeper-application-scenarios.html":{"url":"advanced-java/docs/distributed-system/zookeeper-application-scenarios.html","title":"Zookeeper 都有哪些应用场景？","keywords":"","body":"面试题 zookeeper 都有哪些使用场景？ 面试官心理分析 现在聊的 topic 是分布式系统，面试官跟你聊完了 dubbo 相关的一些问题之后，已经确认你对分布式服务框架/RPC框架基本都有一些认知了。那么他可能开始要跟你聊分布式相关的其它问题了。 分布式锁这个东西，很常用的，你做 Java 系统开发，分布式系统，可能会有一些场景会用到。最常用的分布式锁就是基于 zookeeper 来实现的。 其实说实话，问这个问题，一般就是看看你是否了解 zookeeper，因为 zookeeper 是分布式系统中很常见的一个基础系统。而且问的话常问的就是说 zookeeper 的使用场景是什么？看你知道不知道一些基本的使用场景。但是其实 zookeeper 挖深了自然是可以问的很深很深的。 面试题剖析 大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了： 分布式协调 分布式锁 元数据/配置信息管理 HA高可用性 分布式协调 这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。 分布式锁 举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。 元数据/配置信息管理 zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？ HA高可用性 这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/distributed-lock-redis-vs-zookeeper.html":{"url":"advanced-java/docs/distributed-system/distributed-lock-redis-vs-zookeeper.html","title":"使用 Redis 如何设计分布式锁？使用 Zookeeper 来设计分布式锁可以吗？以上两种分布式锁的实现方式哪种效率比较高？","keywords":"","body":"面试题 一般实现分布式锁都有哪些方式？使用 redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？ 面试官心理分析 其实一般问问题，都是这么问的，先问问你 zk，然后其实是要过度到 zk 关联的一些问题里去，比如分布式锁。因为在分布式系统开发中，分布式锁的使用场景还是很常见的。 面试题剖析 redis 分布式锁 官方叫做 RedLock 算法，是 redis 官方支持的分布式锁算法。 这个分布式锁有 3 个重要的考量点： 互斥（只能有一个客户端获取锁） 不能死锁 容错（只要大部分 redis 节点创建了这把锁就可以） redis 最普通的分布式锁 第一个最普通的实现方式，就是在 redis 里创建一个 key，这样就算加锁。 SET my:lock 随机值 NX PX 30000 执行这个命令就 ok。 NX：表示只有 key 不存在的时候才会设置成功。（如果此时 redis 中存在这个 key，那么设置失败，返回 nil） PX 30000：意思是 30s 后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。 释放锁就是删除 key ，但是一般可以用 lua 脚本删除，判断 value 一样才删除： -- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 为啥要用随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，比如说超过了 30s，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除 key 的话会有问题，所以得用随机值加上面的 lua 脚本来释放锁。 但是这样是肯定不行的。因为如果是普通的 redis 单实例，那就是单点故障。或者是 redis 普通主从，那 redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。 RedLock 算法 这个场景是假设有一个 redis cluster，有 5 个 redis master 实例。然后执行如下步骤获取一把锁： 获取当前时间戳，单位是毫秒； 跟上面类似，轮流尝试在每个 master 节点上创建锁，过期时间较短，一般就几十毫秒； 尝试在大多数节点上建立一个锁，比如 5 个节点就要求是 3 个节点 n / 2 + 1； 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了； 要是锁建立失败了，那么就依次之前建立过的锁删除； 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。 zk 分布式锁 zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。 /** * ZooKeeperSession * * @author bingo * @since 2018/11/29 * */ public class ZooKeeperSession { private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; private CountDownLatch latch; public ZooKeeperSession() { try { this.zookeeper = new ZooKeeper(\"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\", 50000, new ZooKeeperWatcher()); try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"ZooKeeper session established......\"); } catch (Exception e) { e.printStackTrace(); } } /** * 获取分布式锁 * * @param productId */ public Boolean acquireDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; try { zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch (Exception e) { while (true) { try { // 相当于是给node注册一个监听器，去看看这个监听器是否存在 Stat stat = zk.exists(path, true); if (stat != null) { this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } zookeeper.create(path, \"\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch (Exception ee) { continue; } } } return true; } /** * 释放掉一个分布式锁 * * @param productId */ public void releaseDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; try { zookeeper.delete(path, -1); System.out.println(\"release the lock for product[id=\" + productId + \"]......\"); } catch (Exception e) { e.printStackTrace(); } } /** * 建立zk session的watcher * * @author bingo * @since 2018/11/29 * */ private class ZooKeeperWatcher implements Watcher { public void process(WatchedEvent event) { System.out.println(\"Receive watched event: \" + event.getState()); if (KeeperState.SyncConnected == event.getState()) { connectedSemaphore.countDown(); } if (this.latch != null) { this.latch.countDown(); } } } /** * 封装单例的静态内部类 * * @author bingo * @since 2018/11/29 * */ private static class Singleton { private static ZooKeeperSession instance; static { instance = new ZooKeeperSession(); } public static ZooKeeperSession getInstance() { return instance; } } /** * 获取单例 * * @return */ public static ZooKeeperSession getInstance() { return Singleton.getInstance(); } /** * 初始化单例的便捷方法 */ public static void init() { getInstance(); } } 也可以采用另一种方式，创建临时顺序节点： 如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁；后面的每个人都会去监听排在自己前面的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 zookeeper 给通知，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。 public class ZooKeeperDistributedLock implements Watcher { private ZooKeeper zk; private String locksRoot = \"/locks\"; private String productId; private String waitNode; private String lockNode; private CountDownLatch latch; private CountDownLatch connectedLatch = new CountDownLatch(1); private int sessionTimeout = 30000; public ZooKeeperDistributedLock(String productId) { this.productId = productId; try { String address = \"192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\"; zk = new ZooKeeper(address, sessionTimeout, this); connectedLatch.await(); } catch (IOException e) { throw new LockException(e); } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public void process(WatchedEvent event) { if (event.getState() == KeeperState.SyncConnected) { connectedLatch.countDown(); return; } if (this.latch != null) { this.latch.countDown(); } } public void acquireDistributedLock() { try { if (this.tryLock()) { return; } else { waitForLock(waitNode, sessionTimeout); } } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public boolean tryLock() { try { // 传入进去的locksRoot + “/” + productId // 假设productId代表了一个商品id，比如说1 // locksRoot = locks // /locks/10000000000，/locks/10000000001，/locks/10000000002 lockNode = zk.create(locksRoot + \"/\" + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 看看刚创建的节点是不是最小的节点 // locks：10000000000，10000000001，10000000002 List locks = zk.getChildren(locksRoot, false); Collections.sort(locks); if(lockNode.equals(locksRoot+\"/\"+ locks.get(0))){ //如果是最小的节点,则表示取得锁 return true; } //如果不是最小的节点，找到比自己小1的节点 int previousLockIndex = -1; for(int i = 0; i redis 分布式锁和 zk 分布式锁的对比 redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。 另外一点就是，如果是 redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。 redis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等......zk 的分布式锁语义清晰实现简单。 所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 redis 的分布式锁牢靠、而且模型简单易用。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:50:48 "},"advanced-java/docs/distributed-system/distributed-transaction.html":{"url":"advanced-java/docs/distributed-system/distributed-transaction.html","title":"分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？","keywords":"","body":"面试题 分布式事务了解吗？你们是如何解决分布式事务问题的？ 面试官心理分析 只要聊到你做了分布式系统，必问分布式事务，你对分布式事务一无所知的话，确实会很坑，你起码得知道有哪些方案，一般怎么来做，每个方案的优缺点是什么。 现在面试，分布式系统成了标配，而分布式系统带来的分布式事务也成了标配了。因为你做系统肯定要用事务吧，如果是分布式系统，肯定要用分布式事务吧。先不说你搞过没有，起码你得明白有哪几种方案，每种方案可能有啥坑？比如 TCC 方案的网络问题、XA 方案的一致性问题。 面试题剖析 分布式事务的实现主要有以下 5 种方案： XA 方案 TCC 方案 本地消息表 可靠消息最终一致性方案 最大努力通知方案 两阶段提交方案/XA方案 所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不 ok，那么就回滚事务。 这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定，自己随便搜个 demo 看看就知道了。 这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。 如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。 如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。 TCC 方案 TCC 的全称是：Try、Confirm、Cancel。 Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。 Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。 Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚） 这种方案说实话几乎很少人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。 比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用 TCC，严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，保证在资金上不会出现问题。 而且最好是你的各个业务执行的时间都比较短。 但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。 本地消息表 本地消息表其实是国外的 ebay 搞出来的这么一套思想。 这个大概意思是这样的： A 系统在自己本地一个事务里操作同时，插入一条数据到消息表； 接着 A 系统将这个消息发送到 MQ 中去； B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息； B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态； 如果 B 系统处理失败了，那么就不会更新消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理； 这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。 这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用。 可靠消息最终一致性方案 这个的意思，就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。 大概的意思就是： A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了； 如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息； 如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务； mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。 这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的。 最大努力通知方案 这个方案的大致意思就是： 系统 A 本地事务执行完之后，发送个消息到 MQ； 这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口； 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。 你们公司是如何处理分布式事务的？ 如果你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。 你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。 友情提示一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。 当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于 RocketMQ 来玩儿。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/distributed-system/distributed-session.html":{"url":"advanced-java/docs/distributed-system/distributed-session.html","title":"集群部署时的分布式 Session 如何实现？","keywords":"","body":"面试题 集群部署时的分布式 session 如何实现？ 面试官心理分析 面试官问了你一堆 dubbo 是怎么玩儿的，你会玩儿 dubbo 就可以把单块系统弄成分布式系统，然后分布式之后接踵而来的就是一堆问题，最大的问题就是分布式事务、接口幂等性、分布式锁，还有最后一个就是分布式 session。 当然了，分布式系统中的问题何止这么一点，非常之多，复杂度很高，这里只是说一下常见的几个问题，也是面试的时候常问的几个。 面试题剖析 session 是啥？浏览器有个 cookie，在一段时间内这个 cookie 都存在，然后每次发请求过来都带上一个特殊的 jsessionid cookie，就根据这个东西，在服务端可以维护一个对应的 session 域，里面可以放点数据。 一般的话只要你没关掉浏览器，cookie 还在，那么对应的那个 session 就在，但是如果 cookie 没了，session 也就没了。常见于什么购物车之类的东西，还有登录状态保存之类的。 这个不多说了，懂 Java 的都该知道这个。 单块系统的时候这么玩儿 session 没问题，但是你要是分布式系统呢，那么多的服务，session 状态在哪儿维护啊？ 其实方法很多，但是常见常用的是以下几种： 完全不用 session 使用 JWT Token 储存用户身份，然后再从数据库或者 cache 中获取其他的信息。这样无论请求分配到哪个服务器都无所谓。 tomcat + redis 这个其实还挺方便的，就是使用 session 的代码，跟以前一样，还是基于 tomcat 原生的 session 支持即可，然后就是用一个叫做 Tomcat RedisSessionManager 的东西，让所有我们部署的 tomcat 都将 session 数据存储到 redis 即可。 在 tomcat 的配置文件中配置： 然后指定 redis 的 host 和 port 就 ok 了。 :26379,:26379,:26379\" maxInactiveInterval=\"60\"/> 还可以用上面这种方式基于 redis 哨兵支持的 redis 高可用集群来保存 session 数据，都是 ok 的。 spring session + redis 上面所说的第二种方式会与 tomcat 容器重耦合，如果我要将 web 容器迁移成 jetty，难道还要重新把 jetty 都配置一遍？ 因为上面那种 tomcat + redis 的方式好用，但是会严重依赖于web容器，不好将代码移植到其他 web 容器上去，尤其是你要是换了技术栈咋整？比如换成了 spring cloud 或者是 spring boot 之类的呢？ 所以现在比较好的还是基于 Java 一站式解决方案，也就是 spring。人家 spring 基本上承包了大部分我们需要使用的框架，spirng cloud 做微服务，spring boot 做脚手架，所以用 sping session 是一个很好的选择。 在 pom.xml 中配置： org.springframework.session spring-session-data-redis 1.2.1.RELEASE redis.clients jedis 2.8.1 在 spring 配置文件中配置： 在 web.xml 中配置： springSessionRepositoryFilter org.springframework.web.filter.DelegatingFilterProxy springSessionRepositoryFilter /* 示例代码： @RestController @RequestMapping(\"/test\") public class TestController { @RequestMapping(\"/putIntoSession\") public String putIntoSession(HttpServletRequest request, String username) { request.getSession().setAttribute(\"name\", \"leo\"); return \"ok\"; } @RequestMapping(\"/getFromSession\") public String getFromSession(HttpServletRequest request, Model model){ String name = request.getSession().getAttribute(\"name\"); return name; } } 上面的代码就是 ok 的，给 sping session 配置基于 redis 来存储 session 数据，然后配置了一个 spring session 的过滤器，这样的话，session 相关操作都会交给 spring session 来管了。接着在代码中，就用原生的 session 操作，就是直接基于 spring sesion 从 redis 中获取数据了。 实现分布式的会话有很多种方式，我说的只不过是比较常见的几种方式，tomcat + redis 早期比较常用，但是会重耦合到 tomcat 中；近些年，通过 spring session 来实现。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-availability/hystrix-introduction.html":{"url":"advanced-java/docs/high-availability/hystrix-introduction.html","title":"Hystrix 介绍","keywords":"","body":"用 Hystrix 构建高可用服务架构 参考 Hystrix Home。 Hystrix 是什么？ 在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。 Hystrix 可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。 Hystrix 通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障时在整个系统所有的依赖服务调用中进行蔓延；同时Hystrix 还提供故障时的 fallback 降级机制。 总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性。 Hystrix 的历史 Hystrix 是高可用性保障的一个框架。Netflix（可以认为是国外的优酷或者爱奇艺之类的视频网站）的 API 团队从 2011 年开始做一些提升系统可用性和稳定性的工作，Hystrix 就是从那时候开始发展出来的。 在 2012 年的时候，Hystrix 就变得比较成熟和稳定了，Netflix 中，除了 API 团队以外，很多其他的团队都开始使用 Hystrix。 时至今日，Netflix 中每天都有数十亿次的服务间调用，通过 Hystrix 框架在进行，而 Hystrix 也帮助 Netflix 网站提升了整体的可用性和稳定性。 2018 年 11 月，Hystrix 在其 Github 主页宣布，不再开放新功能，推荐开发者使用其他仍然活跃的开源项目。维护模式的转变绝不意味着 Hystrix 不再有价值。相反，Hystrix 激发了很多伟大的想法和项目，我们高可用的这一块知识还是会针对 Hystrix 进行讲解。 Hystrix 的设计原则 对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护。 在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延。比如某一个服务故障了，导致其它服务也跟着故障。 提供 fail-fast（快速失败）和快速恢复的支持。 提供 fallback 优雅降级的支持。 支持近实时的监控、报警以及运维操作。 举个栗子。 有这样一个分布式系统，服务 A 依赖于服务 B，服务 B 依赖于服务 C/D/E。在这样一个成熟的系统内，比如说最多可能只有 100 个线程资源。正常情况下，40 个线程并发调用服务 C，各 30 个线程并发调用 D/E。 调用服务 C，只需要 20ms，现在因为服务 C 故障了，比如延迟，或者挂了，此时线程会 hang 住 2s 左右。40 个线程全部被卡住，由于请求不断涌入，其它的线程也用来调用服务 C，同样也会被卡住。这样导致服务 B 的线程资源被耗尽，无法接收新的请求，甚至可能因为大量线程不断的运转，导致自己宕机。服务 A 也挂。 Hystrix 可以对其进行资源隔离，比如限制服务 B 只有 40 个线程调用服务 C。当此 40 个线程被 hang 住时，其它 60 个线程依然能正常调用工作。从而确保整个系统不会被拖垮。 Hystrix 更加细节的设计原则 阻止任何一个依赖服务耗尽所有的资源，比如 tomcat 中的所有线程资源。 避免请求排队和积压，采用限流和 fail fast 来控制故障。 提供 fallback 降级机制来应对故障。 使用资源隔离技术，比如 bulkhead（舱壁隔离技术）、swimlane（泳道技术）、circuit breaker（断路技术）来限制任何一个依赖服务的故障的影响。 通过近实时的统计/监控/报警功能，来提高故障发现的速度。 通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度。 保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/e-commerce-website-detail-page-architecture.html":{"url":"advanced-java/docs/high-availability/e-commerce-website-detail-page-architecture.html","title":"电商网站详情页系统架构","keywords":"","body":"电商网站的商品详情页系统架构 小型电商网站的商品详情页系统架构 小型电商网站的页面展示采用页面全量静态化的思想。数据库中存放了所有的商品信息，页面静态化系统，将数据填充进静态模板中，形成静态化页面，推入 Nginx 服务器。用户浏览网站页面时，取用一个已经静态化好的 html 页面，直接返回回去，不涉及任何的业务逻辑处理。 下面是页面模板的简单 Demo 。 商品名称：#{productName} 商品价格：#{productPrice} 商品描述：#{productDesc} 这样做，好处在于，用户每次浏览一个页面，不需要进行任何的跟数据库的交互逻辑，也不需要执行任何的代码，直接返回一个 html 页面就可以了，速度和性能非常高。 对于小网站，页面很少，很实用，非常简单，Java 中可以使用 velocity、freemarker、thymeleaf 等等，然后做个 cms 页面内容管理系统，模板变更的时候，点击按钮或者系统自动化重新进行全量渲染。 坏处在于，仅仅适用于一些小型的网站，比如页面的规模在几十到几万不等。对于一些大型的电商网站，亿级数量的页面，你说你每次页面模板修改了，都需要将这么多页面全量静态化，靠谱吗？每次渲染花个好几天时间，那你整个网站就废掉了。 大型电商网站的商品详情页系统架构 大型电商网站商品详情页的系统设计中，当商品数据发生变更时，会将变更消息压入 MQ 消息队列中。缓存服务从消息队列中消费这条消息时，感知到有数据发生变更，便通过调用数据服务接口，获取变更后的数据，然后将整合好的数据推送至 redis 中。Nginx 本地缓存的数据是有一定的时间期限的，比如说 10 分钟，当数据过期之后，它就会从 redis 获取到最新的缓存数据，并且缓存到自己本地。 用户浏览网页时，动态将 Nginx 本地数据渲染到本地 html 模板并返回给用户。 虽然没有直接返回 html 页面那么快，但是因为数据在本地缓存，所以也很快，其实耗费的也就是动态渲染一个 html 页面的性能。如果 html 模板发生了变更，不需要将所有的页面重新静态化，也不需要发送请求，没有网络请求的开销，直接将数据渲染进最新的 html 页面模板后响应即可。 在这种架构下，我们需要保证系统的高可用性。 如果系统访问量很高，Nginx 本地缓存过期失效了，redis 中的缓存也被 LRU 算法给清理掉了，那么会有较高的访问量，从缓存服务调用商品服务。但如果此时商品服务的接口发生故障，调用出现了延时，缓存服务全部的线程都被这个调用商品服务接口给耗尽了，每个线程去调用商品服务接口的时候，都会卡住很长时间，后面大量的请求过来都会卡在那儿，此时缓存服务没有足够的线程去调用其它一些服务的接口，从而导致整个大量的商品详情页无法正常显示。 这其实就是一个商品接口服务故障导致缓存服务资源耗尽的现象。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-thread-pool-isolation.html":{"url":"advanced-java/docs/high-availability/hystrix-thread-pool-isolation.html","title":"Hystrix 线程池技术实现资源隔离","keywords":"","body":"基于 Hystrix 线程池技术实现资源隔离 上一讲提到，如果从 Nginx 开始，缓存都失效了，Nginx 会直接通过缓存服务调用商品服务获取最新商品数据（我们基于电商项目做个讨论），有可能出现调用延时而把缓存服务资源耗尽的情况。这里，我们就来说说，怎么通过 Hystrix 线程池技术实现资源隔离。 资源隔离，就是说，你如果要把对某一个依赖服务的所有调用请求，全部隔离在同一份资源池内，不会去用其它资源了，这就叫资源隔离。哪怕对这个依赖服务，比如说商品服务，现在同时发起的调用量已经到了 1000，但是线程池内就 10 个线程，最多就只会用这 10 个线程去执行，不会说，对商品服务的请求，因为接口调用延时，将 tomcat 内部所有的线程资源全部耗尽。 Hystrix 进行资源隔离，其实是提供了一个抽象，叫做 command。这也是 Hystrix 最最基本的资源隔离技术。 利用 HystrixCommand 获取单条数据 我们通过将调用商品服务的操作封装在 HystrixCommand 中，限定一个 key，比如下面的 GetProductInfoCommandGroup，在这里我们可以简单认为这是一个线程池，每次调用商品服务，就只会用该线程池中的资源，不会再去用其它线程资源了。 public class GetProductInfoCommand extends HystrixCommand { private Long productId; public GetProductInfoCommand(Long productId) { super(HystrixCommandGroupKey.Factory.asKey(\"GetProductInfoCommandGroup\")); this.productId = productId; } @Override protected ProductInfo run() { String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; // 调用商品服务接口 String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); } } 我们在缓存服务接口中，根据 productId 创建 command 并执行，获取到商品数据。 @RequestMapping(\"/getProductInfo\") @ResponseBody public String getProductInfo(Long productId) { HystrixCommand getProductInfoCommand = new GetProductInfoCommand(productId); // 通过command执行，获取最新商品数据 ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println(productInfo); return \"success\"; } 上面执行的是 execute() 方法，其实是同步的。也可以对 command 调用 queue() 方法，它仅仅是将 command 放入线程池的一个等待队列，就立即返回，拿到一个 Future 对象，后面可以继续做其它一些事情，然后过一段时间对 Future 调用 get() 方法获取数据。这是异步的。 利用 HystrixObservableCommand 批量获取数据 只要是获取商品数据，全部都绑定到同一个线程池里面去，我们通过 HystrixObservableCommand 的一个线程去执行，而在这个线程里面，批量把多个 productId 的 productInfo 拉回来。 public class GetProductInfosCommand extends HystrixObservableCommand { private String[] productIds; public GetProductInfosCommand(String[] productIds) { // 还是绑定在同一个线程池 super(HystrixCommandGroupKey.Factory.asKey(\"GetProductInfoGroup\")); this.productIds = productIds; } @Override protected Observable construct() { return Observable.unsafeCreate((Observable.OnSubscribe) subscriber -> { for (String productId : productIds) { // 批量获取商品数据 String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); ProductInfo productInfo = JSONObject.parseObject(response, ProductInfo.class); subscriber.onNext(productInfo); } subscriber.onCompleted(); }).subscribeOn(Schedulers.io()); } } 在缓存服务接口中，根据传来的 id 列表，比如是以 , 分隔的 id 串，通过上面的 HystrixObservableCommand，执行 Hystrix 的一些 API 方法，获取到所有商品数据。 public String getProductInfos(String productIds) { String[] productIdArray = productIds.split(\",\"); HystrixObservableCommand getProductInfosCommand = new GetProductInfosCommand(productIdArray); Observable observable = getProductInfosCommand.observe(); observable.subscribe(new Observer() { @Override public void onCompleted() { System.out.println(\"获取完了所有的商品数据\"); } @Override public void onError(Throwable e) { e.printStackTrace(); } /** * 获取完一条数据，就回调一次这个方法 * @param productInfo */ @Override public void onNext(ProductInfo productInfo) { System.out.println(productInfo); } }); return \"success\"; } 我们回过头来，看看 Hystrix 线程池技术是如何实现资源隔离的。 从 Nginx 开始，缓存都失效了，那么 Nginx 通过缓存服务去调用商品服务。缓存服务默认的线程大小是 10 个，最多就只有 10 个线程去调用商品服务的接口。即使商品服务接口故障了，最多就只有 10 个线程会 hang 死在调用商品服务接口的路上，缓存服务的 tomcat 内其它的线程还是可以用来调用其它的服务，干其它的事情。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-semphore-isolation.html":{"url":"advanced-java/docs/high-availability/hystrix-semphore-isolation.html","title":"Hystrix 信号量机制实现资源隔离","keywords":"","body":"基于 Hystrix 信号量机制实现资源隔离 Hystrix 里面核心的一项功能，其实就是所谓的资源隔离，要解决的最最核心的问题，就是将多个依赖服务的调用分别隔离到各自的资源池内。避免说对某一个依赖服务的调用，因为依赖服务的接口调用的延迟或者失败，导致服务所有的线程资源全部耗费在这个服务的接口调用上。一旦说某个服务的线程资源全部耗尽的话，就可能导致服务崩溃，甚至说这种故障会不断蔓延。 Hystrix 实现资源隔离，主要有两种技术： 线程池 信号量 默认情况下，Hystrix 使用线程池模式。 前面已经说过线程池技术了，这一小节就来说说信号量机制实现资源隔离，以及这两种技术的区别与具体应用场景。 信号量机制 信号量的资源隔离只是起到一个开关的作用，比如，服务 A 的信号量大小为 10，那么就是说它同时只允许有 10 个 tomcat 线程来访问服务 A，其它的请求都会被拒绝，从而达到资源隔离和限流保护的作用。 线程池与信号量区别 线程池隔离技术，并不是说去控制类似 tomcat 这种 web 容器的线程。更加严格的意义上来说，Hystrix 的线程池隔离技术，控制的是 tomcat 线程的执行。Hystrix 线程池满后，会确保说，tomcat 的线程不会因为依赖服务的接口调用延迟或故障而被 hang 住，tomcat 其它的线程不会卡死，可以快速返回，然后支撑其它的事情。 线程池隔离技术，是用 Hystrix 自己的线程去执行调用；而信号量隔离技术，是直接让 tomcat 线程去调用依赖服务。信号量隔离，只是一道关卡，信号量有多少，就允许多少个 tomcat 线程通过它，然后去执行。 适用场景： 线程池技术，适合绝大多数场景，比如说我们对依赖服务的网络请求的调用和访问、需要对调用的 timeout 进行控制（捕捉 timeout 超时异常）。 信号量技术，适合说你的访问不是对外部依赖的访问，而是对内部的一些比较复杂的业务逻辑的访问，并且系统内部的代码，其实不涉及任何的网络请求，那么只要做信号量的普通限流就可以了，因为不需要去捕获 timeout 类似的问题。 信号量简单 Demo 业务背景里，比较适合信号量的是什么场景呢？ 比如说，我们一般来说，缓存服务，可能会将一些量特别少、访问又特别频繁的数据，放在自己的纯内存中。 举个栗子。一般我们在获取到商品数据之后，都要去获取商品是属于哪个地理位置、省、市、卖家等，可能在自己的纯内存中，比如就一个 Map 去获取。对于这种直接访问本地内存的逻辑，比较适合用信号量做一下简单的隔离。 优点在于，不用自己管理线程池啦，不用 care timeout 超时啦，也不需要进行线程的上下文切换啦。信号量做隔离的话，性能相对来说会高一些。 假如这是本地缓存，我们可以通过 cityId，拿到 cityName。 public class LocationCache { private static Map cityMap = new HashMap<>(); static { cityMap.put(1L, \"北京\"); } /** * 通过cityId 获取 cityName * * @param cityId 城市id * @return 城市名 */ public static String getCityName(Long cityId) { return cityMap.get(cityId); } } 写一个 GetCityNameCommand，策略设置为信号量。run() 方法中获取本地缓存。我们目的就是对获取本地缓存的代码进行资源隔离。 public class GetCityNameCommand extends HystrixCommand { private Long cityId; public GetCityNameCommand(Long cityId) { // 设置信号量隔离策略 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetCityNameGroup\")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE))); this.cityId = cityId; } @Override protected String run() { // 需要进行信号量隔离的代码 return LocationCache.getCityName(cityId); } } 在接口层，通过创建 GetCityNameCommand，传入 cityId，执行 execute() 方法，那么获取本地 cityName 缓存的代码将会进行信号量的资源隔离。 @RequestMapping(\"/getProductInfo\") @ResponseBody public String getProductInfo(Long productId) { HystrixCommand getProductInfoCommand = new GetProductInfoCommand(productId); // 通过command执行，获取最新商品数据 ProductInfo productInfo = getProductInfoCommand.execute(); Long cityId = productInfo.getCityId(); GetCityNameCommand getCityNameCommand = new GetCityNameCommand(cityId); // 获取本地内存(cityName)的代码会被信号量进行资源隔离 String cityName = getCityNameCommand.execute(); productInfo.setCityName(cityName); System.out.println(productInfo); return \"success\"; } Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-execution-isolation.html":{"url":"advanced-java/docs/high-availability/hystrix-execution-isolation.html","title":"Hystrix 隔离策略细粒度控制","keywords":"","body":"Hystrix 隔离策略细粒度控制 Hystrix 实现资源隔离，有两种策略： 线程池隔离 信号量隔离 对资源隔离这一块东西，其实可以做一定细粒度的一些控制。 execution.isolation.strategy 指定了 HystrixCommand.run() 的资源隔离策略：THREAD or SEMAPHORE，一种基于线程池，一种基于信号量。 // to use thread isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.THREAD) // to use semaphore isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE) 线程池机制，每个 command 运行在一个线程中，限流是通过线程池的大小来控制的；信号量机制，command 是运行在调用线程中，通过信号量的容量来进行限流。 如何在线程池和信号量之间做选择？ 默认的策略就是线程池。 线程池其实最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住。 而使用信号量的场景，通常是针对超大并发量的场景下，每个服务实例每秒都几百的 QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护。一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求。 command key & command group 我们使用线程池隔离，要怎么对依赖服务、依赖服务接口、线程池三者做划分呢？ 每一个 command，都可以设置一个自己的名称 command key，同时可以设置一个自己的组 command group。 private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"HelloWorld\")); public CommandHelloWorld(String name) { super(cachedSetter); this.name = name; } command group 是一个非常重要的概念，默认情况下，就是通过 command group 来定义一个线程池的，而且还会通过 command group 来聚合一些监控和报警信息。同一个 command group 中的请求，都会进入同一个线程池中。 command thread pool ThreadPoolKey 代表了一个 HystrixThreadPool，用来进行统一监控、统计、缓存。默认的 ThreadPoolKey 就是 command group 的名称。每个 command 都会跟它的 ThreadPoolKey 对应的 ThreadPool 绑定在一起。 如果不想直接用 command group，也可以手动设置 ThreadPool 的名称。 private static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"HelloWorld\")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"HelloWorldPool\")); public CommandHelloWorld(String name) { super(cachedSetter); this.name = name; } command key & command group & command thread pool command key ，代表了一类 command，一般来说，代表了底层的依赖服务的一个接口。 command group ，代表了某一个底层的依赖服务，这是很合理的，一个依赖服务可能会暴露出来多个接口，每个接口就是一个 command key。command group 在逻辑上去组织起来一堆 command key 的调用、统计信息、成功次数、timeout 超时次数、失败次数等，可以看到某一个服务整体的一些访问情况。一般来说，推荐根据一个服务区划分出一个线程池，command key 默认都是属于同一个线程池的。 比如说你以一个服务为粒度，估算出来这个服务每秒的所有接口加起来的整体 QPS 在 100 左右，你调用这个服务，当前这个服务部署了 10 个服务实例，每个服务实例上，其实用这个 command group 对应这个服务，给一个线程池，量大概在 10 个左右就可以了，你对整个服务的整体的访问 QPS 就大概在每秒 100 左右。 但是，如果说 command group 对应了一个服务，而这个服务暴露出来的几个接口，访问量很不一样，差异非常之大。你可能就希望在这个服务 command group 内部，包含的对应多个接口的 command key，做一些细粒度的资源隔离。就是说，对同一个服务的不同接口，使用不同的线程池。 command key -> command group command key -> 自己的 thread pool key 逻辑上来说，多个 command key 属于一个command group，在做统计的时候，会放在一起统计。每个 command key 有自己的线程池，每个接口有自己的线程池，去做资源隔离和限流。 说白点，就是说如果你的 command key 要用自己的线程池，可以定义自己的 thread pool key，就 ok 了。 coreSize 设置线程池的大小，默认是 10。一般来说，用这个默认的 10 个线程大小就够了。 HystrixThreadPoolProperties.Setter().withCoreSize(int value); queueSizeRejectionThreshold 如果说线程池中的 10 个线程都在工作中，没有空闲的线程来做其它的事情，此时再有请求过来，会先进入队列积压。如果说队列积压满了，再有请求过来，就直接 reject，拒绝请求，执行 fallback 降级的逻辑，快速返回。 控制 queue 满了之后 reject 的 threshold，因为 maxQueueSize 不允许热修改，因此提供这个参数可以热修改，控制队列的最大大小。 HystrixThreadPoolProperties.Setter().withQueueSizeRejectionThreshold(int value); execution.isolation.semaphore.maxConcurrentRequests 设置使用 SEMAPHORE 隔离策略的时候允许访问的最大并发量，超过这个最大并发量，请求直接被 reject。 这个并发量的设置，跟线程池大小的设置，应该是类似的，但是基于信号量的话，性能会好很多，而且 Hystrix 框架本身的开销会小很多。 默认值是 10，尽量设置的小一些，因为一旦设置的太大，而且有延时发生，可能瞬间导致 tomcat 本身的线程资源被占满。 HystrixCommandProperties.Setter().withExecutionIsolationSemaphoreMaxConcurrentRequests(int value); Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-process.html":{"url":"advanced-java/docs/high-availability/hystrix-process.html","title":"深入 Hystrix 执行时内部原理","keywords":"","body":"深入 Hystrix 执行时内部原理 前面我们了解了 Hystrix 最基本的支持高可用的技术：资源隔离 + 限流。 创建 command； 执行这个 command； 配置这个 command 对应的 group 和线程池。 这里，我们要讲一下，你开始执行这个 command，调用了这个 command 的 execute() 方法之后，Hystrix 底层的执行流程和步骤以及原理是什么。 在讲解这个流程的过程中，我会带出来 Hystrix 其他的一些核心以及重要的功能。 这里是整个 8 大步骤的流程图，我会对每个步骤进行细致的讲解。学习的过程中，对照着这个流程图，相信思路会比较清晰。 步骤一：创建 command 一个 HystrixCommand 或 HystrixObservableCommand 对象，代表了对某个依赖服务发起的一次请求或者调用。创建的时候，可以在构造函数中传入任何需要的参数。 HystrixCommand 主要用于仅仅会返回一个结果的调用。 HystrixObservableCommand 主要用于可能会返回多条结果的调用。 // 创建 HystrixCommand HystrixCommand hystrixCommand = new HystrixCommand(arg1, arg2); // 创建 HystrixObservableCommand HystrixObservableCommand hystrixObservableCommand = new HystrixObservableCommand(arg1, arg2); 步骤二：调用 command 执行方法 执行 command，就可以发起一次对依赖服务的调用。 要执行 command，可以在 4 个方法中选择其中的一个：execute()、queue()、observe()、toObservable()。 其中 execute() 和 queue() 方法仅仅对 HystrixCommand 适用。 execute()：调用后直接 block 住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常。 queue()：返回一个 Future，属于异步调用，后面可以通过 Future 获取单条结果。 observe()：订阅一个 Observable 对象，Observable 代表的是依赖服务返回的结果，获取到一个那个代表结果的 Observable 对象的拷贝对象。 toObservable()：返回一个 Observable 对象，如果我们订阅这个对象，就会执行 command 并且获取返回结果。 K value = hystrixCommand.execute(); Future fValue = hystrixCommand.queue(); Observable oValue = hystrixObservableCommand.observe(); Observable toOValue = hystrixObservableCommand.toObservable(); execute() 实际上会调用 queue().get() 方法，可以看一下 Hystrix 源码。 public R execute() { try { return queue().get(); } catch (Exception e) { throw Exceptions.sneakyThrow(decomposeException(e)); } } 而在 queue() 方法中，会调用 toObservable().toBlocking().toFuture()。 final Future delegate = toObservable().toBlocking().toFuture(); 也就是说，先通过 toObservable() 获得 Future 对象，然后调用 Future 的 get() 方法。那么，其实无论是哪种方式执行 command，最终都是依赖于 toObservable() 去执行的。 步骤三：检查是否开启缓存 从这一步开始，就进入到 Hystrix 底层运行原理啦，看一下 Hystrix 一些更高级的功能和特性。 如果这个 command 开启了请求缓存 Request Cache，而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果。否则，继续往后的步骤。 步骤四：检查是否开启了断路器 检查这个 command 对应的依赖服务是否开启了断路器。如果断路器被打开了，那么 Hystrix 就不会执行这个 command，而是直接去执行 fallback 降级机制，返回降级结果。 步骤五：检查线程池/队列/信号量是否已满 如果这个 command 线程池和队列已满，或者 semaphore 信号量已满，那么也不会执行 command，而是直接去调用 fallback 降级机制，同时发送 reject 信息给断路器统计。 步骤六：执行 command 调用 HystrixObservableCommand 对象的 construct() 方法，或者 HystrixCommand 的 run() 方法来实际执行这个 command。 HystrixCommand.run() 返回单条结果，或者抛出异常。 // 通过command执行，获取最新一条商品数据 ProductInfo productInfo = getProductInfoCommand.execute(); HystrixObservableCommand.construct() 返回一个 Observable 对象，可以获取多条结果。 Observable observable = getProductInfosCommand.observe(); // 订阅获取多条结果 observable.subscribe(new Observer() { @Override public void onCompleted() { System.out.println(\"获取完了所有的商品数据\"); } @Override public void onError(Throwable e) { e.printStackTrace(); } /** * 获取完一条数据，就回调一次这个方法 * @param productInfo */ @Override public void onNext(ProductInfo productInfo) { System.out.println(productInfo); } }); 如果是采用线程池方式，并且 HystrixCommand.run() 或者 HystrixObservableCommand.construct() 的执行时间超过了 timeout 时长的话，那么 command 所在的线程会抛出一个 TimeoutException，这时会执行 fallback 降级机制，不会去管 run() 或 construct() 返回的值了。另一种情况，如果 command 执行出错抛出了其它异常，那么也会走 fallback 降级。这两种情况下，Hystrix 都会发送异常事件给断路器统计。 注意，我们是不可能终止掉一个调用严重延迟的依赖服务的线程的，只能说给你抛出来一个 TimeoutException。 如果没有 timeout，也正常执行的话，那么调用线程就会拿到一些调用依赖服务获取到的结果，然后 Hystrix 也会做一些 logging 记录和 metric 度量统计。 步骤七：断路健康检查 Hystrix 会把每一个依赖服务的调用成功、失败、Reject、Timeout 等事件发送给 circuit breaker 断路器。断路器就会对这些事件的次数进行统计，根据异常事件发生的比例来决定是否要进行断路（熔断）。如果打开了断路器，那么在接下来一段时间内，会直接断路，返回降级结果。 如果在之后，断路器尝试执行 command，调用没有出错，返回了正常结果，那么 Hystrix 就会把断路器关闭。 步骤八：调用 fallback 降级机制 在以下几种情况中，Hystrix 会调用 fallback 降级机制。 断路器处于打开状态； 线程池/队列/semaphore满了； command 执行超时； run() 或者 construct() 抛出异常。 一般在降级机制中，都建议给出一些默认的返回值，比如静态的一些代码逻辑，或者从内存中的缓存中提取一些数据，在这里尽量不要再进行网络请求了。 在降级中，如果一定要进行网络调用的话，也应该将那个调用放在一个 HystrixCommand 中进行隔离。 HystrixCommand 中，实现 getFallback() 方法，可以提供降级机制。 HystrixObservableCommand 中，实现 resumeWithFallback() 方法，返回一个 Observable 对象，可以提供降级结果。 如果没有实现 fallback，或者 fallback 抛出了异常，Hystrix 会返回一个 Observable，但是不会返回任何数据。 不同的 command 执行方式，其 fallback 为空或者异常时的返回结果不同。 对于 execute()，直接抛出异常。 对于 queue()，返回一个 Future，调用 get() 时抛出异常。 对于 observe()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。 对于 toObservable()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。 不同的执行方式 execute()，获取一个 Future.get()，然后拿到单个结果。 queue()，返回一个 Future。 observe()，立即订阅 Observable，然后启动 8 大执行步骤，返回一个拷贝的 Observable，订阅时立即回调给你结果。 toObservable()，返回一个原始的 Observable，必须手动订阅才会去执行 8 大步骤。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-request-cache.html":{"url":"advanced-java/docs/high-availability/hystrix-request-cache.html","title":"基于 request cache 请求缓存技术优化批量商品数据查询接口","keywords":"","body":"基于 request cache 请求缓存技术优化批量商品数据查询接口 Hystrix command 执行时 8 大步骤第三步，就是检查 Request cache 是否有缓存。 首先，有一个概念，叫做 Request Context 请求上下文，一般来说，在一个 web 应用中，如果我们用到了 Hystrix，我们会在一个 filter 里面，对每一个请求都施加一个请求上下文。就是说，每一次请求，就是一次请求上下文。然后在这次请求上下文中，我们会去执行 N 多代码，调用 N 多依赖服务，有的依赖服务可能还会调用好几次。 在一次请求上下文中，如果有多个 command，参数都是一样的，调用的接口也是一样的，而结果可以认为也是一样的。那么这个时候，我们可以让第一个 command 执行返回的结果缓存在内存中，然后这个请求上下文后续的其它对这个依赖的调用全部从内存中取出缓存结果就可以了。 这样的话，好处在于不用在一次请求上下文中反复多次执行一样的 command，避免重复执行网络请求，提升整个请求的性能。 举个栗子。比如说我们在一次请求上下文中，请求获取 productId 为 1 的数据，第一次缓存中没有，那么会从商品服务中获取数据，返回最新数据结果，同时将数据缓存在内存中。后续同一次请求上下文中，如果还有获取 productId 为 1 的数据的请求，直接从缓存中取就好了。 HystrixCommand 和 HystrixObservableCommand 都可以指定一个缓存 key，然后 Hystrix 会自动进行缓存，接着在同一个 request context 内，再次访问的话，就会直接取用缓存。 下面，我们结合一个具体的业务场景，来看一下如何使用 request cache 请求缓存技术。当然，以下代码只作为一个基本的 Demo 而已。 现在，假设我们要做一个批量查询商品数据的接口，在这个里面，我们是用 HystrixCommand 一次性批量查询多个商品 id 的数据。但是这里有个问题，如果说 Nginx 在本地缓存失效了，重新获取一批缓存，传递过来的 productIds 都没有进行去重，比如 productIds=1,1,1,2,2，那么可能说，商品 id 出现了重复，如果按照我们之前的业务逻辑，可能就会重复对 productId=1 的商品查询三次，productId=2 的商品查询两次。 我们对批量查询商品数据的接口，可以用 request cache 做一个优化，就是说一次请求，就是一次 request context，对相同的商品查询只执行一次，其余重复的都走 request cache。 实现 Hystrix 请求上下文过滤器并注册 定义 HystrixRequestContextFilter 类，实现 Filter 接口。 /** * Hystrix 请求上下文过滤器 */ public class HystrixRequestContextFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { filterChain.doFilter(servletRequest, servletResponse); } catch (IOException | ServletException e) { e.printStackTrace(); } finally { context.shutdown(); } } @Override public void destroy() { } } 然后将该 filter 对象注册到 SpringBoot Application 中。 @SpringBootApplication public class EshopApplication { public static void main(String[] args) { SpringApplication.run(EshopApplication.class, args); } @Bean public FilterRegistrationBean filterRegistrationBean() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new HystrixRequestContextFilter()); filterRegistrationBean.addUrlPatterns(\"/*\"); return filterRegistrationBean; } } command 重写 getCacheKey() 方法 在 GetProductInfoCommand 中，重写 getCacheKey() 方法，这样的话，每一次请求的结果，都会放在 Hystrix 请求上下文中。下一次同一个 productId 的数据请求，直接取缓存，无须再调用 run() 方法。 public class GetProductInfoCommand extends HystrixCommand { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\"GetProductInfoCommand\"); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ProductInfoService\")) .andCommandKey(KEY)); this.productId = productId; } @Override protected ProductInfo run() { String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(\"调用接口查询商品数据，productId=\" + productId); return JSONObject.parseObject(response, ProductInfo.class); } /** * 每次请求的结果，都会放在Hystrix绑定的请求上下文上 * * @return cacheKey 缓存key */ @Override public String getCacheKey() { return \"product_info_\" + productId; } /** * 将某个商品id的缓存清空 * * @param productId 商品id */ public static void flushCache(Long productId) { HystrixRequestCache.getInstance(KEY, HystrixConcurrencyStrategyDefault.getInstance()).clear(\"product_info_\" + productId); } } 这里写了一个 flushCache() 方法，用于我们开发手动删除缓存。 controller 调用 command 查询商品信息 在一次 web 请求上下文中，传入商品 id 列表，查询多条商品数据信息。对于每个 productId，都创建一个 command。 如果 id 列表没有去重，那么重复的 id，第二次查询的时候就会直接走缓存。 @Controller public class CacheController { /** * 一次性批量查询多条商品数据的请求 * * @param productIds 以,分隔的商品id列表 * @return 响应状态 */ @RequestMapping(\"/getProductInfos\") @ResponseBody public String getProductInfos(String productIds) { for (String productId : productIds.split(\",\")) { // 对每个productId，都创建一个command GetProductInfoCommand getProductInfoCommand = new GetProductInfoCommand(Long.valueOf(productId)); ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println(\"是否是从缓存中取的结果：\" + getProductInfoCommand.isResponseFromCache()); } return \"success\"; } } 发起请求 调用接口，查询多个商品的信息。 http://localhost:8080/getProductInfos?productIds=1,1,1,2,2,5 在控制台，我们可以看到以下结果。 调用接口查询商品数据，productId=1 是否是从缓存中取的结果：false 是否是从缓存中取的结果：true 是否是从缓存中取的结果：true 调用接口查询商品数据，productId=2 是否是从缓存中取的结果：false 是否是从缓存中取的结果：true 调用接口查询商品数据，productId=5 是否是从缓存中取的结果：false 第一次查询 productId=1 的数据，会调用接口进行查询，不是从缓存中取结果。而随后再出现查询 productId=1 的请求，就直接取缓存了，这样的话，效率明显高很多。 删除缓存 我们写一个 UpdateProductInfoCommand，在更新商品信息之后，手动调用之前写的 flushCache()，手动将缓存删除。 public class UpdateProductInfoCommand extends HystrixCommand { private Long productId; public UpdateProductInfoCommand(Long productId) { super(HystrixCommandGroupKey.Factory.asKey(\"UpdateProductInfoGroup\")); this.productId = productId; } @Override protected Boolean run() throws Exception { // 这里执行一次商品信息的更新 // ... // 然后清空缓存 GetProductInfoCommand.flushCache(productId); return true; } } 这样，以后查询该商品的请求，第一次就会走接口调用去查询最新的商品信息。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-fallback.html":{"url":"advanced-java/docs/high-availability/hystrix-fallback.html","title":"基于本地缓存的 fallback 降级机制","keywords":"","body":"基于本地缓存的 fallback 降级机制 Hystrix 出现以下四种情况，都会去调用 fallback 降级机制： 断路器处于打开的状态。 资源池已满（线程池+队列 / 信号量）。 Hystrix 调用各种接口，或者访问外部依赖，比如 MySQL、Redis、Zookeeper、Kafka 等等，出现了任何异常的情况。 访问外部依赖的时候，访问时间过长，报了 TimeoutException 异常。 两种最经典的降级机制 纯内存数据 在降级逻辑中，你可以在内存中维护一个 ehcache，作为一个纯内存的基于 LRU 自动清理的缓存，让数据放在缓存内。如果说外部依赖有异常，fallback 这里直接尝试从 ehcache 中获取数据。 默认值 fallback 降级逻辑中，也可以直接返回一个默认值。 在 HystrixCommand，降级逻辑的书写，是通过实现 getFallback() 接口；而在 HystrixObservableCommand 中，则是实现 resumeWithFallback() 方法。 现在，我们用一个简单的栗子，来演示 fallback 降级是怎么做的。 比如，有这么个场景。我们现在有个包含 brandId 的商品数据，假设正常的逻辑是这样：拿到一个商品数据，根据 brandId 去调用品牌服务的接口，获取品牌的最新名称 brandName。 假如说，品牌服务接口挂掉了，那么我们可以尝试从本地内存中，获取一份稍过期的数据，先凑合着用。 步骤一：本地缓存获取数据 本地获取品牌名称的代码大致如下。 /** * 品牌名称本地缓存 * */ public class BrandCache { private static Map brandMap = new HashMap<>(); static { brandMap.put(1L, \"Nike\"); } /** * brandId 获取 brandName * @param brandId 品牌id * @return 品牌名 */ public static String getBrandName(Long brandId) { return brandMap.get(brandId); } 步骤二：实现 GetBrandNameCommand 在 GetBrandNameCommand 中，run() 方法的正常逻辑是去调用品牌服务的接口获取到品牌名称，如果调用失败，报错了，那么就会去调用 fallback 降级机制。 这里，我们直接模拟接口调用报错，给它抛出个异常。 而在 getFallback() 方法中，就是我们的降级逻辑，我们直接从本地的缓存中，获取到品牌名称的数据。 /** * 获取品牌名称的command * */ public class GetBrandNameCommand extends HystrixCommand { private Long brandId; public GetBrandNameCommand(Long brandId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"BrandService\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetBrandNameCommand\")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 设置降级机制最大并发请求数 .withFallbackIsolationSemaphoreMaxConcurrentRequests(15))); this.brandId = brandId; } @Override protected String run() throws Exception { // 这里正常的逻辑应该是去调用一个品牌服务的接口获取名称 // 如果调用失败，报错了，那么就会去调用fallback降级机制 // 这里我们直接模拟调用报错，抛出异常 throw new Exception(); } @Override protected String getFallback() { return BrandCache.getBrandName(brandId); } } FallbackIsolationSemaphoreMaxConcurrentRequests 用于设置 fallback 最大允许的并发请求量，默认值是 10，是通过 semaphore 信号量的机制去限流的。如果超出了这个最大值，那么直接 reject。 步骤三：CacheController 调用接口 在 CacheController 中，我们通过 productInfo 获取 brandId，然后创建 GetBrandNameCommand 并执行，去尝试获取 brandName。这里执行会报错，因为我们在 run() 方法中直接抛出异常，Hystrix 就会去调用 getFallback() 方法走降级逻辑。 @Controller public class CacheController { @RequestMapping(\"/getProductInfo\") @ResponseBody public String getProductInfo(Long productId) { HystrixCommand getProductInfoCommand = new GetProductInfoCommand(productId); ProductInfo productInfo = getProductInfoCommand.execute(); Long brandId = productInfo.getBrandId(); HystrixCommand getBrandNameCommand = new GetBrandNameCommand(brandId); // 执行会抛异常报错，然后走降级 String brandName = getBrandNameCommand.execute(); productInfo.setBrandName(brandName); System.out.println(productInfo); return \"success\"; } } 关于降级逻辑的演示，基本上就结束了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-availability/hystrix-circuit-breaker.html":{"url":"advanced-java/docs/high-availability/hystrix-circuit-breaker.html","title":"深入 Hystrix 断路器执行原理","keywords":"","body":"深入 Hystrix 断路器执行原理 RequestVolumeThreshold HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold(int) 表示在滑动窗口中，至少有多少个请求，才可能触发断路。 Hystrix 经过断路器的流量超过了一定的阈值，才有可能触发断路。比如说，要求在 10s 内经过断路器的流量必须达到 20 个，而实际经过断路器的流量才 10 个，那么根本不会去判断要不要断路。 ErrorThresholdPercentage HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(int) 表示异常比例达到多少，才会触发断路，默认值是 50(%)。 如果断路器统计到的异常调用的占比超过了一定的阈值，比如说在 10s 内，经过断路器的流量达到了 30 个，同时其中异常访问的数量也达到了一定的比例，比如 60% 的请求都是异常（报错 / 超时 / reject），就会开启断路。 SleepWindowInMilliseconds HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds(int) 断路开启，也就是由 close 转换到 open 状态（close -> open）。那么之后在 SleepWindowInMilliseconds 时间内，所有经过该断路器的请求全部都会被断路，不调用后端服务，直接走 fallback 降级机制。 而在该参数时间过后，断路器会变为 half-open 半开闭状态，尝试让一条请求经过断路器，看能不能正常调用。如果调用成功了，那么就自动恢复，断路器转为 close 状态。 Enabled HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(boolean) 控制是否允许断路器工作，包括跟踪依赖服务调用的健康状况，以及对异常情况过多时是否允许触发断路。默认值是 true。 ForceOpen HystrixCommandProperties.Setter() .withCircuitBreakerForceOpen(boolean) 如果设置为 true 的话，直接强迫打开断路器，相当于是手动断路了，手动降级，默认值是 false。 ForceClosed HystrixCommandProperties.Setter() .withCircuitBreakerForceClosed(boolean) 如果设置为 true，直接强迫关闭断路器，相当于手动停止断路了，手动升级，默认值是 false。 实例 Demo HystrixCommand 配置参数 在 GetProductInfoCommand 中配置 Setter 断路器相关参数。 滑动窗口中，最少 20 个请求，才可能触发断路。 异常比例达到 40% 时，才触发断路。 断路后 3000ms 内，所有请求都被 reject，直接走 fallback 降级，不会调用 run() 方法。3000ms 过后，变为 half-open 状态。 run() 方法中，我们判断一下 productId 是否为 -1，是的话，直接抛出异常。这么写，我们之后测试的时候就可以传入 productId=-1，模拟服务执行异常了。 在降级逻辑中，我们直接给它返回降级商品就好了。 public class GetProductInfoCommand extends HystrixCommand { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\"GetProductInfoCommand\"); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ProductInfoService\")) .andCommandKey(KEY) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 是否允许断路器工作 .withCircuitBreakerEnabled(true) // 滑动窗口中，最少有多少个请求，才可能触发断路 .withCircuitBreakerRequestVolumeThreshold(20) // 异常比例达到多少，才触发断路，默认50% .withCircuitBreakerErrorThresholdPercentage(40) // 断路后多少时间内直接reject请求，之后进入half-open状态，默认5000ms .withCircuitBreakerSleepWindowInMilliseconds(3000))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\"调用接口查询商品数据，productId=\" + productId); if (productId == -1L) { throw new Exception(); } String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\"降级商品\"); return productInfo; } } 断路测试类 我们在测试类中，前 30 次请求，传入 productId=-1，然后休眠 3s，之后 70 次请求，传入 productId=1。 @SpringBootTest @RunWith(SpringRunner.class) public class CircuitBreakerTest { @Test public void testCircuitBreaker() { String baseURL = \"http://localhost:8080/getProductInfo?productId=\"; for (int i = 0; i 测试结果 测试结果，我们可以明显看出系统断路与恢复的整个过程。 调用接口查询商品数据，productId=-1 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) // ... // 这里重复打印了 20 次上面的结果 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) // ... // 这里重复打印了 8 次上面的结果 // 休眠 3s 后 调用接口查询商品数据，productId=1 ProductInfo(id=1, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=1, modifiedTime=2017-01-01 12:00:00, cityId=1, cityName=null, brandId=1, brandName=null) // ... // 这里重复打印了 69 次上面的结果 前 30 次请求，我们传入的 productId 为 -1，所以服务执行过程中会抛出异常。我们设置了最少 20 次请求通过断路器并且异常比例超出 40% 就触发断路。因此执行了 21 次接口调用，每次都抛异常并且走降级，21 次过后，断路器就被打开了。 之后的 9 次请求，都不会执行 run() 方法，也就不会打印以下信息。 调用接口查询商品数据，productId=-1 而是直接走降级逻辑，调用 getFallback() 执行。 休眠了 3s 后，我们在之后的 70 次请求中，都传入 productId 为 1。由于我们前面设置了 3000ms 过后断路器变为 half-open 状态。因此 Hystrix 会尝试执行请求，发现成功了，那么断路器关闭，之后的所有请求也都能正常调用了。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:28:48 "},"advanced-java/docs/high-availability/hystrix-thread-pool-current-limiting.html":{"url":"advanced-java/docs/high-availability/hystrix-thread-pool-current-limiting.html","title":"深入 Hystrix 线程池隔离与接口限流","keywords":"","body":"深入 Hystrix 线程池隔离与接口限流 前面讲了 Hystrix 的 request cache 请求缓存、fallback 优雅降级、circuit breaker 断路器快速熔断，这一讲，我们来详细说说 Hystrix 的线程池隔离与接口限流。 Hystrix 通过判断线程池或者信号量是否已满，超出容量的请求，直接 Reject 走降级，从而达到限流的作用。 限流是限制对后端的服务的访问量，比如说你对 MySQL、Redis、Zookeeper 以及其它各种后端中间件的资源的访问的限制，其实是为了避免过大的流量直接打死后端的服务。 线程池隔离技术的设计 Hystrix 采用了 Bulkhead Partition 舱壁隔离技术，来将外部依赖进行资源隔离，进而避免任何外部依赖的故障导致本服务崩溃。 舱壁隔离，是说将船体内部空间区隔划分成若干个隔舱，一旦某几个隔舱发生破损进水，水流不会在其间相互流动，如此一来船舶在受损时，依然能具有足够的浮力和稳定性，进而减低立即沉船的危险。 Hystrix 对每个外部依赖用一个单独的线程池，这样的话，如果对那个外部依赖调用延迟很严重，最多就是耗尽那个依赖自己的线程池而已，不会影响其他的依赖调用。 Hystrix 应用线程池机制的场景 每个服务都会调用几十个后端依赖服务，那些后端依赖服务通常是由很多不同的团队开发的。 每个后端依赖服务都会提供它自己的 client 调用库，比如说用 thrift 的话，就会提供对应的 thrift 依赖。 client 调用库随时会变更。 client 调用库随时可能会增加新的网络请求的逻辑。 client 调用库可能会包含诸如自动重试、数据解析、内存中缓存等逻辑。 client 调用库一般都对调用者来说是个黑盒，包括实现细节、网络访问、默认配置等等。 在真实的生产环境中，经常会出现调用者，突然间惊讶的发现，client 调用库发生了某些变化。 即使 client 调用库没有改变，依赖服务本身可能有会发生逻辑上的变化。 有些依赖的 client 调用库可能还会拉取其他的依赖库，而且可能那些依赖库配置的不正确。 大多数网络请求都是同步调用的。 调用失败和延迟，也有可能会发生在 client 调用库本身的代码中，不一定就是发生在网络请求中。 简单来说，就是你必须默认 client 调用库很不靠谱，而且随时可能发生各种变化，所以就要用强制隔离的方式来确保任何服务的故障不会影响当前服务。 线程池机制的优点 任何一个依赖服务都可以被隔离在自己的线程池内，即使自己的线程池资源填满了，也不会影响任何其他的服务调用。 服务可以随时引入一个新的依赖服务，因为即使这个新的依赖服务有问题，也不会影响其他任何服务的调用。 当一个故障的依赖服务重新变好的时候，可以通过清理掉线程池，瞬间恢复该服务的调用，而如果是 tomcat 线程池被占满，再恢复就很麻烦。 如果一个 client 调用库配置有问题，线程池的健康状况随时会报告，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机。 基于线程池的异步本质，可以在同步的调用之上，构建一层异步调用层。 简单来说，最大的好处，就是资源隔离，确保说任何一个依赖服务故障，不会拖垮当前的这个服务。 线程池机制的缺点 线程池机制最大的缺点就是增加了 CPU 的开销。 除了 tomcat 本身的调用线程之外，还有 Hystrix 自己管理的线程池。 每个 command 的执行都依托一个独立的线程，会进行排队，调度，还有上下文切换。 Hystrix 官方自己做了一个多线程异步带来的额外开销统计，通过对比多线程异步调用+同步调用得出，Netflix API 每天通过 Hystrix 执行 10 亿次调用，每个服务实例有 40 个以上的线程池，每个线程池有 10 个左右的线程。）最后发现说，用 Hystrix 的额外开销，就是给请求带来了 3ms 左右的延时，最多延时在 10ms 以内，相比于可用性和稳定性的提升，这是可以接受的。 我们可以用 Hystrix semaphore 技术来实现对某个依赖服务的并发访问量的限制，而不是通过线程池/队列的大小来限制流量。 semaphore 技术可以用来限流和削峰，但是不能用来对调研延迟的服务进行 timeout 和隔离。 execution.isolation.strategy 设置为 SEMAPHORE，那么 Hystrix 就会用 semaphore 机制来替代线程池机制，来对依赖服务的访问进行限流。如果通过 semaphore 调用的时候，底层的网络调用延迟很严重，那么是无法 timeout 的，只能一直 block 住。一旦请求数量超过了 semaphore 限定的数量之后，就会立即开启限流。 接口限流 Demo 假设一个线程池大小为 8，等待队列的大小为 10。timeout 时长我们设置长一些，20s。 在 command 内部，写死代码，做一个 sleep，比如 sleep 3s。 withCoreSize：设置线程池大小 withMaxQueueSize：设置等待队列大小 withQueueSizeRejectionThreshold：这个与 withMaxQueueSize 配合使用，等待队列的大小，取得是这两个参数的较小值。 如果只设置了线程池大小，另外两个 queue 相关参数没有设置的话，等待队列是处于关闭的状态。 public class GetProductInfoCommand extends HystrixCommand { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\"GetProductInfoCommand\"); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ProductInfoService\")) .andCommandKey(KEY) // 线程池相关配置信息 .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 设置线程池大小为8 .withCoreSize(8) // 设置等待队列大小为10 .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(12)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(40) .withCircuitBreakerSleepWindowInMilliseconds(3000) // 设置超时时间 .withExecutionTimeoutInMilliseconds(20000) // 设置fallback最大请求并发数 .withFallbackIsolationSemaphoreMaxConcurrentRequests(30))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\"调用接口查询商品数据，productId=\" + productId); if (productId == -1L) { throw new Exception(); } // 请求过来，会在这里hang住3秒钟 if (productId == -2L) { TimeUtils.sleep(3); } String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(response); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\"降级商品\"); return productInfo; } } 我们模拟 25 个请求。前 8 个请求，调用接口时会直接被 hang 住 3s，那么后面的 10 个请求会先进入等待队列中等待前面的请求执行完毕。最后的 7 个请求过来，会直接被 reject，调用 fallback 降级逻辑。 @SpringBootTest @RunWith(SpringRunner.class) public class RejectTest { @Test public void testReject() { for (int i = 0; i HttpClientUtils.sendGetRequest(\"http://localhost:8080/getProductInfo?productId=-2\")).start(); } // 防止主线程提前结束执行 TimeUtils.sleep(50); } } 从执行结果中，我们可以明显看出一共打印出了 7 个降级商品。这也就是请求数超过线程池+队列的数量而直接被 reject 的结果。 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) {\"id\": -2, \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1, \"modifiedTime\": \"2017-01-01 12:00:00\", \"cityId\": 1, \"brandId\": 1} // 后面都是一些正常的商品信息，就不贴出来了 //... Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"advanced-java/docs/high-availability/hystrix-timeout.html":{"url":"advanced-java/docs/high-availability/hystrix-timeout.html","title":"基于 timeout 机制为服务接口调用超时提供安全保护","keywords":"","body":"基于 timeout 机制为服务接口调用超时提供安全保护 一般来说，在调用依赖服务的接口的时候，比较常见的一个问题就是超时。超时是在一个复杂的分布式系统中，导致系统不稳定，或者系统抖动。出现大量超时，线程资源会被 hang 死，从而导致吞吐量大幅度下降，甚至服务崩溃。 你去调用各种各样的依赖服务，特别是在大公司，你甚至都不认识开发一个服务的人，你都不知道那个人的技术水平怎么样，对那个人根本不了解。 Peter Steiner 说过，\"On the Internet, nobody knows you're a dog\"，也就是说在互联网的另外一头，你都不知道甚至坐着一条狗。 像特别复杂的分布式系统，特别是在大公司里，多个团队、大型协作，你可能都不知道服务是谁的，很可能说开发服务的那个哥儿们甚至是一个实习生。依赖服务的接口性能可能很不稳定，有时候 2ms，有时候 200ms，甚至 2s，都有可能。 如果你不对各种依赖服务接口的调用做超时控制，来给你的服务提供安全保护措施，那么很可能你的服务就被各种垃圾的依赖服务的性能给拖死了。大量的接口调用很慢，大量的线程被卡死。如果你做了资源的隔离，那么也就是线程池的线程被卡死，但其实我们可以做超时控制，没必要让它们全卡死。 TimeoutMilliseconds 在 Hystrix 中，我们可以手动设置 timeout 时长，如果一个 command 运行时间超过了设定的时长，那么就被认为是 timeout，然后 Hystrix command 标识为 timeout，同时执行 fallback 降级逻辑。 TimeoutMilliseconds 默认值是 1000，也就是 1000ms。 HystrixCommandProperties.Setter() ..withExecutionTimeoutInMilliseconds(int) TimeoutEnabled 这个参数用于控制是否要打开 timeout 机制，默认值是 true。 HystrixCommandProperties.Setter() .withExecutionTimeoutEnabled(boolean) 实例 Demo 我们在 command 中，将超时时间设置为 500ms，然后在 run() 方法中，设置休眠时间 1s，这样一个请求过来，直接休眠 1s，结果就会因为超时而执行降级逻辑。 public class GetProductInfoCommand extends HystrixCommand { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\"GetProductInfoCommand\"); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ProductInfoService\")) .andCommandKey(KEY) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(8) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(8)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(40) .withCircuitBreakerSleepWindowInMilliseconds(3000) // 设置是否打开超时，默认是true .withExecutionTimeoutEnabled(true) // 设置超时时间，默认1000(ms) .withExecutionTimeoutInMilliseconds(500) .withFallbackIsolationSemaphoreMaxConcurrentRequests(30))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\"调用接口查询商品数据，productId=\" + productId); // 休眠1s TimeUtils.sleep(1); String url = \"http://localhost:8081/getProductInfo?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(response); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\"降级商品\"); return productInfo; } } 在测试类中，我们直接发起请求。 @SpringBootTest @RunWith(SpringRunner.class) public class TimeoutTest { @Test public void testTimeout() { HttpClientUtils.sendGetRequest(\"http://localhost:8080/getProductInfo?productId=1\"); } } 结果中可以看到，打印出了降级商品相关信息。 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) {\"id\": 1, \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1, \"modifiedTime\": \"2017-01-01 12:00:00\", \"cityId\": 1, \"brandId\": 1} Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 07:49:11 "},"work/note/net/":{"url":"work/note/net/","title":"虚拟网络","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/net/vlan.html":{"url":"work/note/net/vlan.html","title":"vlan","keywords":"","body":"为什么需要VLAN 什么是VLAN？  VLAN（Virtual LAN），翻译成中文是“虚拟局域网”。LAN可以是由少数几台家用计算机构成的网络，也可以是数以百计的计算机构成的企业网络。VLAN所指的LAN特指使用路由器分割的网络——也就是广播域。  广播域，指的是广播帧（目标MAC地址全部为1）所能传递到的范围，亦即能够直接通信的范围。严格地说，并不仅仅是广播帧，多播帧（Multicast Frame）和目标不明的单播帧（Unknown Unicast Frame）也能在同一个广播域中畅行无阻。  二层交换机原本只能构建单一的广播域，不过使用VLAN功能后，它能够将网络分割成多个广播域。 未分割广播域时将会发生什么？  仅有一个广播域，有可能会影响到网络整体的传输性能。具体原因，请参看附图加深理解。通过vlan隔离p; Image - Phra Nakhon Khiri, Phetchaburi 图中，是一个由5台二层交换机（交换机1～5）连接了大量客户机构成的网络。假设这时，计算机A需要与计算机B通信。在基于以太网的通信中，必须在数据帧中指定目标 MAC地址才能正常通信，因此计算机A必须先广播“ARP请求（ARP Request）信息”，来尝试获取计算机B的MAC地址。 交换机1收到广播帧（ARP请求）后，会将它转发给除接收端口外的其他所有端口，也就是Flooding了。接着，交换机2收到广播帧后也会Flooding。交换机3、4、5也还会 Flooding。最终ARP请求会被转发到同一网络中的所有客户机上。 VLAN2.png 请大家注意一下，这个ARP请求原本是为了获得计算机B的MAC地址而发出的。也就是说：只要计算机B能收到就万事大吉了。可是事实上，数据帧却传遍整个网络，导致所有的 计算机都收到了它。如此一来，一方面广播信息消耗了网络整体的带宽，另一方面，收到广播信息的计算机还要消耗一部分CPU时间来对它进行处理。造成了网络带宽和CPU运 算能力的大量无谓消耗。 广播信息是那么经常发出的吗？ 读到这里，你也许会问：广播信息真是那么频繁出现的吗？ 答案是：是的！实际上广播帧会非常频繁地出现。利用TCP/IP协议栈通信时，除了前面出现的ARP外，还有可能需要发出DHCP、RIP等很多其他类型的广播信息。 ARP广播，是在需要与其他主机通信时发出的。当客户机请求DHCP服务器分配IP地址时，就必须发出DHCP的广播。而使用RIP作为路由协议时，每隔30秒路由器都会对邻近 的其他路由器广播一次路由信息。RIP以外的其他路由协议使用多播传输路由信息，这也会被交换机转发（Flooding）。除了TCP/IP以外，NetBEUI、IPX和Apple Talk等协 议也经常需要用到广播。例如在Windows下双击打开“网络计算机”时就会发出广播（多播）信息。（Windows XP除外……） 总之，广播就在我们身边。下面是一些常见的广播通信： ■ ARP请求：建立IP地址和MAC地址的映射关系。 ■ RIP：一种路由协议。 ■ DHCP：用于自动设定IP地址的协议。 ■ NetBEUI：Windows下使用的网络协议。 ■ IPX：Novell Netware使用的网络协议。 ■ Apple Talk：苹果公司的Macintosh计算机使用的网络协议。 注意：在使用ip协议时，到达数据链路层仍然需要mac寻址，此时需要建立IP地址和MAC地址的映射关系，则需要用到arp请求。arp报文会泛洪到整个网络。 如果整个网络只有一个广播域，那么一旦发出广播信息，就会传遍整个网络，并且对网络中的主机带来额外的负担。因此，在设计LAN时，需要注意如何才能有效地分割广播域。 广播域的分割与VLAN的必要性 分割广播域时，一般都必须使用到路由器。使用路由器后，可以以路由器上的网络接口（LAN Interface）为单位分割广播域。 但是，通常情况下路由器上不会有太多的网络接口，其数目多在1～4个左右。随着宽带连接的普及，宽带路由器（或者叫IP共享器）变得较为常见，但是需要注意的是，它们上面 虽然带着多个（一般为4个左右）连接LAN一侧的网络接口，但那实际上是路由器内置的交换机，并不能分割广播域。 况且使用路由器分割广播域的话，所能分割的个数完全取决于路由器的网络接口个数，使得用户无法自由地根据实际需要分割广播域。 与路由器相比，二层交换机一般带有多个网络接口。因此如果能使用它分割广播域，那么无疑运用上的灵活性会大大提高。 用于在二层交换机上分割广播域的技术，就是VLAN。通过利用VLAN，我们可以自由设计广播域的构成，提高网络设计的自由度。 实现VLAN的机制 实现VLAN的机制 在理解了“为什么需要VLAN”之后，接下来让我们来了解一下交换机是如何使用VLAN分割广播域的。 首先，在一台未设置任何VLAN的二层交换机上，任何广播帧都会被转发给除接收端口外的所有其他端口（Flooding）。例如，计算机A发送广播信息后，会被转发给端口2、3、4。 VLAN3.png 这时，如果在交换机上生成红、蓝两个VLAN；同时设置端口1、2属于红色VLAN、端口3、4属于蓝色VLAN。再从A发出广播帧的话，交换机就只会把它转发给同属于一个VLAN的其他 端口——也就是同属于红色VLAN的端口2，不会再转发给属于蓝色VLAN的端口。 同样，C发送广播信息时，只会被转发给其他属于蓝色VLAN的端口，不会被转发给属于红色VLAN的端口。 VLAN4.png 就这样，VLAN通过限制广播帧转发的范围分割了广播域。上图中为了便于说明，以红、蓝两色识别不同的VLAN，在实际使用中则是用“VLAN ID”来区分的。 直观地描述VLAN 如果要更为直观地描述VLAN的话，我们可以把它理解为将一台交换机在逻辑上分割成了数台交换机。在一台交换机上生成红、蓝两个VLAN，也可以看作是将一 台交换机换做一红一蓝两台虚拟的交换机。VLAN5.png 在红、蓝两个VLAN之外生成新的VLAN时，可以想象成又添加了新的交换机。 但是，VLAN生成的逻辑上的交换机是互不相通的。因此，在交换机上设置VLAN后，如果未做其他处理，VLAN间是无法通信的。 明明接在同一台交换机上，但却偏偏无法通信——这个事实也许让人难以接受。但它既是VLAN方便易用的特征，又是使VLAN令人难以理解的原因。 2.3、需要VLAN间通信时怎么办？ 那么，当我们需要在不同的VLAN间通信时又该如何是好呢？ 请大家再次回忆一下：VLAN是广播域。而通常两个广播域之间由路由器连接，广播域之间来往的数据包都是由路由器中继的。因此，VLAN间的通信也需要路由器提供中继服务， 这被称作“VLAN间路由”。 VLAN间路由，可以使用普通的路由器，也可以使用三层交换机。其中的具体内容，等有机会再细说吧。在这里希望大家先记住不同VLAN间互相通信时需要用到路由功能。 VLAN的访问链接 交换机的端口 交换机的端口，可以分为以下两种： ■ 访问链接（Access Link） ■ 汇聚链接（Trunk Link） 接下来就让我们来依次学习这两种不同端口的特征。这一讲，首先学习“访问链接”。 访问链接 访问链接，指的是“只属于一个VLAN，且仅向该VLAN转发数据帧”的端口。在大多数情况下，访问链接所连的是客户机。 通常设置VLAN的顺序是： ●生成VLAN ● 设定访问链接（决定各端口属于哪一个VLAN） 设定访问链接的手法，可以是事先固定的、也可以是根据所连的计算机而动态改变设定。前者被称为“静态VLAN”、后者自然就是“动态VLAN”了。 静态VLAN 静态VLAN又被称为基于端口的VLAN（Port Based VLAN）。顾名思义，就是明确指定各端口属于哪个VLAN的设定方法。 由于需要一个个端口地指定，因此当网络中的计算机数目超过一定数字（比如数百台）后，设定操作就会变得烦杂无比。并且，客户机每次变更所连端口， 都必须同时更改该端口所属VLAN的设定——这显然静态VLAN不适合那些需要频繁改变拓补结构的网络。 动态VLAN 另一方面，动态VLAN则是根据每个端口所连的计算机，随时改变端口所属的VLAN。这就可以避免上述的更改设定之类的操作。动态VLAN可以大致分为3类： ● 基于MAC地址的VLAN（MAC Based VLAN） ● 基于子网的VLAN（Subnet Based VLAN） ● 基于用户的VLAN（User Based VLAN） 其间的差异，主要在于根据OSI参照模型哪一层的信息决定端口所属的VLAN。 ①、基于MAC地址的VLAN，就是通过查询并记录端口所连计算机上网卡的MAC地址来决定端口的所属。假定有一个MAC地址“A”被交换机设定为属于VLAN“10”， 那么不论MAC地址为“A”的这台计算机连在交换机哪个端口，该端口都会被划分到VLAN10中去。计算机连在端口1时，端口1属于VLAN10；而计算机连在端口2时， 则是端口2属于VLAN10。 ②、基于子网的VLAN，则是通过所连计算机的IP地址，来决定端口所属VLAN的。不像基于MAC地址的VLAN，即使计算机因为交换了网卡或是其他原因导致MAC地址改变，只要它的IP地址不变， 就仍可以加入原先设定的VLAN。 因此，与基于MAC地址的VLAN相比，能够更为简便地改变网络结构。IP地址是OSI参照模型中第三层的信息，所以我们可以理解为基于子网的VLAN是一种在OSI的第三层设定访问链接的方法。 ③、基于用户的VLAN，则是根据交换机各端口所连的计算机上当前登录的用户，来决定该端口属于哪个VLAN。这里的用户识别信息，一般是计算机操作系统登录的用户，比如可以是Windows 域中使用的用户名。这些用户名信息，属于OSI第四层以上的信息。 总的来说，决定端口所属VLAN时利用的信息在OSI中的层面越高，就越适于构建灵活多变的网络。 访问链接的总结 综上所述，设定访问链接的手法有静态VLAN和动态VLAN两种，其中动态VLAN又可以继续细分成几个小类。 其中基于子网的VLAN和基于用户的VLAN有可能是网络设备厂商使用独有的协议实现的，不同厂商的设备之间互联有可能出现兼容性问题；因此在选择交换机时，一定要注意事先确认。 下表总结了静态VLAN和动态VLAN的相关信息。 种类 解说 静态VLAN（基于端口的VLAN） 将交换机的各端口固定指派给VLAN 动态VLAN 基于MAC地址的VLAN 根据各端口所连计算机的MAC地址设定 基于子网的VLAN 根据各端口所连计算机的IP地址设定 基于用户的VLAN 根据端口所连计算机上登录用户设定 VLAN的汇聚链接 设置跨越多台交换机的VLAN 到此为止，我们学习的都是使用单台交换机设置VLAN时的情况。那么，如果需要设置跨越多台交换机的VLAN时又如何呢？ 在规划企业级网络时，很有可能会遇到隶属于同一部门的用户分散在同一座建筑物中的不同楼层的情况，这时可能就需要考虑到如何跨越多台交换机设置VLAN的问题了。 假设有如下图所示的网络，且需要将不同楼层的A、C和B、D设置为同一个VLAN。 VLAN8.png 这时最关键的就是“交换机1和交换机2该如何连接才好呢？” 最 简单的方法，自然是在交换机1和交换机2上各设一个红、蓝VLAN专用的接口并互联了。 VLAN9.png 但是，这个办法从扩展性和管理效率来看都不好。例如，在现有网络基础上再新建VLAN时，为了让这个VLAN能够互通，就需要在交换机间连接新的网线。建筑物楼层间的纵向布线 是比较麻烦的，一般不能由基层管理人员随意进行。并且，VLAN越多，楼层间（严格地说是交换机间）互联所需的端口也越来越多，交换机端口的利用效率低是对资源的一种浪费、 也限制了网络的扩展。 为了避免这种低效率的连接方式，人们想办法让交换机间互联的网线集中到一根上，这时使用的就是汇聚链接（Trunk Link）。 何谓汇聚链接？ 汇聚链接（Trunk Link）指的是能够转发多个不同VLAN的通信的端口。 汇聚链路上流通的数据帧，都被附加了用于识别分属于哪个VLAN的特殊信息。 现在再让我们回过头来考虑一下刚才那个网络如果采用汇聚链路又会如何呢？用户只需要简单地将交换机间互联的端口设定为汇聚链接就可以了。这时使用的网线还是普通的UTP线， 而不是什么其他的特殊布线。图例中是交换机间互联，因此需要用交叉线来连接。 接下来，让我们具体看看汇聚链接是如何实现跨越交换机间的VLAN的。 ①、A发送的数据帧从交换机1经过汇聚链路到达交换机2时，在数据帧上附加了表示属于红色VLAN的标记。 ②、交换机2收到数据帧后，经过检查VLAN标识发现这个数据帧是属于红色VLAN的。 ③、因此去除标记后根据需要将复原的数据帧只转发给其他属于红色VLAN的端口。 这时的转送，是指经过确认目标MAC地址并与MAC地址列表比对后只转发给目标MAC地址所连的端口。 只有当数据帧是一个广播帧、多播帧或是目标不明的帧时，它才会被转发到所有属于红色VLAN的端口。 同理，蓝色VLAN发送数据帧时的情形也与此相同。 VLAN10.png 通过汇聚链路时附加的VLAN识别信息，有可能支持标准的“IEEE 802.1Q”协议，也可能是Cisco产品独有的“ISL（Inter Switch Link）”。如果交换机支持这些规格， 那么用户就能够高效率地构筑横跨多台交换机的VLAN。 另外，汇聚链路上流通着多个VLAN的数据，自然负载较重。因此，在设定汇聚链接时，有一个前提就是必须支持100Mbps以上的传输速度。 另外，默认条件下，汇聚链接会转发交换机上存在的所有VLAN的数据。换一个角度看，可以认为汇聚链接（端口）同时属于交换机上所有的VLAN。由于实际应用中很 可能并不需要转发所有VLAN的数据，因此为了减轻交换机的负载、也为了减少对带宽的浪费，我们可以通过用户设定限制能够经由汇聚链路互联的VLAN。 关于IEEE802.1Q和ISL的具体内容，将在下一讲中提到。 VLAN的汇聚方式（IEEE802.1Q与ISL） 汇聚方式 在交换机的汇聚链接上，可以通过对数据帧附加VLAN信息，构建跨越多台交换机的VLAN。 附加VLAN信息的方法，最具有代表性的有： ● IEEE802.1Q ● ISL 现在就让我们看看这两种协议分别如何对数据帧附加VLAN信息。 IEEE802.1Q IEEE802.1Q，俗称“Dot One Q”，是经过IEEE认证的对数据帧附加VLAN识别信息的协议。 在此，请大家先回忆一下以太网数据帧的标准格式。 IEEE802.1Q所附加的VLAN识别信息，位于数据帧中“发送源MAC地址”与“类别域（Type Field）”之间。具体内容为2字节的TPID和2字节的TCI，共计4字节。 在数据帧中添加了4字节的内容，那么CRC值自然也会有所变化。这时数据帧上的CRC是插入TPID、TCI后，对包括它们在内的整个数据帧重新计算后所得的值。 VLAN11.png 而当数据帧离开汇聚链路时，TPID和TCI会被去除，这时还会进行一次CRC的重新计算。 TPID的值，固定为0x8100。交换机通过TPID，来确定数据帧内附加了基于IEEE802.1Q的VLAN信息。而实质上的VLAN ID，是TCI中的12位元。由于总共有12位，因此最多可供识别4096个VLAN。 基于IEEE802.1Q附加的VLAN信息，就像在传递物品时附加的标签。因此，它也被称作“标签型VLAN（Tagging VLAN）”。 ISL（Inter Switch Link） ISL，是Cisco产品支持的一种与IEEE802.1Q类似的、用于在汇聚链路上附加VLAN信息的协议。 使用ISL后，每个数据帧头部都会被附加26字节的“ISL包头（ISL Header）”，并且在帧尾带上通过对包括ISL包头在内的整个数据帧进行计算后得到的4字节CRC值。换而言之，就是总共增加了30字节的信息。 在使用ISL的环境下，当数据帧离开汇聚链路时，只要简单地去除ISL包头和新CRC就可以了。由于原先的数据帧及其CRC都被完整保留，因此无需重新计算CRC。 VLAN12.png ISL有如用ISL包头和新CRC将原数据帧整个包裹起来，因此也被称为“封装型VLAN（Encapsulated VLAN）”。 需要注意的是，不论是IEEE802.1Q的“Tagging VLAN”，还是ISL的“Encapsulated VLAN”，都不是很严密的称谓。不同的书籍与参考资料中，上述词语有可能被混合使用，因此需要大家在学习时格外注意。 并且由于ISL是Cisco独有的协议，因此只能用于Cisco网络设备之间的互联。 VLAN间路由 VLAN间路由的必要性 根据目前为止学习的知识，我们已经知道两台计算机即使连接在同一台交换机上，只要所属的VLAN不同就无法直接通信。 接下来我们将要学习的就是如何在不同的VLAN间进行路由，使分属不同VLAN的主机能够互相通信。 首先，先来复习一下为什么不同VLAN间不通过路由就无法通信。在LAN内的通信，必须在数据帧头中指定通信目标的MAC地址。而为了获取MAC地址，TCP/IP协议下使用的是ARP。ARP解析MAC地址的方法， 则是通过广播。也就是说，如果广播报文无法到达，那么就无从解析MAC地址，亦即无法直接通信。 计算机分属不同的VLAN，也就意味着分属不同的广播域，自然收不到彼此的广播报文。因此，属于不同VLAN的计算机之间无法直接互相通信。为了能够在VLAN间通信，需要利用OSI参照模型中更高一层—— 网络层的信息（IP地址）来进行路由。关于路由的具体内容，以后有机会再详细解说吧。 路由功能，一般主要由路由器提供。但在今天的局域网里，我们也经常利用带有路由功能的交换机——三层交换机（Layer 3 Switch）来实现。接下来就让我们分别看看使用路由器和三层交换机进行VLAN间路由时的情况。 使用路由器进行VLAN间路由 在使用路由器进行VLAN间路由时，与构建横跨多台交换机的VLAN时的情况类似，我们还是会遇到“该如何连接路由器与交换机”这个问题。路由器和交换机的接线方式，大致有以下两种： ● 将路由器与交换机上的每个VLAN分别连接 ● 不论VLAN有多少个，路由器与交换机都只用一条网线连接 ①、最容易想到的，当然还是“把路由器和交换机以VLAN为单位分别用网线连接”了。将交换机上用于和路由器互联的每个端口设为访问链接，然后分别用网线与路由器上的独立端口互联。如下图所示， 交换机上有2个VLAN，那么就需要在交换机上预留2个端口用于与路由器互联；路由器上同样需要有2个端口；两者之间用2条网线分别连接。 VLAN13.png 如果采用这个办法，大家应该不难想象它的扩展性很成问题。每增加一个新的VLAN，都需要消耗路由器的端口和交换机上的访问链接，而且还需要重新布设一条网线。而路由器，通常不会带有太多LAN接口的。 新建VLAN时，为了对应增加的VLAN所需的端口，就必须将路由器升级成带有多个LAN接口的高端产品，这部分成本、还有重新布线所带来的开销，都使得这种接线法成为一种不受欢迎的办法。 ②、那么，第二种办法“不论VLAN数目多少，都只用一条网线连接路由器与交换机”呢？当使用一条网线连接路由器与交换机、进行VLAN间路由时，需要用到汇聚链接。 具体实现过程为：首先将用于连接路由器的交换机端口设为汇聚链接，而路由器上的端口也必须支持汇聚链路。双方用于汇聚链路的协议自然也必须相同。接着在路由器上定义对应各个VLAN的 “子接口（Sub Interface）”。尽管实际与交换机连接的物理端口只有一个，但在理论上我们可以把它分割为多个虚拟端口。 VLAN14.png VLAN将交换机从逻辑上分割成了多台，因而用于VLAN间路由的路由器，也必须拥有分别对应各个VLAN的虚拟接口。 采用这种方法的话，即使之后在交换机上新建VLAN，仍只需要一条网线连接交换机和路由器。用户只需要在路由器上新设一个对应新VLAN的子接口就可以了。 与前面的方法相比，这种方法扩展性要强得多，也不用担心需要升级LAN接口数不足的路由器或是重新布线。 同一VLAN内的通信时数据的流程 接下来，我们继续学习使用汇聚链路连接交换机与路由器时，VLAN间路由是如何进行的。如下图所示，为各台计算机以及路由器的子接口设定IP地址。 VLAN15.png 红色VLAN（VLAN ID=1）的网络地址为192.168.1.0/24，蓝色VLAN（VLAN ID=2）的网络地址为192.168.2.0/24。各计算机的MAC地址分别为A/B/C/D，路由器汇聚链接端口的MAC地址为R。 交换机通过对各端口所连计算机MAC地址的学习，生成如下的MAC地址列表。 端口 MAC地址 VLAN 1 A 1 2 B 1 3 C 2 4 D 2 5 － － 6 R 汇聚 首先考虑计算机A与同一VLAN内的计算机B之间通信时的情形。 （1）、计算机A发出ARP请求信息，请求解析B的MAC地址。 （2）、交换机收到数据帧后，检索MAC地址列表中与收信端口同属一个VLAN的表项。 （3）、结果发现，计算机B连接在端口2上，于是交换机将数据帧转发给端口2，最终计算机B收到该帧。 收发信双方同属一个VLAN之内的通信，一切处理均在交换机内完成。 VLAN16.png 不同VLAN间通信时数据的流程 接下来是这一讲的核心内容，不同VLAN间的通信。让我们来考虑一下计算机A与计算机C之间通信时的情况。 VLAN17.png （1）、计算机A从通信目标的IP地址（192.168.2.1）得出C与本机不属于同一个网段。因此会向设定的默认网关（Default Gateway，GW）转发数据帧。在发送数据帧之前，需要先用ARP获取路由器的MAC地址。 （2）、得到路由器的MAC地址R后，接下来就是按图中所示的步骤发送往C去的数据帧。①的数据帧中，目标MAC地址是路由器的地址R、但内含的目标IP地址仍是最终要通信的对象C的地址。这一部分的内容， 涉及到局域网内经过路由器转发时的通信步骤，有机会再详细解说吧。 （3）、交换机在端口1上收到①的数据帧后，检索MAC地址列表中与端口1同属一个VLAN的表项。由于汇聚链路会被看作属于所有的VLAN，因此这时交换机的端口6也属于被参照对象。这样交换机就知道往 MAC地址R发送数据帧，需要经过端口6转发。 从端口6发送数据帧时，由于它是汇聚链接，因此会被附加上VLAN识别信息。由于原先是来自红色VLAN的数据帧，因此如图中②所示，会被加上红色VLAN的识别信息后进入汇聚链路。 （4）、路由器收到②的数据帧后，确认其VLAN识别信息，由于它是属于红色VLAN的数据帧，因此交由负责红色VLAN的子接口接收。 接着，根据路由器内部的路由表，判断该向哪里中继。 由于目标网络192.168.2.0/24是蓝色VLAN，且该网络通过子接口与路由器直连，因此只要从负责蓝色VLAN的子接口转发就可以了。这时，数据帧的目标MAC地址被改写成计算机C的目标地址；并且由于需要经过 汇聚链路转发，因此被附加了属于蓝色VLAN的识别信息。这就是图中③的数据帧。 （5）、交换机收到③的数据帧后，根据VLAN标识信息从MAC地址列表中检索属于蓝色VLAN的表项。由于通信目标——计算机C连接在端口3上、且端口3为普通的访问链接，因此交换机会将数据帧除去VLAN识别信息后（数据帧④）转发给端口3，最终计算机C才能成功地收到这个数据帧。 进行VLAN间通信时，即使通信双方都连接在同一台交换机上，也必须经过：“发送方——交换机——路由器——交换机——接收方”这样一个流程。 三层交换机 使用路由器进行VLAN间路由时的问题 现在，我们知道只要能提供VLAN间路由，就能够使分属不同VLAN的计算机互相通信。 但是，如果使用路由器进行VLAN间路由的话，随着VLAN之间流量的不断增加，很可能导致路由器成为整个网络的瓶颈。 交换机使用被称为ASIC（Application Specified Integrated Circuit）的专用硬件芯片处理数据帧的交换操作，在很多机型上都能实现以缆线速度（Wired Speed）交换。而路由器，则基本上是基于软件处理的。 即使以缆线速度接收到数据包，也无法在不限速的条件下转发出去，因此会成为速度瓶颈。就VLAN间路由而言，流量会集中到路由器和交换机互联的汇聚链路部分，这一部分尤其特别容易成为速度瓶颈。 并且从硬件上看，由于需要分别设置路由器和交换机，在一些空间狭小的环境里可能连设置的场所都成问题。 三层交换机（Layer 3 Switch） 为了解决上述问题，三层交换机应运而生。三层交换机，本质上就是“带有路由功能的（二层）交换机”。路由属于OSI参照模型中第三层网络层的功能，因此带有第三层路由功能的交换机才被称为“三层交换机”。 关于三层交换机的内部结构，可以参照下面的简图。 VLAN18.png 在一台本体内，分别设置了交换机模块和路由器模块；而内置的路由模块与交换模块相同，使用ASIC硬件处理路由。因此，与传统的路由器相比，可以实现高速路由。并且，路由与交换模块是汇聚链接的，由于是内部连接， 可以确保相当大的带宽。 使用三层交换机进行VLAN间路由（VLAN内通信） 在三层交换机内部数据究竟是怎样传播的呢？基本上，它和使用汇聚链路连接路由器与交换机时的情形相同。 假设有如下图所示的4台计算机与三层交换机互联。当使用路由器连接时，一般需要在LAN接口上设置对应各VLAN的子接口；而三层交换机则是在内部生成“VLAN接口（VLAN Interface）”。 VLAN接口，是用于各VLAN收发数据的接口。 注：在Cisco的Catalyst系列交换机上，VLAN Interface被称为SVI——Switched Virtual Interface VLAN19.png 为了与使用路由器进行VLAN间路由对比，让我们同样来考虑一下计算机A与计算机B之间通信时的情况。首先是目标地址为B的数据帧被发到交换机；通过检索同一VLAN的MAC地址列表发现计算机B连在交换 机的端口2上；因此将数据帧转发给端口2。 使用三层交换机进行VLAN间路由（VLAN间通信） 接下来设想一下计算机A与计算机C间通信时的情形。针对目标IP地址，计算机A可以判断出通信对象不属于同一个网络，因此向默认网关发送数据（Frame 1）。 交换机通过检索MAC地址列表后，经由内部汇聚链接，将数据帧转发给路由模块。在通过内部汇聚链路时，数据帧被附加了属于红色VLAN的VLAN识别信息（Frame 2）。 路由模块在收到数据帧时，先由数据帧附加的VLAN识别信息分辨出它属于红色VLAN，据此判断由红色VLAN接口负责接收并进行路由处理。因为目标网络192.168.2.0/24 是直连路由器的网络、且对应蓝色VLAN； 因此，接下来就会从蓝色VLAN接口经由内部汇聚链路转发回交换模块。在通过汇聚链路时，这次数据帧被附加上属于蓝色VLAN的识别信息（Frame 3）。 交换机收到这个帧后，检索蓝色VLAN的MAC地址列表，确认需要将它转发给端口3。由于端口3是通常的访问链接，因此转发前会先将VLAN识别信息除去（Frame 4）。最终，计算机C成功地收到交换机转发来的数据帧。 VLAN20.png 整体的流程，与使用外部路由器时的情况十分相似——都需要经过“发送方→交换模块→路由模块→交换模块→接收方”这样的流程。 加速VLAN间通信的手段 流（Flow） 根据到此为止的学习，我们已经知道VLAN间路由，必须经过外部的路由器或是三层交换机的内置路由模块。但是，有时并不是所有的数据都需要经过路由器（或路由模块）。 例如，使用FTP（File Transfer Protocol）传输容量为数MB以上的较大的文件时，由于MTU的限制，IP协议会将数据分割成小块后传输、并在接收方重新组合。这些被分割的数据，“发送的目标”是完全相同的。发送目标相同，也就意味着同样的目标IP地址、目标端口号（注：特别强调一下，这里指的是TCP/UDP端口）。自然，源IP地址、源端口号也应该相同。这样一连串的数据流被称为“流（Flow）”。 VLAN21.png 只要将流最初的数据正确地路由以后，后继的数据理应也会被同样地路由。 据此，后继的数据不再需要路由器进行路由处理；通过省略反复进行的路由操作，可以进一步提高VLAN间路由的速度。 加速VLAN间路由的机制 接下来，让我们具体考虑一下该如何使用三层交换机进行高速VLAN间路由。 首先，整个流的第一块数据，照常由交换机转发→路由器路由→再次由交换机转发到目标所连端口。这时，将第一块数据路由的结果记录到缓存里保存下来。需要记录的信息有： ● 目标IP地址 ● 源IP地址 ● 目标TCP/UDP端口号 ● 源TCP/UDP端口号 ● 接收端口号（交换机） ● 转发端口号（交换机） ● 转发目标MAC地址 ... 等等。 同一个流的第二块以后的数据到达交换机后，直接通过查询先前保存在缓存中的信息查出“转发端口号”后就可以转发给目标所连端口了。 这样一来，就不需要再一次次经由内部路由模块中继，而仅凭交换机内部的缓存信息就足以判断应该转发的端口。 这时，交换机会对数据帧进行由路由器中继时相似的处理，例如改写MAC地址、IP包头中的TTL和Check Sum校验码信息等。 VLAN22.png 通过在交换机上缓存路由结果，实现了以缆线速度（Wired Speed）接收发送方传输来数据的数据、并且能够全速路由、转发给接收方。 需要注意的是，类似的加速VLAN间路由的手法多由各厂商独有的技术所实现，并且该功能的称谓也因厂商而异。例如，在Cisco的Catalyst系列交换机上，这种功能被称为“多层交换（Multi Layer Switching）”。 另外，除了三层交换机的内部路由模块，外部路由器中的某些机型也支持类似的高速VLAN间路由机制。 传统型路由器存在的意义 路由器的必要性 三层交换机的价格，在问世之初非常昂贵，但是现在它们的价格已经下降了许多。目前国外一些廉价机型的售价，折合成人民币后仅为一万多元，而且还在继续下降中。 既然三层交换机能够提供比传统型路由器更为高速的路由处理，那么网络中还有使用路由器的必要吗？ 答案是：“是”。 使用路由器的必要性，主要表现在以下几个方面： ■ 用于与WAN连接 三层交换机终究是“交换机”。也就是说，绝大多数机型只配有LAN（以太网）接口。在少数高端交换机上也有用于连接WAN的串行接口或是ATM接口，但在大多数情况下，连接WAN还是需要用到路由器。 ■ 保证网络安全 在三层交换机上，通过数据包过滤也能确保一定程度的网络安全。但是使用路由器所提供的各种网络安全功能，用户可以构建更为安全可靠的网络。 路由器提供的网络安全功能中，除了最基本的数据包过滤功能外，还能基于IPSec构建*（Virtual Private Network）、利用RADIUS进行用户认证等等。 ■ 支持除TCP/IP以外的网络架构 尽管TCP/IP已经成为当前网络协议架构的主流，但还有不少网络利用Novell Netware下的IPX/SPX或Macintosh下的Appletalk等网络协议。三层交换机中，除了部分高端机型外基本上还只支持TCP/IP。因此，在需要使用除TCP/IP之外其他网络协议的环境下，路由器还是必不可少的。 注：在少数高端交换机上，也能支持上述路由器的功能。例如Cisco的Catalyst6500系列，就可以选择与WAN连接的接口模块；还有可选的基于IPSec实现*的模块；并且也能支持TCP/IP以外的其他网络协议。 路由器和交换机配合构建LAN的实例 下面让我们来看一个路由器和交换机搭配构建LAN的实例。 VLAN23.png 利用在各楼层配置的二层交换机定义VLAN，连接TCP/IP客户计算机。各楼层间的VLAN间通信，利用三层交换机的高速路由加以实现。如果网络环境要求高可靠性，还可以考虑冗余配置三层交换机。 与WAN的连接，则通过带有各种网络接口的路由器进行。并且，通过路由器的数据包过滤和*等功能实现网络安全。此外，使用路由器还能支持Novell Netware等TCP/IP之外的网络。 只有在充分掌握了二层、三层交换机以及传统路由器的基础上，才能做到物竞其用，构筑出高效率、高性价比的网络。 使用VLAN设计局域网 使用VLAN设计局域网的特点 通过使用VLAN构建局域网，用户能够不受物理链路的限制而自由地分割广播域。 另外，通过先前提到的路由器与三层交换机提供的VLAN间路由，能够适应灵活多变的网络构成。 但是，由于利用VLAN容易导致网络构成复杂化，因此也会造成整个网络的组成难以把握。 可以这样说，在利用VLAN时，除了有： ●网络构成灵活多变 这个优点外，还搭配着： ●网络构成复杂化 这个缺点。 下面，就让我们来看看具体的实例。 不使用VLAN的局域网中网络构成的改变 假设有如图所示的由1台路由器、2台交换机构成的“不使用VLAN构建”的网络。 VLAN24.png 图中的路由器，带有2个LAN接口。左侧的网络是192.168.1.0/24，右侧是192.168.2.0/24。 现在如果想将192.168.1.0/24这个网络上的计算机A转移到192.168.2.0/24上去，就需要改变物理连接、将A接到右侧的交换机上。 并且，当需要新增一个地址为192.168.3.0/24的网络时，还要在路由器上再占用一个LAN接口并添置一台交换机。而由于这台路由器上只带了2个LAN接口，因此为了新增网络还必须将路由器升级为带有3个以上LAN接口的产品。 使用VLAN的局域网中网络构成的改变 接下来再假设有一个由1台路由器、2台交换机构成的“使用VLAN”的局域网。交换机与交换机、交换机与路由器之间均为汇聚链路；并且假设192.168.1.0/24对应红色VLAN、192.168.2.0/24对应蓝色VLAN。 VLAN25.png 需要将连接在交换机1上192.168.1.0/24这个网段的计算机A转属192.168.2.0/24时，无需更改物理布线。只要在交换机上生成蓝色VLAN，然后将计算机A所连的端口1加入到蓝色VLAN中去，使它成为访问链接即可。 然后，根据需要设定计算机A的IP地址、默认网关等信息就可以了。如果IP地址相关的设定是由DHCP获取的，那么在客户机方面无需进行任何设定修改，就可以在不同网段间移动。 利用VLAN后，我们可以在免于改动任何物理布线的前提下，自由进行网络的逻辑设计。如果所处的工作环境恰恰需要经常改变网络布局，那么利用VLAN的优势就非常明显了。 并且，当需要新增一个地址为192.168.3.0/24的网段时，也只需要在交换机上新建一个对应192.168.3.0/24的VLAN，并将所需的端口加入它的访问链路就可以了。 如果网络环境中还需要利用外部路由器，则只要在路由器的汇聚端口上新增一个子接口的设定就可以完成全部操作，而不需要消耗更多的物理接口（LAN接口）。要使用的是三层交换机内部的路由模块，则只需要新设一个VLAN接口即可。 网络环境的成长，往往是难以预测的，很可能经常会出现需要分割现有网络或是增加新网络的情况。而充分活用VLAN后，就可以轻易地解决这些问题。 利用VLAN而导致的网络结构复杂化 虽然利用VLAN可以灵活地构建网络，但是同时，它也带来了网络结构复杂化的问题。 特别是由于数据流纵横交错，一旦发生故障时，准确定位并排除故障会比较困难。 为了便于理解数据流向的复杂化，假设有下图所示的网络。计算机A向计算机C发送数据时，数据流的整体走向如下： 计算机A→交换机1→路由器→交换机1→交换机2→计算机C VLAN26.png （1）、首先计算机A向交换机1送出数据（①） （2）、其后数据被转发给路由器（②）进行VLAN间路由。 （3）、路由后的数据，再从汇聚链路返回交换机1（③）。 （4）、由于通信目标计算机C并不直连在交换机1上，因此还需要经过汇聚链路转发到交换机2（④）。 （5）、在交换机2上，数据最终被转发到C所连的端口2上，这才完成整个流程（⑤）。 在这个例子中，仅由2台交换机构成网络，其数据流已经如此复杂，如果构建横跨多台交换机的VLAN的话，每个数据流的流向显然会更加难以把握。 网络的逻辑结构与物理结构 为了对应日渐复杂化的数据流，管理员需要从“逻辑结构”与“物理结构”两方面入手，把握好网络的现状。 物理结构，指的是从物理层和数据链路层观察到的网络的现状，表示了网络的物理布线形态和VLAN的设定等等。 而逻辑结构，则表示从网络层以上的层面观察到的网络结构。下面我们就试着以路由器为中心分析一个IP网络的逻辑结构。 还是先前的那个例子，描绘了布线形态和VLAN设定的“物理结构”如下图所示。 VLAN27.png 分析这个物理结构并转换成以路由器为中心的逻辑结构后，会得到如下的逻辑结构图。当我们需要进行路由或是数据包过滤的设定时，都必须在逻辑结构的基础上进行。 VLAN28.png 把握这两种网络结构图的区别是十分重要的，特别是在VLAN和三层交换机大行其道的现代企业级网络当中。 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/net/vxlan.html":{"url":"work/note/net/vxlan.html","title":"vxlan","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"./":{"url":"./","title":"传统网络","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 21:16:26 "},"git-tips/":{"url":"git-tips/","title":"git-tips","keywords":"","body":"Git的奇技淫巧:see_no_evil: Git常用命令集合，Fork于tips项目 Git是一个 “分布式版本管理工具”，简单的理解版本管理工具：大家在写东西的时候都用过 “回撤” 这个功能，但是回撤只能回撤几步，假如想要找回我三天之前的修改，光用 “回撤” 是找不回来的。而 “版本管理工具” 能记录每次的修改，只要提交到版本仓库，你就可以找到之前任何时刻的状态（文本状态）。 下面的内容就是列举了常用的 Git 命令和一些小技巧，可以通过 \"页面内查找\" 的方式进行快速查询：Ctrl/Command+f。 开卷必读 如果之前未使用过 Git，可以学习 Git 小白教程入门 一定要先测试命令的效果后，再用于工作环境中，以防造成不能弥补的后果！到时候别拿着砍刀来找我 所有的命令都在git version 2.7.4 (Apple Git-66)下测试通过 统一概念： 工作区：改动（增删文件和内容） 暂存区：输入命令：git add 改动的文件名，此次改动就放到了 ‘暂存区’ 本地仓库(简称：本地)：输入命令：git commit 此次修改的描述，此次改动就放到了 ’本地仓库’，每个 commit，我叫它为一个 ‘版本’。 远程仓库(简称：远程)：输入命令：git push 远程仓库，此次改动就放到了 ‘远程仓库’（GitHub 等) commit-id：输出命令：git log，最上面那行 commit xxxxxx，后面的字符串就是 commit-id 如果喜欢这个项目，欢迎 Star、提交 Pr、反馈问题😊 目录 展示帮助信息 回到远程仓库的状态 重设第一个commit 展示工作区和暂存区的不同 展示暂存区和最近版本的不同 展示暂存区、工作区和最近版本的不同 快速切换分支上一个分支 删除已经合并到 master 的分支 展示本地分支关联远程仓库的情况 关联远程分支 列出所有远程分支 列出本地和远程分支 创建并切换到本地分支 从远程分支中创建并切换到本地分支 删除本地分支 删除远程分支 重命名本地分支 查看标签 查看标签详细信息 本地创建标签 推送标签到远程仓库 删除本地标签 删除远程标签 切回到某个标签 放弃工作区的修改 恢复删除的文件 以新增一个 commit 的方式还原某一个 commit 的修改 回到某个 commit 的状态，并删除后面的 commit 修改上一个 commit 的描述 查看 commit 历史 显示本地更新过 HEAD 的 git 命令记录 修改作者名 修改远程仓库的 url 增加远程仓库 列出所有远程仓库 查看两个星期内的改动 把 A 分支的某一个 commit，放到 B 分支上 给 git 命令起别名 存储当前的修改，但不用提交 commit 保存当前状态，包括 untracked 的文件 展示所有 stashes 回到某个 stash 的状态 回到最后一个 stash 的状态，并删除这个 stash 删除所有的 stash 从 stash 中拿出某个文件的修改 展示所有 tracked 的文件 展示所有 untracked 的文件 展示所有忽略的文件 强制删除 untracked 的文件 强制删除 untracked 的目录 展示简化的 commit 历史 查看某段代码是谁写的 把某一个分支到导出成一个文件 从包中导入分支 执行 rebase 之前自动 stash 从远程仓库根据 ID，拉下某一状态，到本地分支 详细展示一行中的修改 清除 .gitignore 文件中记录的文件 展示所有 alias 和 configs 展示忽略的文件 commit 历史中显示 Branch1 有的，但是 Branch2 没有 commit 在 commit log 中显示 GPG 签名 删除全局设置 新建并切换到新分支上，同时这个分支没有任何 commit 展示任意分支某一文件的内容 clone 下来指定的单一分支 忽略某个文件的改动 忽略文件的权限变化 以最后提交的顺序列出所有 Git 分支 在 commit log 中查找相关内容 把暂存区的指定 file 放到工作区中 强制推送 一图详解 优雅的提交Commit信息 联系我 展示帮助信息 git help -g The command output as below: The common Git guides are: attributes Defining attributes per path cli Git command-line interface and conventions core-tutorial A Git core tutorial for developers cvs-migration Git for CVS users diffcore Tweaking diff output everyday A useful minimum set of commands for Everyday Git glossary A Git Glossary hooks Hooks used by Git ignore Specifies intentionally untracked files to ignore modules Defining submodule properties namespaces Git namespaces repository-layout Git Repository Layout revisions Specifying revisions and ranges for Git tutorial A tutorial introduction to Git tutorial-2 A tutorial introduction to Git: part two workflows An overview of recommended workflows with Git 'git help -a' and 'git help -g' list available subcommands and some concept guides. See 'git help ' or 'git help ' to read about a specific subcommand or concept. 回到远程仓库的状态 抛弃本地所有的修改，回到远程仓库的状态。 git fetch --all && git reset --hard origin/master 重设第一个 commit 也就是把所有的改动都重新放回工作区，并清空所有的 commit，这样就可以重新提交第一个 commit 了 git update-ref -d HEAD 展示工作区和暂存区的不同 输出工作区和暂存区的 different (不同)。 git diff 还可以展示本地仓库中任意两个 commit 之间的文件变动： git diff 展示暂存区和最近版本的不同 输出暂存区和本地最近的版本 (commit) 的 different (不同)。 git diff --cached 展示暂存区、工作区和最近版本的不同 输出工作区、暂存区 和本地最近的版本 (commit) 的 different (不同)。 git diff HEAD 快速切换分支上一个分支 git checkout - 删除已经合并到 master 的分支 git branch --merged master | grep -v '^\\*\\| master' | xargs -n 1 git branch -d 展示本地分支关联远程仓库的情况 git branch -vv 关联远程分支 关联之后，git branch -vv 就可以展示关联的远程分支名了，同时推送到远程仓库直接：git push，不需要指定远程仓库了。 git branch -u origin/mybranch 或者在 push 时加上 -u 参数 git push origin/mybranch -u 列出所有远程分支 -r 参数相当于：remote git branch -r 列出本地和远程分支 -a 参数相当于：all git branch -a 创建并切换到本地分支 git checkout -b 从远程分支中创建并切换到本地分支 git checkout -b origin/ 删除本地分支 git branch -d 删除远程分支 git push origin --delete 或者 git push origin : 重命名本地分支 git branch -m 查看标签 git tag 展示当前分支的最近的 tag git describe --tags --abbrev=0 查看标签详细信息 git tag -ln 本地创建标签 git tag 默认 tag 是打在最近的一次 commit 上，如果需要指定 commit 打 tag： $ git tag -a -m \"v1.0 发布(描述)\" 推送标签到远程仓库 首先要保证本地创建好了标签才可以推送标签到远程仓库： git push origin 一次性推送所有标签，同步到远程仓库： git push origin --tags 删除本地标签 git tag -d 删除远程标签 删除远程标签需要先删除本地标签，再执行下面的命令： git push origin :refs/tags/ 切回到某个标签 一般上线之前都会打 tag，就是为了防止上线后出现问题，方便快速回退到上一版本。下面的命令是回到某一标签下的状态： git checkout -b branch_name tag_name 放弃工作区的修改 git checkout 放弃所有修改： git checkout . 恢复删除的文件 git rev-list -n 1 HEAD -- #得到 deleting_commit git checkout ^ -- #回到删除文件 deleting_commit 之前的状态 以新增一个 commit 的方式还原某一个 commit 的修改 git revert 回到某个 commit 的状态，并删除后面的 commit 和 revert 的区别：reset 命令会抹去某个 commit id 之后的所有 commit git reset #默认就是-mixed参数。 git reset –mixed HEAD^ #回退至上个版本，它将重置HEAD到另外一个commit,并且重置暂存区以便和HEAD相匹配，但是也到此为止。工作区不会被更改。 git reset –soft HEAD~3 #回退至三个版本之前，只回退了commit的信息，暂存区和工作区与回退之前保持一致。如果还要提交，直接commit即可 git reset –hard #彻底回退到指定commit-id的状态，暂存区和工作区也会变为指定commit-id版本的内容 修改上一个 commit 的描述 git commit --amend 查看 commit 历史 git log 查看某段代码是谁写的 blame 的意思为‘责怪’，你懂的。 git blame 显示本地更新过 HEAD 的 git 命令记录 每次更新了 HEAD 的 git 命令比如 commint、amend、cherry-pick、reset、revert 等都会被记录下来（不限分支），就像 shell 的 history 一样。 这样你可以 reset 到任何一次更新了 HEAD 的操作之后，而不仅仅是回到当前分支下的某个 commit 之后的状态。 git reflog 修改作者名 git commit --amend --author='Author Name ' 修改远程仓库的 url git remote set-url origin 增加远程仓库 git remote add origin 列出所有远程仓库 git remote 查看两个星期内的改动 git whatchanged --since='2 weeks ago' 把 A 分支的某一个 commit，放到 B 分支上 这个过程需要 cherry-pick 命令，参考 git checkout && git cherry-pick 给 git 命令起别名 简化命令 git config --global alias. 比如：git status 改成 git st，这样可以简化命令 git config --global alias.st status 存储当前的修改，但不用提交 commit 详解可以参考廖雪峰老师的 git 教程 git stash 保存当前状态，包括 untracked 的文件 untracked 文件：新建的文件 git stash -u 展示所有 stashes git stash list 回到某个 stash 的状态 git stash apply 回到最后一个 stash 的状态，并删除这个 stash git stash pop 删除所有的 stash git stash clear 从 stash 中拿出某个文件的修改 git checkout -- 展示所有 tracked 的文件 git ls-files -t 展示所有 untracked 的文件 git ls-files --others 展示所有忽略的文件 git ls-files --others -i --exclude-standard 强制删除 untracked 的文件 可以用来删除新建的文件。如果不指定文件文件名，则清空所有工作的 untracked 文件。clean 命令，注意两点： clean 后，删除的文件无法找回 不会影响 tracked 的文件的改动，只会删除 untracked 的文件 git clean -f 强制删除 untracked 的目录 可以用来删除新建的目录，注意:这个命令也可以用来删除 untracked 的文件。详情见上一条 git clean -df 展示简化的 commit 历史 git log --pretty=oneline --graph --decorate --all 把某一个分支到导出成一个文件 git bundle create 从包中导入分支 新建一个分支，分支内容就是上面 git bundle create 命令导出的内容 git clone repo.bundle -b 执行 rebase 之前自动 stash git rebase --autostash 从远程仓库根据 ID，拉下某一状态，到本地分支 git fetch origin pull//head: 详细展示一行中的修改 git diff --word-diff 清除 gitignore 文件中记录的文件 git clean -X -f 展示所有 alias 和 configs 注意： config 分为：当前目录（local）和全局（golbal）的 config，默认为当前目录的 config git config --local --list (当前目录) git config --global --list (全局) 展示忽略的文件 git status --ignored commit 历史中显示 Branch1 有的，但是 Branch2 没有 commit git log Branch1 ^Branch2 在 commit log 中显示 GPG 签名 git log --show-signature 删除全局设置 git config --global --unset 新建并切换到新分支上，同时这个分支没有任何 commit 相当于保存修改，但是重写 commit 历史 git checkout --orphan 展示任意分支某一文件的内容 git show : clone 下来指定的单一分支 git clone -b --single-branch https://github.com/user/repo.git 忽略某个文件的改动 关闭 track 指定文件的改动，也就是 Git 将不会在记录这个文件的改动 git update-index --assume-unchanged path/to/file 恢复 track 指定文件的改动 git update-index --no-assume-unchanged path/to/file 忽略文件的权限变化 不再将文件的权限变化视作改动 git config core.fileMode false 以最后提交的顺序列出所有 Git 分支 最新的放在最上面 git for-each-ref --sort=-committerdate --format='%(refname:short)' refs/heads/ 在 commit log 中查找相关内容 通过 grep 查找，given-text：所需要查找的字段 git log --all --grep='' 把暂存区的指定 file 放到工作区中 不添加参数，默认是 -mixed git reset 强制推送 git push -f 一图详解 优雅的提交Commit信息 使用Angular团队提交规范 主要有以下组成 标题行: 必填, 描述主要修改类型和内容 主题内容: 描述为什么修改, 做了什么样的修改, 以及开发的思路等等 页脚注释: 放 Breaking Changes 或 Closed Issues 常用的修改项 type: commit 的类型 feat: 新特性 fix: 修改问题 refactor: 代码重构 docs: 文档修改 style: 代码格式修改, 注意不是 css 修改 test: 测试用例修改 chore: 其他修改, 比如构建流程, 依赖管理. scope: commit 影响的范围, 比如: route, component, utils, build... subject: commit 的概述 body: commit 具体修改内容, 可以分为多行 footer: 一些备注, 通常是 BREAKING CHANGE 或修复的 bug 的链接. 使用Commitizen代替 git commit 可以使用cz-cli工具代替 git commit 全局安装 npm install -g commitizen cz-conventional-changelog echo '{ \"path\": \"cz-conventional-changelog\" }' > ~/.czrc 全局安装后使用 git cz 代替 git commit就可以了,如下图 联系我 博客园：削微寒 或者直接提 Pr，Issues ⬆ 返回顶部 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:29:35 "},"awesome-java-cn/":{"url":"awesome-java-cn/","title":"资源总览","keywords":"","body":"Java资源大全中文版 我想很多程序员应该记得 GitHub 上有一个 Awesome - XXX 系列的资源整理。awesome-java 就是 akullpp 发起维护的 Java 资源列表，内容包括：构建工具、数据库、框架、模板、安全、代码分析、日志、第三方库、书籍、Java 站点等等。伯乐在线已经把 awesome-java 资源列表翻成中文后发布于 ImportNew。 Awesome 系列虽然挺全，但基本只对收录的资源做了极为简要的介绍，如果有更详细的中文介绍，对相应开发者的帮助会更大。这也是我们发起这个开源项目的初衷。 我们要做什么？ 基于 awesome-java 资源列表，我们将对各个资源项进行编译整理。 整理后的内容，将收录在伯乐在线资源频道。可参考已整理的内容： 《OWNER：Java配置文件解决方案》 《Spring Boot：简化Spring应用初始搭建以及开发过程》 《SonarQube：开源的代码质量管理工具》 如何参与本项目？ 如何为列表贡献新资源？ 欢迎大家为列表贡献高质量的新资源，提交PR时请参照以下要求： 请确保推荐的资源自己使用过 提交PR时请注明推荐理由 资源列表管理收到PR请求后，会定期（每周）在微博转发本周提交的PR列表，并在微博上面听取使用过这些资源的意见。确认通过后，会加入资源大全。 感谢您的贡献！ 本项目的参与者 维护者：tangyouhua 贡献者：tangyouhua、kingzone、llhua2329、BadCoderChou、anankun、贺贺、大彭、superXiaoFan、javayrf、John Smith、Jaler、JM、dreamkidd、cheenlie、zhangQian1991、王涛、马、vvkee、凝枫、haixunlu、milly、Hodur、FakeHank、连乐、UncleTim、sunbiaobiao、zhiguo、光光头去打酱油、云中游、Zemo、sdcuike、danielwii、oneDay、邢敏、heikehuan、fgcui1204、wenxueliu、Gentle Yang、黄小非、wangtg、百焱、胡不GUI、Another_mt、Rainbow、super^糖、黄余粮、Sun、李伟高、lixiaobao、You 注：名单不分排名，不定期补充更新 目录 Java资源大全中文版 古董级工具 构建工具 字节码操作 集群管理 代码分析 编译器生成工具 外部配置工具 约束满足问题求解程序 持续集成 CSV解析 数据结构 数据库 时间日期工具库 依赖注入 开发流程增强工具 分布式应用 分布式数据库 发布 文档处理工具 函数式编程 游戏开发 GUI 高性能计算 IDE 图像处理 JSON JVM与JDK 基于JVM的语言 日志 机器学习 消息传递 杂项 应用监控工具 原生开发库 自然语言处理 网络 ORM PDF 性能分析 响应式开发库 REST框架 科学计算与分析 搜索引擎 安全 序列化 应用服务器 模板引擎 测试 通用工具库 网络爬虫 Web框架 业务流程管理套件 资源 社区 有影响力的书 播客 微博、微信公众号 Twitter 知名网站 古董级工具 这些工具伴随着Java一起出现，在各自辉煌之后还在一直使用。 Apache Ant：基于XML的构建管理工具。官网 cglib：字节码生成库。官网 GlassFish：应用服务器，由Oracle赞助支持的Java EE参考实现。官网 Hudson：持续集成服务器，目前仍在活跃开发。官网 JavaServer Faces：Mojarra是JSF标准的一个开源实现，由Oracle开发。官网 JavaServer Pages：支持自定义标签库的网站通用模板库。官网 Liquibase：与具体数据库独立的追踪、管理和应用数据库Scheme变化的工具。官网 构建工具 构建及应用依赖关系处理工具。 Apache Maven：Maven是一款声明式构建及依赖管理工具，采用约定优于配置方式进行管理。相对Apache Ant更推荐使用Maven，前者采用了过程式管理，维护相对困难。官网 Bazel：来自Google的构建工具，可以快速、可靠地构建代码。官网 Gradle：使用Groovy（非XML）进行增量构建，可以很好地与Maven依赖管理配合工作。官网 Buck：Facebook构建工具。官网 字节码操作 编程方式操作字节码的开发库。 ASM：通用底层字节码操作和分析开发库。官网 Byte Buddy：使用流式API进一步简化字节码生成。官网 Byteman：在运行时通过DSL（规则）操作字节码进行测试和故障排除。官网 Javassist：一个简化字节码编辑尝试。官网 集群管理 在集群内动态管理应用程序的框架。 Apache Aurora：Apache Aurora是一个Mesos框架，用于长时间运行服务和定时任务（cron job）。官网 Singularity：Singularity是一个Mesos框架，方便部署和操作。它支持Web Service、后台运行、调度作业和一次性任务。官网 代码分析 测量代码指标和质量工具。 Checkstyle：代码编写规范和标准静态分析工具。官网 Error Prone：将常见编程错误作为运行时错误报告。官网 FindBugs：通过字节码静态分析查找隐藏bug。官网 jQAssistant：使用基于Neo4J查询语言进行代码静态分析。官网 PMD：对源代码分析查找不良的编程习惯。官网 SonarQube：通过插件集成其它分析组件，对过去一段时间内的数据进行统计。官网 编译器生成工具 用来创建解析器、解释器或编译器的框架。 ANTLR：复杂的全功能自顶向下解析框架。官网 JavaCC：JavaCC是更加专门的轻量级工具，易于上手且支持语法超前预测。官网 外部配置工具 支持外部配置的开发库。 config：针对JVM语言的配置库。官网 owner：减少冗余配置属性。官网 约束满足问题求解程序 帮助解决约束满足问题的开发库。 Choco：可直接使用的约束满足问题求解程序，使用了约束规划技术。官网 JaCoP：为FlatZinc语言提供了一个接口，可以执行MiniZinc模型。官网 OptaPlanner：业务规划与资源调度优化求解程序。官网 Sat4J：逻辑代数与优化问题最先进的求解程序。官网 持续集成 Bamboo：Atlassian解决方案，可以很好地集成Atlassian的其他产品。可以选择开源许可，也可以购买商业版。官网 CircleCI：提供托管服务，可以免费试用。官网 Codeship：提供托管服务，提供有限的免费模式。官网 fabric8：容器集成平台。官网 Go：ThoughtWork开源解决方案。官网 Jenkins：支持基于服务器的部署服务。官网 TeamCity：JetBrain的持续集成解决方案，有免费版。官网 Travis：通常用作开源项目的托管服务。官网 Buildkite: 持续集成工具，用简单的脚本就能设置pipeline，而且能快速构建，可以免费试用。官网 CSV解析 简化CSV数据读写的框架与开发库 uniVocity-parsers：速度最快功能最全的CSV开发库之一，同时支持TSV与固定宽度记录的读写。官网 数据库 简化数据库交互的相关工具。 Apache Phoenix：HBase针对低延时应用程序的高性能关系数据库层。官网 Crate：实现了数据同步、分片、缩放、复制的分布式数据存储。除此之外还可以使用基于SQL的语法跨集群查询。官网 Flyway：简单的数据库迁移工具。官网 H2：小型SQL数据库，以可以作为内存数据库使用著称。官网 HikariCP：高性能JDBC连接工具。官网 JDBI：便捷的JDBC抽象。官网 jOOQ：为SQL schema生成typesafe代码。官网 MapDB：以磁盘或堆内存中并发集合为基础的嵌入式数据库引擎。官网 Presto：针对大数据的分布式SQL查询引擎。官网 Querydsl：Typesafe统一查询。官网 数据结构 Apache Parquet：Google Dremel论文中发布的基于组装算法的列式（Columnar）存储格式。官网 Protobuf：Google数据交换格式。官网 SBE：简单二进制编码，是最快速的消息格式之一。官网 Wire：整洁轻量级协议缓存。官网 时间日期工具库 处理时间和日期的开发库。 Joda-Time：在Java 8发布前，Joda-Time是实际使用的时间日期库标准。官网 Time4J：高级时间和日期库。官网 ThreeTen：JSR-310实现，为JDK提供更具特点的时间和日期API。官网 依赖注入 帮实现依赖翻转范式的开发库。 官网 Apache DeltaSpike：CDI扩展框架。官网 Dagger2：编译时注入框架，不需要使用反射。官网 Guice：可以匹敌Dagger的轻量级注入框架。官网 HK2：轻量级动态依赖注入框架。官网 开发流程增强工具 从最基本的层面增强开发流程。 ADT4J：针对代数数据类型的JSR-269代码生成器。官网 AspectJ：面向切面编程（AOP）的无缝扩展。官网 Auto：源代码生成器集合。官网 DCEVM：通过修改JVM在运行时支持对已加载的类进行无限次重定义。官网 HotswapAgent：支持无限次重定义运行时类与资源。官网 Immutables：类似Scala的条件类。官网 JHipster：基于Spring Boot与AngularJS应用程序的Yeoman源代码生成器。官网 JRebel：无需重新部署，可以即时重新加载代码与配置的商业软件。官网 Lombok：减少冗余的代码生成器。官网 Spring Loaded：类重载代理。官网 vert.x：多语言事件驱动应用框架。官网 分布式应用 用来编写分布式容错应用的开发库和框架。 Akka：用来编写分布式容错并发事件驱动应用程序的工具和运行时。官网 Apache Storm：实时计算系统。官网 Apache ZooKeeper：针对大型分布式系统的协调服务，支持分布式配置、同步和名称注册。官网 Hazelcast：高可扩展内存数据网格。官网 Hystrix：提供延迟和容错。官网 JGroups：提供可靠的消息传递和集群创建的工具。官网 Orbit：支持虚拟角色（Actor），在传统角色的基础上增加了另外一层抽象。官网 Quasar：为JVM提供轻量级线程和角色。官网 分布式数据库 对应用程序而言，在分布式系统中的数据库看起来就像是只有一个数据源。 Apache Cassandra：列式数据库，可用性高且没有单点故障。官网 Apache HBase：针对大数据的Hadoop数据库。官网 Druid：实时和历史OLAP数据存储，在聚集查询和近似查询方面表现不俗。官网 Infinispan：针对缓存的高并发键值对数据存储。官网 TiDB：开源分布式HTAP数据库，结合了传统的RDBMS和NoSQL的最佳特性。官网 发布 以本机格式发布应用程序的工具。 Bintray：发布二进制文件版本控制工具。可以于Maven或Gradle一起配合使用。提供开源免费版本和几种商业收费版本。官网 Central Repository：最大的二进制组件仓库，面向开源社区提供免费服务。Apache Maven默认使用Central 官网Repository，也可以在所有其他构建工具中使用。 IzPack：为跨平台部署建立创作工具（Authoring Tool）。官网 JitPack：打包GitHub仓库的便捷工具。可根据需要构建Maven、Gradle项目，发布可立即使用的组件。官网 Launch4j：将JAR包装为轻量级本机Windows可执行程序。官网 Nexus：支持代理和缓存功能的二进制管理工具。官网 packr：将JAR、资源和JVM打包成Windows、Linux和Mac OS X本地发布文件。官网 文档处理工具 处理Office文档的开发库。 Apache POI：支持OOXML规范（XLSX、DOCX、PPTX）以及OLE2规范（XLS、DOC、PPT）。官网 documents4j：使用第三方转换器进行文档格式转换，转成类似MS Word这样的格式。官网 jOpenDocument：处理OpenDocument格式（由Sun公司提出基于XML的文档格式）。官网 函数式编程 函数式编程支持库。 Cyclops：支持一元（Monad）操作和流操作工具类、comprehension（List语法）、模式匹配、trampoline等特性。官网 Fugue：Guava的函数式编程扩展。官网 Functional Java：实现了多种基础和高级编程抽象，用来辅助面向组合开发（composition-oriented development）。官网 Javaslang：一个函数式组件库，提供持久化数据类型和函数式控制结构。官网 jOOλ：旨在填补Java 8 lambda差距的扩展，提供了众多缺失的类型和一组丰富的顺序流API。官网 游戏开发 游戏开发框架。 jMonkeyEngine：现代3D游戏开发引擎。官网 libGDX：全面的跨平台高级框架。官网 LWJGL：对OpenGL/CL/AL等技术进行抽象的健壮框架。官网 GUI 现代图形化用户界面开发库。 JavaFX：Swing的后继者。官网 Scene Builder：开发JavaFX应用的可视化布局工具。官网 高性能计算 涵盖了从集合到特定开发库的高性能计算相关工具。 Agrona：高性能应用中常见的数据结构和工具方法。官网 Disruptor：线程间消息传递开发库。官网 fastutil：快速紧凑的特定类型集合（Collection）。官网 GS Collections：受Smalltalk启发的集合框架。官网 HPPC：基础类型集合。官网 Javolution：实时和嵌入式系统的开发库。官网 JCTools：JDK中缺失的并发工具。官网 Koloboke：Hash set和hash map。官网 Trove：基础类型集合。官网 High-scale-lib:Cliff Click 个人开发的高性能并发库官网 IDE 简化开发的集成开发环境。 Eclipse：老牌开源项目，支持多种插件和编程语言。官网 IntelliJ IDEA：支持众多JVM语言，是安卓开发者好的选择。商业版主要针对企业客户。官网 NetBeans：为多种技术提供集成化支持，包括Java SE、Java EE、数据库访问、HTML5等。官网 Scala IDE：一款基于Eclipse开源平台打造的Scala集成开发环境。官网 SpringSource Tool Suite（STS）:一款基于Eclipse开源平台打造的Spring应用开发环境。官网 图像处理 创建、评价和操作图片的支持库。 Imgscalr：纯Java 2D实现，简单、高效、支持硬件加速的图像缩放开发库。官网 Picasso：安卓图片下载和图片缓存开发库。官网 Thumbnailator：Thumbnailator是一个高质量Java缩略图开发库。官网 ZXing：支持多种格式的一维、二维条形码图片处理开发库。官网 im4java: 基于ImageMagick或GraphicsMagick命令行的图片处理开发库，基本上ImageMagick能够支持的图片格式和处理方式都能够处理。官网 Apache Batik：在Java应用中程序以SVG格式显示、生成及处理图像的工具集，包括SVG解析器、SVG生成器、SVG DOM等模块，可以集成使用也可以单独使用，还可以扩展自定义的SVG标签。官网 JSON 简化JSON处理的开发库。 Genson：强大且易于使用的Java到JSON转换开发库。官网 Gson：谷歌官方推出的JSON处理库，支持在对象与JSON之间双向序列化，性能良好且可以实时调用。官网 Jackson：与GSON类似，在频繁使用时性能更佳。官网 LoganSquare：基于Jackson流式API，提供对JSON解析和序列化。比GSON与Jackson组合方式效果更好。官网 Fastjson：一个Java语言编写的高性能功能完善的JSON库。官网 Kyro：快速、高效、自动化的Java对象序列化和克隆库。官网 JVM与JDK 目前的JVM和JDK实现。 JDK 9：JDK 9的早期访问版本。官网 OpenJDK：JDK开源实现。官网 基于JVM的语言 除Java外，可以用来编写JVM应用程序的编程语言。 Scala：融合了面向对象和函数式编程思想的静态类型编程语言。官网 Groovy：类型可选（Optionally typed）的动态语言，支持静态类型和静态编译。目前是一个Apache孵化器项目。官网 Clojure：可看做现代版Lisp的动态类型语言。官网 Ceylon：RedHat开发的面向对象静态类型编程语言。官网 Kotlin：JetBrain针对JVM、安卓和浏览器提供的静态类型编程语言。官网 Xtend：一种静态编程语言，能够将其代码转换为简洁高效的Java代码，并基于JVM运行。官网 日志 记录应用程序行为日志的开发库。 Apache Log4j 2：使用强大的插件和配置架构进行完全重写。官网 kibana：分析及可视化日志文件。官网 Logback：强健的日期开发库，通过Groovy提供很多有趣的选项。官网 logstash：日志文件管理工具。官网 Metrics：通过JMX或HTTP发布参数，并且支持存储到数据库。官网 SLF4J：日志抽象层，需要与具体的实现配合使用。官网 机器学习 提供具体统计算法的工具。其算法可从数据中学习。 Apache Flink：快速、可靠的大规模数据处理引擎。官网 Apache Hadoop：在商用硬件集群上用来进行大规模数据存储的开源软件框架。官网 Apache Mahout：专注协同过滤、聚类和分类的可扩展算法。官网 Apache Spark：开源数据分析集群计算框架。官网 DeepDive：从非结构化数据建立结构化信息并集成到已有数据库的工具。官网 Deeplearning4j：分布式多线程深度学习开发库。官网 H2O：用作大数据统计的分析引擎。官网 Weka：用作数据挖掘的算法集合，包括从预处理到可视化的各个层次。官网 QuickML：高效机器学习库。官网、GitHub 消息传递 在客户端之间进行消息传递，确保协议独立性的工具。 Aeron：高效可扩展的单播、多播消息传递工具。官网 Apache ActiveMQ：实现JMS的开源消息代理（broker），可将同步通讯转为异步通讯。官网 Apache Camel：通过企业级整合模式（Enterprise Integration Pattern EIP）将不同的消息传输API整合在一起。官网 Apache Kafka：高吞吐量分布式消息系统。官网 Hermes：快速、可靠的消息代理（Broker），基于Kafka构建。官网 JBoss HornetQ：清晰、准确、模块化，可以方便嵌入的消息工具。官网 JeroMQ：ZeroMQ的纯Java实现。官网 Smack：跨平台XMPP客户端函数库。官网 Openfire：是开源的、基于XMPP、采用Java编程语言开发的实时协作服务器。 Openfire安装和使用都非常简单，并可利用Web界面进行管理。 官网 GitHub Spark：是一个开源，跨平台IM客户端。它的特性支持集组聊天，电话集成和强大安全性能。如果企业内部部署IM使用Openfire+Spark是最佳的组合。 官网 GitHub Tigase： 是一个轻量级的可伸缩的 Jabber/XMPP 服务器。无需其他第三方库支持，可以处理非常高的复杂和大量的用户数，可以根据需要进行水平扩展。 官网 杂项 未分类其它资源。 Design Patterns：实现并解释了最常见的设计模式。官网 Jimfs：内存文件系统。官网 Lanterna：类似curses的简单console文本GUI函数库。官网 LightAdmin：可插入式CRUD UI函数库，可用来快速应用开发。官网 OpenRefine：用来处理混乱数据的工具，包括清理、转换、使用Web Service进行扩展并将其关联到数据库。官网 RoboVM：Java编写原生iOS应用。官网 Quartz：强大的任务调度库.官网 应用监控工具 监控生产环境中应用程序的工具。 AppDynamics：性能监测商业工具。官网 JavaMelody：性能监测和分析工具。官网 Kamon：Kamon用来监测在JVM上运行的应用程序。官网 New Relic：性能监测商业工具。官网 SPM：支持对JVM应用程序进行分布式事务追踪的性能监测商业工具。官网 OverOps(Takipi)：产品运行时错误监测及调试商业工具。官网 原生开发库 用来进行特定平台开发的原生开发库。 JNA：不使用JNI就可以使用原生开发库。此外，还为常见系统函数提供了接口。官网 自然语言处理 用来专门处理文本的函数库。 Apache OpenNLP：处理类似分词等常见任务的工具。官网 CoreNLP：斯坦佛CoreNLP提供了一组基础工具，可以处理类似标签、实体名识别和情感分析这样的任务。官网 LingPipe：一组可以处理各种任务的工具集，支持POS标签、情感分析等。官网 Mallet：统计学自然语言处理、文档分类、聚类、主题建模等。官网 网络 网络编程函数库。 Async Http Client：异步HTTP和WebSocket客户端函数库。官网 Grizzly：NIO框架，在Glassfish中作为网络层使用。官网 Netty：构建高性能网络应用程序开发框架。官网 OkHttp：一个Android和Java应用的HTTP+SPDY客户端。官网 Undertow：基于NIO实现了阻塞和非阻塞API的Web服务器，在WildFly中作为网络层使用。官网 unirest-java: Unirest 是一个轻量级的 HTTP 请求库，涵盖 Node、Ruby、Java、PHP、Python、Objective-C、.NET 等多种语言。可发起 GET, POST, PUT, PATCH, DELETE, HEAD, OPTIONS 请求。官网 brpc-java: java版baidu rpc框架，高性能、多协议、易扩展、低耦合。官网 ORM 处理对象持久化的API。 Ebean：支持快速数据访问和编码的ORM框架。官网 EclipseLink：支持许多持久化标准，JPA、JAXB、JCA和SDO。官网 Hibernate：广泛使用、强健的持久化框架。Hibernate的技术社区非常活跃。官网 MyBatis：带有存储过程或者SQL语句的耦合对象（Couples object）。官网 OrmLite：轻量级开发包，免除了其它ORM产品中的复杂性和开销。官网 Nutz：另一个SSH。官网，Github，论坛 JFinal：JAVA WEB + ORM框架。官网，Github Apache OpenJPA: 实现了 EJB 3.0 中的 JPA 标准,为开发者提供功能强大、使用简单的持久化数据管理框架。 官网 PDF 用来帮助创建PDF文件的资源。 Apache FOP：从XSL-FO创建PDF。官网 Apache PDFBox：用来创建和操作PDF的工具集。官网 DynamicReports：JasperReports的精简版。官网 flyingsaucer：XML/XHTML和CSS 2.1渲染器。官网 iText：一个易于使用的PDF函数库，用来编程创建PDF文件。注意，用于商业用途时需要许可证。官网 JasperReports：一个复杂的报表引擎。官网 性能分析 性能分析、性能剖析及基准测试工具。 jHiccup：提供平台中JVM暂停的日志和记录。官网 JMH：JVM基准测试工具。官网 JProfiler：商业分析器。官网 LatencyUtils：测量和报告延迟的工具。官网 VisualVM：对运行中的应用程序信息提供了可视化界面。官网 YourKit Java Profiler：商业分析器。官网 响应式开发库 用来开发响应式应用程序的开发库。 Reactive Streams：异步流处理标准，支持非阻塞式反向压力（backpressure）。官网 Reactor：构建响应式快速数据（fast-data）应用程序的开发库。官网 RxJava：通过JVM可观察序列（observable sequence）构建异步和基于事件的程序。官网 REST框架 用来创建RESTful 服务的框架。 Dropwizard：偏向于自己使用的Web框架。用来构建Web应用程序，使用了Jetty、Jackson、Jersey和Metrics。官网 Feign：受Retrofit、JAXRS-2.0和WebSocket启发的HTTP客户端连接器（binder）。官网 Jersey：JAX-RS参考实现。官网 RESTEasy：经过JAX-RS规范完全认证的可移植实现。官网 RestExpress：一个Java类型安全的REST客户端。官网 RestX：基于注解处理和编译时源码生成的框架。官网 Retrofit：类型安全的REST客户端。官网 Spark：受到Sinatra启发的Java REST框架。官网 Swagger：Swagger是一个规范且完整的框架，提供描述、生产、消费和可视化RESTful Web Service。官网 Blade：国人开发的一个轻量级的MVC框架. 它拥有简洁的代码，优雅的设计。官网 科学计算与分析 用于科学计算和分析的函数库。 DataMelt：用于科学计算、数据分析及数据可视化的开发环境。官网 JGraphT：支持数学图论对象和算法的图形库。官网 JScience：用来进行科学测量和单位的一组类。官网 搜索引擎 文档索引引擎，用于搜索和分析。 Apache Solr：一个完全的企业搜索引擎。为高吞吐量通信进行了优化。官网 Elasticsearch：一个分布式、支持多租户（multitenant）全文本搜索引擎。提供了RESTful Web接口和无schema的JSON文档。官网 Apache Lucene：是一个开放源代码的全文检索引擎工具包，是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎。官网 安全 用于处理安全、认证、授权或会话管理的函数库。 Apache Shiro：执行认证、授权、加密和会话管理。官网 Bouncy Castle，涵盖了从基础的帮助函数到PGP/SMIME操作。官网：多途加密开发库。支持JCA提供者（JCA provider) Cryptomator：在云上进行客户端跨平台透明加密。官网 Keycloak：为浏览器应用和RESTful Web Service集成SSO和IDM。目前还处于beta版本，但是看起来非常有前途。官网 PicketLink：PicketLink是一个针对Java应用进行安全和身份认证管理的大型项目（Umbrella Project）。官网 序列化 用来高效处理序列化的函数库。 FlatBuffers：高效利用内存的序列化函数库，无需解包和解析即可高效访问序列化数据。官网 Kryo：快速、高效的对象图形序列化框架。官网 FST：提供兼容JDK的高性能对象图形序列化。官网 MessagePack：一种高效的二进制序列化格式。官网 应用服务器 用来部署应用程序的服务器。 Apache Tomcat：针对Servlet和JSP的应用服务器，健壮性好且适用性强。官网 Apache TomEE：Tomcat加Java EE。官网 Jetty：轻量级、小巧的应用服务器，通常会嵌入到项目中。官网 WebSphere Liberty：轻量级、模块化应用服务器，由IBM开发。官网 WildFly：之前被称作JBoss，由Red Hat开发。支持很多Java EE功能。官网 模板引擎 在模板中替换表达式的工具。 Apache Velocity：提供HTML页面模板、email模板和通用开源代码生成器模板。官网 FreeMarker：通用模板引擎，不需要任何重量级或自己使用的依赖关系。官网 Handlebars.java：使用Java编写的模板引擎，逻辑简单，支持语义扩展（semantic Mustache）。官网 Thymeleaf：旨在替换JSP，支持XML文件的工具。官网 Beetl：新一代的模板引擎，功能强大，性能良好，超过当前流行的模板引擎。而且还易学易用。官网 测试 测试内容从对象到接口，涵盖性能测试和基准测试工具。 Apache JMeter：功能性测试和性能评测。官网 Arquillian：集成测试和功能行测试平台，集成Java EE容器。官网 AssertJ：支持流式断言提高测试的可读性。官网 Awaitility：用来同步异步操作的DSL。官网 Cucumber：BDD测试框架。官网 Gatling：设计为易于使用、可维护的和高性能负载测试工具。官网 Hamcrest：可用来灵活创建意图（intent）表达式的匹配器。官网 JMockit：用来模拟静态、final方法等。官网 JUnit：通用测试框架。官网 Mockito：在自动化单元测试中创建测试对象，为TDD或BDD提供支持。官网 PowerMock： 支持模拟静态方法、构造函数、final类和方法、私有方法以及移除静态初始化器的模拟工具。官网 REST Assured：为REST/HTTP服务提供方便测试的Java DSL。官网 Selenide：为Selenium提供精准的周边API，用来编写稳定且可读的UI测试。官网 Selenium：为Web应用程序提供可移植软件测试框架。官网 Spock：JUnit-compatible framework featuring an expressive Groovy-derived specification language.官网兼容JUnit框架，支持衍生的Groovy范的语言。 TestNG：测试框架。官网 Truth：Google的断言和命题（proposition）框架。官网 Unitils：模块化测试函数库，支持单元测试和集成测试。官网 WireMock：Web Service测试桩（Stub）和模拟函数。官网 通用工具库 通用工具类函数库。 Apache Commons：提供各种用途的函数，比如配置、验证、集合、文件上传或XML处理等。官网 args4j：命令行参数解析器。官网 CRaSH：为运行进行提供CLI。官网 Gephi：可视化跨平台网络图形化操作程序。官网 Guava：集合、缓存、支持基本类型、并发函数库、通用注解、字符串处理、I/O等。官网 JADE：构建、调试多租户系统的框架和环境。官网 javatuples：正如名字表示的那样，提供tuple支持。尽管目前tuple的概念还有留有争议。官网 JCommander：命令行参数解析器。官网 Protégé：提供存在论（ontology）编辑器以及构建知识系统的框架。官网 网络爬虫 用于分析网站内容的函数库。 Apache Nutch：可用于生产环境的高度可扩展、可伸缩的网络爬虫。官网 Crawler4j：简单的轻量级网络爬虫。官网 JSoup：刮取、解析、操作和清理HTML。官网 webmagic：一个可扩展的Java爬虫框架，架构类似Python的Scrapy。 Web框架 用于处理Web应用程序不同层次间通讯的框架。 Apache Tapestry：基于组件的框架，使用Java创建动态、强健的、高度可扩展的Web应用程序。官网 Apache Wicket：基于组件的Web应用框架，与Tapestry类似带有状态显示GUI。官网 Google Web Toolkit：一组Web开发工具集，包含在客户端将Java代码转为JavaScript的编译器、XML解析器、RCP 官网API、JUnit集成、国际化支持和GUI控件。 Grails：Groovy框架，旨在提供一个高效开发环境，使用约定而非配置、没有XML并支持混入（mixin）。官网 Ninja：Java全栈Web开发框架。非常稳固、快速和高效。官网 Pippo：小型、高度模块化的类Sinatra框架。官网 Play：使用约定而非配置，支持代码热加载并在浏览器中显示错误。官网 PrimeFaces：JSF框架，提供免费和带支持的商业版本。包括若干前端组件。官网 Ratpack：一组Java开发函数库，用于构建快速、高效、可扩展且测试完备的HTTP应用程序。官网 Spring Boot：微框架，简化了Spring新程序的开发过程。官网 Spring：旨在简化Java EE的开发过程，提供依赖注入相关组件并支持面向切面编程。官网 Vaadin：基于GWT构建的事件驱动框架。使用服务端架构，客户端使用Ajax。官网 Blade：国人开发的一个轻量级的MVC框架. 它拥有简洁的代码，优雅的设计。官网 业务流程管理套件 流程驱动的软件系统构建。 jBPM：非常灵活的业务流程管理框架，致力于构建开发与业务分析人员之间的桥梁。官网 Activity：轻量级工作流和业务流程管理框架。官网 github 资源 社区 r/java：Reddit的Java子社区。官网 stackoverflow：问答平台。官网 vJUG：虚拟Java用户组。官网 java8 新特性教程例子。github 有影响力的书 具有广泛影响且值得阅读的Java经典书籍。 Effective Java (2nd Edition) Java 8 in Action Java Concurrency in Practice | Java并发编程实战 Thinking in Java | Java编程思想 Java Puzzlers | Java解惑 播客 可以一边编程一边听的东西。 Java Council：官网 Java Posse：Discontinued as of 02/2015.官网 微博、微信公众号 ImportNew：是最受欢迎的、专注Java技术分享的微信公众号。专注Java技术分享，包括Java基础技术、进阶技能、架构设计和Java技术领域动态等。 ImportNew 微博：@ImportNew Twitter Adam Bien：自由职业者、作家、JavaONE明星演讲者、顾问、Java Champion。 Antonio Goncalves：Java Champion、JUG Leader、Devoxx France、Java EE 6/7、JCP、作家。 Arun Gupta：Java Champion、JavaONE明星演讲者、JUG Leader、Devoxx4Kids成员、Red Hatter。 Bruno Borges：Oracle产品经理、Java Jock。 Ed Burns：Oracle技术团队顾问。 Eugen Paraschiv：Spring安全课程作者。 James Weaver：Java、JavaFX、IoT开发者、作者和演讲者。 Java EE：Java EE Twitter官方账号。 Java Magazine：Java杂志官方账号。 Java.net：Java.net官方账号。 Java：Java Twitter官方账号。 Javin Paul：知名Java博客作者。 Lukas Eder：Data Geekery（jOOQ）创始人兼CEO。 Mario Fusco：RedHatter、JUG协调、活跃讲师和作者。 Mark Reinhold：Oracle首席架构师、Java平台开发组。 Martijn Verburg：London JUG co-leader、演讲者、作家、Java Champion等。 OpenJDK：OpenJDK官方账号。 Reza Rahman：Java EE、GlassFish、WebLogic传道者、作家、演讲者、开源黑客。 Simon Maple：Java Champion、virtualJUG创始人、LJC leader、RebelLabs作者。 Stephen Colebourne： Java Champion、演讲者。 Tim Boudreau：作家、NetBeans大牛。 Trisha Gee：Java Champion、演讲者。 微博、微信公众号 ImportNew 微博：@ImportNew ImportNew：最受欢迎的、专注Java技术分享的微信公众号。专注Java技术分享，包括Java基础技术、进阶技能、架构设计和Java技术领域动态等。 知名网站 值得关注的Java技术站点。 中文站点 ImportNew（ImportNew 专注 Java 技术） 英文站点 Android Arsenal Google Java Style：官网 InfoQ：官网 Java Code Geeks Java, SQL, and jOOQ Java.net Javalobby JavaWorld：官网 JAXenter：官网 RebelLabs The Java Specialist' Newsletter：官网 The Takipi Blog TheServerSide.com：服务器编程交流平台是一个老牌的IT信息网站，关注服务器端编程的，以Java和.Net周边信息为主。官网 Thoughts On Java Vanilla Java Vlad Mihalcea on Hibernate Voxxed OnJava：O'Reilly Java包含最新的Java技术资讯，优质代码，完全的实例和详解。官网 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:25:11 "},"akka/":{"url":"akka/","title":"akka","keywords":"","body":"akka第三方资源 [akka官方资源]（https://doc.akka.io/docs/akka/current/） private LinkedList getChildren(String path) { LinkedList strings = new LinkedList<>(); system.actorSelection(path).resolveOne(new Timeout(new FiniteDuration(1, TimeUnit.MINUTES))) .onComplete((Function1, Object>) v1 -> { //普通actor转换成RepointableActorRef RepointableActorRef repointableActorRef = (RepointableActorRef) v1.get(); strings.add(repointableActorRef.path().name()); repointableActorRef.children().foreach(new Function1>() { @Override public LinkedList apply(ActorRef v1) { return getChildren(v1.path().name()); } }); return strings; }, executionContext); return strings; } @Test //遍历所有的ACTOR public void whenRequest_thenActorResponds() { LinkedList strings = new LinkedList<>(); system.actorSelection(\"/user/\").resolveOne(new Timeout(new FiniteDuration(1, TimeUnit.MINUTES))) .onComplete((Function1, Object>) v1 -> { // /user: 守护角色与这个角色打交道最多的就是用户创建角色的父角色。我们将其命名为“/user”。所有以system.actorOf()创建的角色都是它的子角色。这意味着当这个角色终止时， //系统中所有的普通角色也都将被关闭。同时，这也意味着这个守护者的监管策略决定了最顶层的普通角色是怎样被监管的。从Akka版本2.1以后，可以通过配置akka.actor.guardian-supervisor-strategy // 来设置它。它的值必须是一个SupervisorStrategyConfigurator类的全路径。当该守护者向上汇报失败时，根守护者的响应就是终止该守护者，即停止整个角色系统。 LocalActorRef localActorRef = (LocalActorRef) v1.get(); strings.add(localActorRef.path().name()); localActorRef.children().foreach(new Function1() { @Override public Object apply(ActorRef v1) { getChildren(v1.path().name()); return null; } }); return localActorRef; }, executionContext); } Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-23 03:24:06 "},"work/note/openCode/":{"url":"work/note/openCode/","title":"gitbook","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/openCode/gitbook.html":{"url":"work/note/openCode/gitbook.html","title":"安装使用","keywords":"","body":"安装 node.js安装  打开nodejs官网：https://nodejs.org/en/  下载稳定版本(LTS)的node，下载完成之后通过如下命令检查： node -v // 检查node的版本，由于nodejsy已经集成了npm，所以安装了node，npm也一并安装好啦 npm -v // 检查npm的版本  如果是使用下载包安装node.js 则需要创建软链接 ln -s /usr/local/mine/node-v8.11.3-linux-x64/bin/node /usr/local/bin/node ln -s /usr/local/mine/node-v8.11.3-linux-x64/bin/npm /usr/local/bin/npm gitbook-cli安装 npm install gitbook-cli -g //mac && windows //gitbook-cli不同于gitbook，而是其一个命令行工具 gitbook -V // 注意V为大写，不识别小写v 常用命令 命令 解释 gitbook build 构建 gitbook build 生成静态网页 gitbook build --gitbook=2.0.1 指生成时指定 gitbook 的版本, 本地没有会先下载 gitbook build ./ --log=debug --debug 获取更好的错误消息（使用堆栈跟踪） gitbook fetch beta 下载并安装其他版本的 GitBook gitbook help 列出 gitbook 所有的命令 gitbook init 初始化 gitbook init ./directory 将书籍创建到一个新目录中 gitbook install 安装插件 gitbook ls-remote 会列举可以下载的版本 gitbook serve 运行一个 web 服务 gitbook serve --port 指定端口 gitbook update 更新到 gitbook 的最新版本 全局配置 title  设置书本的标题 \"title\" : \"Gitbook Use\" author  作者的相关信息 \"author\" : \"mingyue\" description  本书的简单描述 \"description\" : \"记录Gitbook的配置和一些插件的使用\" language  Gitbook使用的语言, 版本2.6.4中可选的语言如下： en, ar, bn, cs, de, en, es, fa, fi, fr, he, it, ja, ko, no, pl, pt, ro, ru, sv, uk, vi, zh-hans, zh-tw  例如，配置使用简体中文 \"language\" : \"zh-hans\" links  在左侧导航栏添加链接信息 \"links\" : { \"sidebar\" : { \"Home\" : \"https://www.baidu.com\" } } styles  自定义页面样式， 默认情况下各generator对应的css文件 \"styles\": { \"website\": \"styles/website.css\", \"ebook\": \"styles/ebook.css\", \"pdf\": \"styles/pdf.css\", \"mobi\": \"styles/mobi.css\", \"epub\": \"styles/epub.css\" }  例如使 标签有下边框， 可以在website.css中设置 h1 , h2{ border-bottom: 1px solid #EFEAEA; } 插件 插件配置 配置使用的插件 \"plugins\": [ \"-search\", \"back-to-top-button\", \"expandable-chapters-small\", \"insert-logo\" ]  其中\"-search\"中的 - 符号代表去除默认自带的插件  Gitbook默认自带有5个插件： highlight： 代码高亮 search： 导航栏查询功能（不支持中文） sharing：右上角分享功能 font-settings：字体设置（最上方的\"A\"符号） livereload：为GitBook实时重新加载 插件属性配置pluginsConfig  配置插件的属性,例如配置insert-logo的属性： \"pluginsConfig\": { \"insert-logo\": { \"url\": \"images/logo.png\", \"style\": \"background: none; max-height: 30px; min-height: 30px\" } } 生成pdf  1. 安装 Calibre apt install calibre  2. 配置book.json { \"title\": \"Cloudroom SDK Refference\", \"description\": \"云屋视频SDK参考文档\", \"author\": \"HeDonghai\", \"language\": \"zh-hans\", \"styles\": { \"website\": \"style.css\" }, \"pluginsConfig\": { \"fontSettings\": { \"theme\": \"white\", \"family\": \"msyh\", \"size\": 2 } }, \"pdf\": { \"toc\": true, \"pageNumbers\": true, \"fontSize\": 18, \"paperSize\": \"a3\", \"margin\": { \"right\": 30, \"left\": 30, \"top\": 30, \"bottom\": 50 } } }  3. 安装ebook-convert npm install ebook-convert -g  4. 生成pdf gitbook pdf  5. pdf文档内中文不显示或中文乱码原因是中文字体未支持，可手动配置支持，操作如下： 在插件配置中使用”yahei”(雅黑字体)并配置fontSettings 手动从windows系统的Fonts目录下复制c/windows/Fonts/msyh.ttc文件或msyh.ttf文件上传到Linux的/usr/share/fonts/truetype目录下 重新生成pdf即可 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/openCode/gitbook2.html":{"url":"work/note/openCode/gitbook2.html","title":"插件","keywords":"","body":"常用插件  用法：在book.json中添加\"plugins\"和\"pluginConfig\"字段。然后执行gitbook install，或者使用NPM安装npm install gitbook-plugin-插件名，也可以从源码GitHub地址中下载，放到node_modules文件夹里（GitHub地址在进入插件地址右侧的GitHub链接） 常用插件 作用 back-to-top-button 回到顶部 chapter-fold 导航目录折叠，支持多层目录，点击导航栏的标题名就可以实现折叠扩展。（最好用） expandable-chapters-small 导航目录 expandable-chapters 导航 code 代码添加行号&复制按钮（可选) todo 待做项 advanced-emoji 支持emoji表情 github 在右上角添加github图标 search-pro 中文搜索，需要去掉默认插件：\"-lunr\", \"-search\", emphasize 为文字加上底色 splitter 侧边栏宽度可调节 sharing-plus 分享 tbfed-pagefooter 页面添加页脚（简单的）添加页脚，版权信息 page-copyright 页面页脚版权（内容多） sectionx 将页面分块显示.用于将页面分成多个部分 hide-element 隐藏元素 flexible-alerts 警报；这个GitBook插件将块引用转换为漂亮的警报。可以在全局和警报特定级别配置外观 auto-scroll-table 表格滚动条。为避免表格过宽，增加滚动条 popup 弹出大图。单击图片，在新页面查看大图。 lightbox 单击查看图片。点击图片可显示，大小不变 click-reveal 点击显示。默认隐藏，点击可显示。 custom-favicon 修改标题栏图标 accordion 折叠模块 插件详细介绍 code  代码添加行号&复制按钮（可选) ，如果想去掉复制按钮，在book.json的插件配置块更新： { \"plugins\" : [ \"code\" ], \"pluginsConfig\": { \"code\": { \"copyButtons\": false } } } todo 待做项☑  添加 Todo 功能。默认的 checkbox 会向右偏移 2em，如果不希望偏移，可以在 website.css 里加上下面的代码: input[type=checkbox]{ margin-left: -2em; } 使用示例： * [ ] write some articles * [x] drink a cup of tea insert-logo 插入logo将logo插入到导航栏上方中 { \"plugins\": [ \"insert-logo\" ] \"pluginsConfig\": { \"insert-logo\": { \"url\": \"images/logo.png\", \"style\": \"background: none; max-height: 30px; min-height: 30px\" } } } github 在右上角添加github图标 { \"plugins\": [ \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/zhangjikai\" } } } emphasize 为文字加上底色 { \"plugins\": [ \"emphasize\" ] } 然后在markdown / asciidoc内容中，使用以下内容突出显示一些文本： This text is highlighted ! This text is highlighted with **markdown**! This text is highlighted in green! This text is highlighted in red! This text is highlighted with a custom color! tbfed-pagefooter 页面添加页脚（简单的）添加页脚，版权信息 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy xxxx.com 2017\", \"modify_label\": \"该文件修订时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } } page-copyright 页面页脚版权（内容多） { \"plugins\" : [\"page-copyright\"], \"pluginsConfig\" : { \"page-copyright\": { \"description\": \"modified at\", \"signature\": \"你的签名\", \"wisdom\": \"Designer, Frontend Developer & overall web enthusiast\", \"format\": \"YYYY-MM-dd hh:mm:ss\", \"copyright\": \"Copyright &#169; 你的名字\", \"timeColor\": \"#666\", \"copyrightColor\": \"#666\", \"utcOffset\": \"8\", \"style\": \"normal\", \"noPowered\": false, } } } sectionx 将页面分块显示.用于将页面分成多个部分，并添加按钮以允许读者控制每个部分的可见性。 插件配置 命令 解释 data-collapse 表示默认情况下是否打开（但仍然可见）该部分 data-id 对按钮控制很有用（在下一节中讨论） data-id 对按钮控制很有用（在下一节中讨论） data-nopdf 该部分是否将隐藏在 pdf 导出中。true：面板不会显示在.pdf 或.epub 中。 data-show true：默认情况下，面板内容对用户可见，面板标题可以单击。 false：默认情况下，面板内容对用户隐藏，面板标题不可点击，只能通过添加自定义按钮查看（在下一节中讨论） data-show true：默认情况下，面板内容对用户可见，面板标题可以单击。 false：默认情况下，面板内容对用户隐藏，面板标题不可点击，只能通过添加自定义按钮查看（在下一节中讨论） data-show true：默认情况下，面板内容对用户可见，面板标题可以单击。 false：默认情况下，面板内容对用户隐藏，面板标题不可点击，只能通过添加自定义按钮查看（在下一节中讨论） data-title 该部分的标题，它将显示为 bootstrap 面板的标题（大小为 h2）。请注意，您不能使用\"标题中的字符，请\"改用。 data-title 该部分的标题，它将显示为 bootstrap 面板的标题（大小为 h2）。请注意，您不能使用\"标题中的字符，请\"改用。 ancre-navigation| 悬浮目录和回到顶部 klipse |嵌入类似IDE的功能。嵌入一块功能，可在代码段中实时交互，即（输入代码 > 执行结果 | 按钮配置  通过在GitBook中添加内联HTML，以下代码可以添加一个按钮，以允许您查看或隐藏其他部分。 简单来说，就是在【使用1】的内容部分添加一个按钮： 参数 说明 class 该按钮必须属于类“section”。//这里就是用到 1 部分的 section class 该按钮必须属于类“section”。//这里就是用到 1 部分的 section hide 目标部分可见时按钮上的文本。 show 隐藏目标部分时按钮上的文本。 target 当按下时，将切换 id 为 target 的部分。 target 当按下时，将切换 id 为 target 的部分。 markdown中示例代码： \\ \\` 混合使用 将第2节的button块添加到第1节的内容部分 markdown中示例代码： \\ data \\ \\ \\ \\ page-treeview 生成页内目录，不需要插入标签，能支持到6级目录，安装可用 { \"plugins\": [\"page-treeview\"] } 非必要的配置项： { \"plugins\": [ \"page-treeview\" ], \"pluginsConfig\": { \"page-treeview\": { \"copyright\": \"Copyright &#169; aleen42\", \"minHeaderCount\": \"2\", \"minHeaderDeep\": \"2\" } } } simple-page-toc  生成本页目录，需要在文章中插入标签，支持1-3级目录；页面顶端生成。另外 GitBook 在处理重复的标题时有些问题，所以尽量不适用重复的标题。 { \"plugins\" : [ \"simple-page-toc\" ], \"pluginsConfig\": { \"simple-page-toc\": { \"maxDepth\": 3, \"skipFirstH1\": true } } } Column A Column B \"maxDepth\" 使用深度最多为 maxdepth 的标题。 skipFirstH1 排除文件中的第一个 h1 级标题。 使用方法: 在需要生成目录的地方用下面的标签括起来，全文都生成的话就在首尾添加 \\ page-toc-button 悬浮目录 donate 打赏插件 change_girls 可自动切换的背景 alerts 警报 accordion 折叠模块。这个插件名叫手风琴，可以实现将内容隐藏起来，外部显示模块标题和显示箭头，点击箭头可显示里面的内容。 内容部分 page-toc-button 悬浮目录 { \"plugins\" : [ \"page-toc-button\" ], \"pluginsConfig\": { \"page-toc-button\": { \"maxTocDepth\": 2, \"minTocSize\": 2 } } } donate 打赏插件 { \"plugins\": [ \"donate\" ], \"pluginsConfig\": { \"donate\": { \"wechat\": \"微信收款的二维码URL\", \"alipay\": \"支付宝收款的二维码URL\", \"title\": \"\", \"button\": \"赏\", \"alipayText\": \"支付宝打赏\", \"wechatText\": \"微信打赏\" } } } change_girls 可自动切换的背景 { \"plugins\":[\"change_girls\"], \"pluginsConfig\": { \"change_girls\" : { \"time\" : 10, \"urls\" : [ \"girlUrl1\", \"girlUrl2\",...\"\" ] } } } alerts 警报 这个GitBook插件将块引用转换为漂亮的警报。 插件地址 在book.json中添加以下内容。然后执行gitbook install，或者使用NPM安装npm install gitbook-plugin-flexible-alerts { \"plugins\": [\"alerts\"] } 用法样式： 信息样式 [info] For info Use this for infomation messages. 警告造型 [warning] For warning Use this for warning messages. 危险造型 [danger] For danger Use this for danger messages. 成功造型 [success] For success Use this for success messages. accordion 折叠模块。这个插件名叫手风琴，可以实现将内容隐藏起来，外部显示模块标题和显示箭头，点击箭头可显示里面的内容。 用法： 编辑内容，用下面的标签括起来。 %accordion%模块标题%accordion% 内容部分 %/accordion% Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 22:18:00 "},"work/note/odl/":{"url":"work/note/odl/","title":"odl","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/odl/netvirt/":{"url":"work/note/odl/netvirt/","title":"netvirt","keywords":"","body":"代码结构目前，合并之后的Netvirt主要以VPNService为基础来进行演进的，下面我们将以carbon为例来介绍VPNService的实现过程。VPNService中的主要模块，如下表所示： 主要模块 功能 Neutronvpn 监听Datastore中由neutron-northbound plugin的transcriber转换后的数据结构，再次转换为vpndervice中的数据结构，并存入datastroe。 dhcpservice0 控制器代答dhcp ElanManager 维护租户网络的二层连通性 VPNManager 维护租户网络的三层连通性，需要通过bgpMansger操纵控制满，通过fibManager来操作数据平面。 bgpManager 接受VPNManager的控制，通过Thrift与外部的BGP引擎进行通信 fibManager FIB（Forward Information dataBase转发表）接受VPNmansger的控制，向OVS下发流表，转发三层流量。 natService 支持SNAT和DANT aclservice 提供安全组功能 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/odl/netvirt/neutronvpn.html":{"url":"work/note/odl/netvirt/neutronvpn.html","title":"neutronvpn","keywords":"","body":"主要功能  监听Datastore中由neutron-northbound plugin的transcriber转换后的数据结构，再次转换为vpndervice中的数据结构，并存入datastroe。 主要结构 api: yang: neutronvpn impl: 类 监听库 操作库 NeutronBgpvpnChangeListener neutron:neutron/bgpvpns NeutronFloatingToFixedIpMappingChangeListener NeutronHostConfigChangeListener NeutronNetworkChangeListener neutron:neutron/networks NeutronPortChangeListener NeutronRouterChangeListener NeutronSecurityRuleConstants NeutronSubnetChangeListener NeutronSubnetGwMacResolver NeutronTrunkChangeListener UpgradeStateListener IPV6InternetDefaultRouteProgrammer NeutronvpnManager NeutronvpnManagerImpl NeutronvpnNatManager NeutronvpnUtils NeutronExternalSubnetHandler NeutronNetworkChangeListener add node1 Create ietf-interfaces based on the ELAN segment type. For segment type flat - create transparent interface pointing to thepatch-port attached to the physnet port. For segment type vlan - create trunk interface pointing to the patch-portattached to the physnet port + trunk-member interface pointing to thetrunk interface. node2 使用非集群监听，并通过 ConcurrentMap 缓存数据，若A为master时，创建网络并缓存，一旦集群震荡，B切换为master，此时删除网络，则A中ConcurrentMap会产生数据残留， 不清楚是否会影响业务，但是肯定会造成内存泄漏。 是否可以使用集群监听，所有节点均对缓存进行操作，但是仅仅只有owner处理业务逻辑EntityOwnershipUtils NeutronSubnetChangeListener add NeutronRouterChangeListener add Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/openstack/":{"url":"work/note/openstack/","title":"openstack","keywords":"","body":"Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/openstack/error.html":{"url":"work/note/openstack/error.html","title":"错误集锦","keywords":"","body":"端口绑定失败  PortBindingFailed: Binding failed for  检查ml2_conf.ini [ml2_type_vlan] network_vlan_ranges = external:1:1000 控制台无法登陆  检查防火墙,即使防火墙关闭，仍有可能在生效。使用 ipstables -F 清除所有防火墙规则 Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/inspur/overlay/framework.html":{"url":"work/note/inspur/overlay/framework.html","title":"架构","keywords":"","body":" Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "},"work/note/inspur/overlay/analysis.html":{"url":"work/note/inspur/overlay/analysis.html","title":"分析","keywords":"","body":"日志分析 创建网络 日志 #### 创建网络 overlay netruon 介入 ##### IceNeutronNetworkChangeListener IceNeutronNetworkChangeListener data tree changed. IceNeutronNetworkChangeListener WRITE IceNetwork IceNeutronNetworkChangeListener IceNeutronNetworkChangeListener Adding Network:Name=network, isAdminStateUp=true, isShared=false,getNetworkType=IceNetworkTypeVxlan, getSegmentationId=10071}, IceNetworkL3Extension{isExternal=false}}} #### mapper 介入 写入vxlan id #### IceNeutronUtils getSegmentationId 10071 IceNeutronNetworkChangeListener Network segmentationID: value=10071 IceNeutronNetworkChangeListener handle vxlan network created with segmentationId 10071 #### 获取server信息 #### IceNeutronUtils serverLeafSet:[Uri [_value=iceDeviceId:172.20.1.174]] IceVxlanManager ChannelId is : Uri [_value=iceDeviceId:172.20.1.174] with device : Uri [_value=iceDeviceId:172.20.1.174] #### 获取vlan id #### IceOverlayPoolsManager Allocate id: 500 ,poolType :1 #### ICe vxlan介入 #### IceVxlanCacheManager writeL2Vxlan IceDeviceVxlanL2 iceDeviceId:172.20.1.174, _iceNetworkId=21e67093-9cf3-4172-9943-64fca7be2da5, _iceNetworkType=IceNetworkTypeVxlan, _iceVlanId=500, _iceVxlanId=10071 IceVxlanL2ChangeListener IceVxlanL2ChangeListener Write #### overlay调用underlay下发配置 #### DistributeConfig distribute vxlanConfigl2Dynamic input VconfigType=Config, _deviceIp=Ipv4Address [_value=172.20.1.174], vlanId=500, _vni=10071, _isMulticastMode=true ConfigCache init vni : 10071 for device : iceDeviceId:172.20.1.174 DistributeConfig modifyDeviceVniCache true EvpnVni config segmentId start DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V #### underlay 第一条命令（主要配置vni） l2Config.configEvpnL2 #### HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;evpn ;vni 10071 L2 ;rd auto ;route-target import auto ;route-target export auto ;exit EvpnVni save segment id of 172.20.1.174 succeeded EvpnVni configSegmentId command exec succeed, deviceIp is :172.20.1.174 DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V #### underlay 第二条命令（配置vlan-vxlan映射） l2Config.configVlanVnSegmentL2 #### HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;vlan 500 ;no vn-segment ;vn-segment 10071 ;end Vlan configVlanVnSegment sendCommands return :true Vlan vlan is not created, so create vlan! #### overlay mapper收到对device switch 的vlan号修改 l2Config.configNve1L2Multicast #### IceLeafSwitchChangeListener LeafSwitch getDeviceVlanId=1,500, 修改device vlan IceUnderlayPoolManager | 44 - com.inspur.ice.resourcepool-impl - 0.1.0 | Allocate ip: 225.0.0.1 ,poolType :9 DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V #### underlay 第三条命令（配置bgp loopback）#### HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;interface nve1 ;host-reachability protocol bgp ;shutdown ;source-interface loopback 95 ;no shutdown ;member vni 10071 ;suppress-arp ;mcast-group 225.0.0.1 ;end 交换机配置  部分配置对不上，是因为不是同一次截取 vlan 2 vn-segment 10070 interface nve1 no shutdown host-reachability protocol bgp source-interface loopback95 member vni 10070 suppress-arp mcast-group 225.0.0.1 router bgp 65101 evpn vni 10070 l2 rd auto route-target import auto route-target export auto 命令 devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;evpn ;vni 10071 L2 ;rd auto ;route-target import auto ;route-target export auto ;exit devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;vlan 500 ;no vn-segment ;vn-segment 10071 ;end devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;interface nve1 ;host-reachability protocol bgp ;shutdown ;source-interface loopback 95 ;no shutdown ;member vni 10071 ;suppress-arp ;mcast-group 225.0.0.1 ;end 创建子网（同时会创建dhcp端口） 日志分析 #### Ice Neutron介入监听到子网创建 #### IceNeutronSubnetChangeListener IceNeutronSubnetChangeListener data tree changed. IceNeutronSubnetChangeListener WRITE IceSubnet IceNeutronSubnetChangeListener IceNeutronSubnetChangeListener Adding Subnet : IceNeutronUtils getSegmentationId 10071 #### overlay mapper 介入，写 vxlan l2 #### IceNeutronUtils serverLeafSet:[Uri [_value=iceDeviceId:172.20.1.177], Uri [_value=iceDeviceId:172.20.1.174]] IceVxlanL2ChangeListener IceVxlanL2ChangeListener 写入subnet信息 ConfigCache ADD IceDeviceVxlanL2{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], getIceInterfaceConfig=[], getIceNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getIceSubnet=[IceSubnet{getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=500], getIceVxlanId=IceVxlanId [_value=10071], augmentations={}} failed #### Ice Neutron介入监听dhcp port创建 (多次信息写入)#### IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. IceNeutronPortChangeListener IcePort data tree changed. IceNeutronPortChangeListener port vtepType null IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. IceNeutronPortChangeListener port vtepType HW IceNeutronPortChangeListener Add ice port with hw host IceVxlanManager getHostNodeFromHostName jw-controller IceVxlanManager uncompleted topology IceNeutronPortChangeListener Get portBindingInfo [IceVxlanManager$PortBindingInfo@16b5b00c] with icePort Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9] #### overlay mapper 介入，写vxlan l2#### IceVlanCacheManager writeNeutronSubnet IceVlanCacheManager writeNeutronPort Port netvirt.natservice.internal.NatUtil getSubnetGwMac : No GW ip found for subnet 49c27f08-05d1-3f62-a810-91a6f700856a NeutronPortChangeListener NeutronPortChangeListener data tree changed. NeutronPortChangeListener NeutronPortChangeListener data tree changed write. IceVxlanCacheManager writeIceInterfacePort IceInterfaceConfig #### overlay config 介入，准备下发配置 #### IceVxlanL2ChangeListener IceVxlanL2ChangeListener Write 端口写入 DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V SshcProvider sshInput is : SshInput [_enablePassword=inspur@123, _sshCommand=[SshCommand [_command=show running-config interface Ethernet1/1, augmentation=[]]], _sshIp=172.20.1.177, _sshPassword=inspur@123, _sshUser=admin, _nxapiUsed=false, augmentation=[]] #### netvirt 介入#### NeutronvpnManager updateSubnetmapNodeWithPorts NatUtil getSubnetGwMac : No GW ip found for subnet 49c27f08-05d1-3f62-a810-91a6f700856a VpnSubnetRouteHandler SUBNETROUTE: onPortAddedToSubnet: Port 85617daf-4a17-49c0-a3a2-9841625aa5c9 being added to subnet 49c27f08-05d1-3f62-a810-91a6f700856a #### genius 介入#### InterfacemgrProvider Interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 is not present ElanInterfaceManager Interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 is removed from Interface Oper DS due to port down InterfaceConfigListener parent refs not specified for 85617daf-4a17-49c0-a3a2-9841625aa5c9 #### 返回netvirt #### VpnSubnetRouteHandler SUBNETROUTE #### underlay 下发配置 #### SshcProvider Vlan checkPortIsInPortChannel sendCommands return :true VlanProvider setPort2VlanMode start IceLeafSwitchChangeListener LeafSwitch write. DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;vlan 500 ;interface Ethernet1/1 ;switchport ;switchport mode trunk ;switchport trunk allowed vlan 500 ;no shutdown Vlan setPort2VlanMode1 sendCommands return :true #### genius netvirt 介入监听端口状态 ietf-interfaces:interfaces-state #### statehelpers.OvsInterfaceStateAddHelper Adding Interface State to Oper DS for interface: tap85617daf-4a netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: add: Processed interface tap85617daf-4a up event genius.utils.batching.ResourceBatchingManager Total taken ##time = 2ms for resourceList of size 1 for resourceType INTERFACEMGR-DEFAULT-OPERATIONAL genius.interfacemanager.renderer.ovs.confighelpers.OvsInterfaceConfigUpdateHelper port attributes modified, requires a delete and recreate of 85617daf-4a17-49c0-a3a2-9841625aa5c9 configuration genius.interfacemanager.commons.InterfaceManagerCommonUtils Creating child interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 of type Trunk bound on parent-interface tap85617daf-4a genius.interfacemanager.renderer.ovs.confighelpers.OvsInterfaceConfigAddHelper adding vlan configuration for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 #### genius id manager分配id#### genius.idmanager.IdManager Got pool IdLocalPool [poolName=interfaces.2130706689, availableIds=AvailableIdHolder [low=1, high=6553, cur=3], releasedIds=ReleasedIdHolder [availableIdCount=2, timeDelaySec=30, delayedEntries=[{Id: 3 ReadyTimeSec: 1553930804}, {Id: 1 ReadyTimeSec: 1553930806}]]] genius.idmanager.IdManager The newIdValues [3] for the idKey 85617daf-4a17-49c0-a3a2-9841625aa5c9 #### genius netvirt 绑定服务 （datastore interface-service-bindings）#### #### { \"name\": \"85617daf-4a17-49c0-a3a2-9841625aa5c9\", \"type\": \"iana-if-type:l2vlan\", \"enabled\": true, \"odl-interface:l2vlan-mode\": \"trunk\", \"odl-interface:parent-interface\": \"tap85617daf-4a\" } #### genius.interfacemanager.IfmUtil Binding Service default.85617daf-4a17-49c0-a3a2-9841625aa5c9 for : 85617daf-4a17-49c0-a3a2-9841625aa5c9 netvirt.vpnmanager.InterfaceStateChangeListener VPN Interface add event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 from InterfaceStateChangeListener netvirt.natservice.internal.NatInterfaceStateChangeListener call : Unable to process add for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 netvirt.vpnmanager.InterfaceStateChangeListener Detected interface add event for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 genius.interfacemanager.servicebindings.flowbased.listeners.FlowBasedServicesConfigListener Service Binding Entry created for Interface: 85617daf-4a17-49c0-a3a2-9841625aa5c9, ServiceName: default.85617daf-4a17-49c0-a3a2-9841625aa5c9, ServicePriority 9 genius.interfacemanager.servicebindings.flowbased.utilities.FlowBasedServicesUtils adding bound-service state information for interface : 85617daf-4a17-49c0-a3a2-9841625aa5c9, service-mode : yang.gen.v1.urn.opendaylight.genius.interfacemanager.servicebinding.rev160406.ServiceModeEgress netvirt.ipv6service.Ipv6ServiceInterfaceEventListener Port Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9] does not include IPv6Address, skipping. #### genius netvirt 收到 85617daf-4a17-49c0-a3a2-9841625aa5c9 端口状态改变事件#### netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: add: Processed interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 up event netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: add: Received port UP event for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 subnetId 49c27f08-05d1-3f62-a810-91a6f700856a genius.interfacemanager.servicebindings.flowbased.config.helpers.FlowBasedEgressServicesConfigBindHelper binding egress service default.85617daf-4a17-49c0-a3a2-9841625aa5c9 for interface: 85617daf-4a17-49c0-a3a2-9841625aa5c9 genius.idmanager.jobs.UpdateIdEntryJob Updated id entry with idValues [3], idKey 85617daf-4a17-49c0-a3a2-9841625aa5c9, pool interfaces.2130706689 #### DpnInterfaceListener 监听elan ，保证路由通告消息进入#### DpnInterfaceListener DpnInterfaceListener data tree changed. DpnInterfaceListener begin processFlowTableEgressAclDel DpnInterfaceListener data is null or dpninterface is null!! genius.interfacemanager.InterfacemgrProvider Create VLAN interface : 115377256237675:ens192:500 DpnInterfaceListener begin processFlowTableEgressAclAdd!!! DpnInterfaceListener begin for ifname is 85617daf-4a17-49c0-a3a2-9841625aa5c9!!! DpnInterfaceListener begin getIcePortByIfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 com.inspur.ice.overlaymapper.util.Utils begin getIcePortByIfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 com.inspur.ice.overlaymapper.util.Utils begin read ds 85617daf-4a17-49c0-a3a2-9841625aa5c9 DpnInterfaceListener begin check is nova port genius.interfacemanager.InterfacemgrProvider Interface 115377256237675:ens192:500 is not present netvirt.elan.internal.ElanInterfaceManager Interface 115377256237675:ens192:500 is removed from Interface Oper DS due to port down genius.interfacemanager.renderer.ovs.confighelpers.OvsVlanMemberConfigAddHelper adding vlan member configuration for interface 115377256237675:ens192:500 #### genius id manager分配id(上面获取不到后面才创建？)#### genius.idmanager.IdManager Got pool IdLocalPool [poolName=interfaces.2130706689, availableIds=AvailableIdHolder [low=1, high=6553, cur=3], releasedIds=ReleasedIdHolder [availableIdCount=1, timeDelaySec=30, delayedEntries=[{Id: 1 ReadyTimeSec: 1553930806}]]] genius.idmanager.IdManager The newIdValues [1] for the idKey 115377256237675:ens192:500 genius.idmanager.jobs.UpdateIdEntryJob Updated id entry with idValues [1], idKey 115377256237675:ens192:500, pool interfaces.2130706689 genius.interfacemanager.IfmUtil Binding Service default.115377256237675:ens192:500 for : 115377256237675:ens192:500 netvirt.vpnmanager.InterfaceStateChangeListener VPN Interface add event - intfName 115377256237675:ens192:500 from InterfaceStateChangeListener netvirt.vpnmanager.InterfaceStateChangeListener Detected interface add event for interface 115377256237675:ens192:500 netvirt.natservice.internal.NatInterfaceStateChangeListener call : Unable to process add for interface 115377256237675:ens192:500 #### 发现服务被绑定，下发流表 datastore interface-service-bindings#### genius.interfacemanager.servicebindings.flowbased.listeners.FlowBasedServicesConfigListener Service Binding Entry created for Interface: 115377256237675:ens192:500, ServiceName: default.115377256237675:ens192:500, ServicePriority 9 genius.interfacemanager.servicebindings.flowbased.utilities.FlowBasedServicesUtils adding bound-service state information for interface : 115377256237675:ens192:500, service-mode : yang.gen.v1.urn.opendaylight.genius.interfacemanager.servicebinding.rev160406.ServiceModeEgress genius.interfacemanager.servicebindings.flowbased.config.helpers.FlowBasedEgressServicesConfigBindHelper binding egress service default.115377256237675:ens192:500 for interface: 115377256237675:ens192:500 genius.utils.batching.ResourceBatchingManager Total taken ##time = 2ms for resourceList of size 4 for resourceType INTERFACEMGR-DEFAULT-OPERATIONAL netvirt.elan.internal.InterfaceAddWorkerOnElanInterface Handling elan interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 add for elan 476baec5-fd39-349d-a84e-57c1d2221ec5 DpnInterfaceListener DpnInterfaceListener data tree changed. DpnInterfaceListener begin processFlowTableEgressAclDel DpnInterfaceListener begin get EgressAclFlow 85617daf-4a17-49c0-a3a2-9841625aa5c9 DpnInterfaceListener begin get EgressAclFlowoption 85617daf-4a17-49c0-a3a2-9841625aa5c9 DpnInterfaceListener processFlowTableEgressAclDel EgressAclFlow is null!! ifname is 85617daf-4a17-49c0-a3a2-9841625aa5c9 DpnInterfaceListener begin processFlowTableEgressAclAdd!!! DpnInterfaceListener begin for ifname is 85617daf-4a17-49c0-a3a2-9841625aa5c9!!! DpnInterfaceListener begin getIcePortByIfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 com.inspur.ice.overlaymapper.util.Utils begin getIcePortByIfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 com.inspur.ice.overlaymapper.util.Utils begin read ds 85617daf-4a17-49c0-a3a2-9841625aa5c9 DpnInterfaceListener begin check is nova port DpnInterfaceListener begin for ifname is 115377256237675:ens192:500!!! DpnInterfaceListener ifname contain dpid , don't need process 115377256237675:ens192:500 ####elan，/operational/elan:elan-forwarding-tables #### netvirt.elan.internal.ElanInterfaceManager Programming remote dmac flows on the newly connected dpn 115377256237675 for elan 476baec5-fd39-349d-a84e-57c1d2221ec5 netvirt.elan.internal.ElanInterfaceManager programming smac and dmacs for fa:16:3e:d8:2c:da on source and other DPNs for elan 476baec5-fd39-349d-a84e-57c1d2221ec5 and interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 netvirt.elan.evpn.listeners.ElanMacEntryListener ElanMacEntryListener : ADD macEntry KeyedInstanceIdentifier{targetType=interface yang.gen.v1.urn.opendaylight.netvirt.elan.rev150602.forwarding.entries.MacEntry, path=[yang.gen.v1.urn.opendaylight.netvirt.elan.rev150602.ElanForwardingTables, yang.gen.v1.urn.opendaylight.netvirt.elan.rev150602.elan.forwarding.tables.MacTable[key=MacTableKey [_elanInstanceName=476baec5-fd39-349d-a84e-57c1d2221ec5]], yang.gen.v1.urn.opendaylight.netvirt.elan.rev150602.forwarding.entries.MacEntry[key=MacEntryKey [_macAddress=PhysAddress [_value=fa:16:3e:d8:2c:da]]]]} genius.interfacemanager.servicebindings.flowbased.listeners.FlowBasedServicesConfigListener Service Binding Entry created for Interface: 85617daf-4a17-49c0-a3a2-9841625aa5c9, ServiceName: elan.476baec5-fd39-349d-a84e-57c1d2221ec5.85617daf-4a17-49c0-a3a2-9841625aa5c9, ServicePriority 9 genius.interfacemanager.servicebindings.flowbased.utilities.FlowBasedServicesUtils adding bound-service state information for interface : 85617daf-4a17-49c0-a3a2-9841625aa5c9, service-mode : yang.gen.v1.urn.opendaylight.genius.interfacemanager.servicebinding.rev160406.ServiceModeIngress genius.interfacemanager.servicebindings.flowbased.config.helpers.FlowBasedIngressServicesConfigBindHelper binding ingress service elan.476baec5-fd39-349d-a84e-57c1d2221ec5.85617daf-4a17-49c0-a3a2-9841625aa5c9 for vlan port: 85617daf-4a17-49c0-a3a2-9841625aa5c9 #### genius netvirt 收到odl-interface:parent-interface\": \"tap85617daf-4a 端口状态改变事件#### netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Processed Interface tap85617daf-4a update event netvirt.vpnmanager.InterfaceStateChangeListener VPN Interface update event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 from InterfaceStateChangeListener netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Processed Interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 update event netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Received port UP event for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 in subnet 49c27f08-05d1-3f62-a810-91a6f700856a netvirt.elan.internal.InterfaceAddWorkerOnElanInterface Handling elan interface 115377256237675:ens192:500 add for elan 476baec5-fd39-349d-a84e-57c1d2221ec5 genius.interfacemanager.servicebindings.flowbased.listeners.FlowBasedServicesConfigListener Service Binding Entry created for Interface: 115377256237675:ens192:500, ServiceName: elan.476baec5-fd39-349d-a84e-57c1d2221ec5.115377256237675:ens192:500, ServicePriority 9 genius.interfacemanager.servicebindings.flowbased.utilities.FlowBasedServicesUtils adding bound-service state information for interface : 115377256237675:ens192:500, service-mode : yang.gen.v1.urn.opendaylight.genius.interfacemanager.servicebinding.rev160406.ServiceModeIngress genius.interfacemanager.servicebindings.flowbased.config.helpers.FlowBasedIngressServicesConfigBindHelper binding ingress service elan.476baec5-fd39-349d-a84e-57c1d2221ec5.115377256237675:ens192:500 for vlan port: 115377256237675:ens192:500 netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Processed Interface tap85617daf-4a update event netvirt.vpnmanager.InterfaceStateChangeListener VPN Interface update event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 from InterfaceStateChangeListener netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Processed Interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 update event netvirt.vpnmanager.SubnetRouteInterfaceStateChangeListener SUBNETROUTE: update: Received port UP event for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 in subnet 49c27f08-05d1-3f62-a810-91a6f700856a IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. IceNeutronPortChangeListener IcePort data tree changed. IceNeutronPortChangeListener WRITE IceNeutronPortChangeListener Updating Port : original = IcePortKey [_uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]] update = IcePortKey [_uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]] IceVlanCacheManager writeNeutronPort Port [_allowedAddressPairs=[], _deviceId=dhcp639d7107-1693-5abe-bb94-18ba198147ec-21e67093-9cf3-4172-9943-64fca7be2da5, _deviceOwner=network:dhcp, _extraDhcpOpts=[], _fixedIps=[FixedIps [_ipAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]], _key=FixedIpsKey [_subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], _ipAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]]], _subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], augmentation=[]]], _key=PortKey [_uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]], _macAddress=MacAddress [_value=fa:16:3e:d8:2c:da], _name=, _networkId=Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5], _revisionNumber=7, _securityGroups=[], _tenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], _uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9], _adminStateUp=true, augmentation=[PortBindingExtension [_hostId=jw-controller, _vifDetails=[VifDetails [_detailsKey=has_datapath_type_netdev, _key=VifDetailsKey [_detailsKey=has_datapath_type_netdev], _value=false, augmentation=[]], VifDetails [_detailsKey=uuid, _key=VifDetailsKey [_detailsKey=uuid], _value=dc328b61-430d-4d06-a432-a9e3afc9660a, augmentation=[]], VifDetails [_detailsKey=support_vhost_user, _key=VifDetailsKey [_detailsKey=support_vhost_user], _value=false, augmentation=[]]], _vifType=ovs, _vnicType=normal], PortSecurityExtension [_portSecurityEnabled=false]]] IceVlanCacheManager vlanPortMap ：ADD Port [_allowedAddressPairs=[], _deviceId=dhcp639d7107-1693-5abe-bb94-18ba198147ec-21e67093-9cf3-4172-9943-64fca7be2da5, _deviceOwner=network:dhcp, _extraDhcpOpts=[], _fixedIps=[FixedIps [_ipAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]], _key=FixedIpsKey [_subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], _ipAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]]], _subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], augmentation=[]]], _key=PortKey [_uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]], _macAddress=MacAddress [_value=fa:16:3e:d8:2c:da], _name=, _networkId=Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5], _revisionNumber=7, _securityGroups=[], _tenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], _uuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9], _adminStateUp=true, augmentation=[PortBindingExtension [_hostId=jw-controller, _vifDetails=[VifDetails [_detailsKey=has_datapath_type_netdev, _key=VifDetailsKey [_detailsKey=has_datapath_type_netdev], _value=false, augmentation=[]], VifDetails [_detailsKey=uuid, _key=VifDetailsKey [_detailsKey=uuid], _value=dc328b61-430d-4d06-a432-a9e3afc9660a, augmentation=[]], VifDetails [_detailsKey=support_vhost_user, _key=VifDetailsKey [_detailsKey=support_vhost_user], _value=false, augmentation=[]]], _vifType=ovs, _vnicType=normal], PortSecurityExtension [_portSecurityEnabled=false]]] failed NeutronPortChangeListener NeutronPortChangeListener data tree changed. NeutronPortChangeListener NeutronPortChangeListener data tree changed write. netvirt.neutronvpn.NeutronPortChangeListener Update port 85617daf-4a17-49c0-a3a2-9841625aa5c9 from network Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5] netvirt.neutronvpn.NeutronPortChangeListener Update port 85617daf-4a17-49c0-a3a2-9841625aa5c9 from network Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5] IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. IceNeutronPortChangeListener IcePort data tree changed. before IcePort{getDeviceId=dhcp639d7107-1693-5abe-bb94-18ba198147ec-21e67093-9cf3-4172-9943-64fca7be2da5, getDeviceOwner=network:dhcp, getIceAllowedAddressPairs=[], getIceExtraDhcpOpts=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:d8:2c:da], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getRevisionNumber=7, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9], isAdminStateUp=true, augmentations={interface yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=jw-controller, getIceVifDetails=[IceVifDetails{getDetailsKey=has_datapath_type_netdev, getValue=false, augmentations={}}, IceVifDetails{getDetailsKey=uuid, getValue=dc328b61-430d-4d06-a432-a9e3afc9660a, augmentations={}}, IceVifDetails{getDetailsKey=support_vhost_user, getValue=false, augmentations={}}], getVifType=ovs, getVnicType=normal}, interface yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} after IcePort{getDeviceId=dhcp639d7107-1693-5abe-bb94-18ba198147ec-21e67093-9cf3-4172-9943-64fca7be2da5, getDeviceOwner=network:dhcp, getIceAllowedAddressPairs=[], getIceExtraDhcpOpts=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.2]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:d8:2c:da], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getRevisionNumber=8, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9], isAdminStateUp=true, augmentations={interface yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=jw-controller, getIceVifDetails=[IceVifDetails{getDetailsKey=has_datapath_type_netdev, getValue=false, augmentations={}}, IceVifDetails{getDetailsKey=uuid, getValue=dc328b61-430d-4d06-a432-a9e3afc9660a, augmentations={}}, IceVifDetails{getDetailsKey=support_vhost_user, getValue=false, augmentations={}}], getVifType=ovs, getVnicType=normal}, interface yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} 交换机配置 interface Ethernet1/1 switchport mode trunk switchport trunk allowed vlan 2 命令 devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;vlan 2 ;interface Ethernet1/1 ;switchport ;switchport mode trunk ;switchport trunk allowed vlan 2 ;no shutdown 创建路由 日志 #### neutron 监听到路由器创建 #### IceNeutronRouterChangeListener IceNeutronRouterChangeListener data tree changed. IceNeutronRouterChangeListener WRITE IceRouter IceNeutronRouterChangeListener IceNeutronRouterChangeListener Adding IceRouter : value=IceRouter{getIceRoutes=[], getName=router, getProjectId=baec961077ea4e67933a8c140ca444d1, getRevisionNumber=1, getStatus=ACTIVE, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], isAdminStateUp=true, isDistributed=false, augmentations={}} #### 分配vlan vxlan id #### com.inspur.ice.impl.overlay.IceOverlayPoolsManager Allocate id: 11500 ,poolType :4 com.inspur.ice.impl.overlay.IceOverlayPoolsManager Allocate id: 2500 ,poolType :2 #### overlay mapper 介入，写vxlan L3 #### IceNeutronUtils serverLeafSet:[Uri [_value=iceDeviceId:172.20.1.177], Uri [_value=iceDeviceId:172.20.1.174]] IceVxlanCacheManager writeL3Vxlan IceDeviceVxlanL3 [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], _iceExtGateways=[], _iceRouterSubnet=[], _iceVlanId=IceVlanId [_value=2500], _iceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], _iceVpnType=Router, _iceVrfRoute=[], _iceVxlanId=IceVxlanId [_value=11500], _key=IceDeviceVxlanL3Key [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], _iceVxlanId=IceVxlanId [_value=11500]], _distributed=true, augmentation=[]] create true IceVxlanCacheManager writeL3Vxlan IceDeviceVxlanL3 [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], _iceExtGateways=[], _iceRouterSubnet=[], _iceVlanId=IceVlanId [_value=2500], _iceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], _iceVpnType=Router, _iceVrfRoute=[], _iceVxlanId=IceVxlanId [_value=11500], _key=IceDeviceVxlanL3Key [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], _iceVxlanId=IceVxlanId [_value=11500]], _distributed=true, augmentation=[]] create true #### overlay config 介入，监听到vxlan L3变化，准备下发配置 #### com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener IceVxlanL3ChangeListener Write - before : null after : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], getIceExtGateways=[], getIceRouterSubnet=[], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} com.inspur.ice.overlayconfig.impl.DistributeConfig distribute l3 vxlan config input VxlanConfigl3Input [_configType=Config, _deviceIp=Ipv4Address [_value=172.20.1.177], _routerId=de55a13c-2352-4dd6-b418-3cec4b149890, _vlanId=2500, _vni=VniRange [_value=11500], _vrfContext=de55a13c-23:11500, augmentation=[]] com.inspur.ice.overlayconfig.impl.ConfigCache init vrf : 11500 for device : iceDeviceId:172.20.1.177 com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V #### underlay 介入 ，下发第一条配置 #### com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;vlan 2500 ;no vn-segment ;vn-segment 11500 ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"body\": \"Warning: Enable double-wide arp-ether tcam carving if igmp snooping/Hsrp over vxlan is enabled. Ignore if tcam carving is already configured.\\n\", \"code\": \"200\", \"msg\": \"Success\" }] } }} com.inspur.ice.impl.devices.config.vlan.Vlan configVlanVnSegment sendCommands return :true com.inspur.ice.impl.devices.config.vlan.Vlan vlan is not created, so create vlan! IceLeafSwitchChangeListener LeafSwitch before:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.177], getDevIpAdd=Ipv4Address [_value=172.20.1.177], getDevLocation=, getDevManu=Inspur, getDevName=server177, getDevType=CN61108TC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=96, getPointY=45, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} after:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.177], getDevIpAdd=Ipv4Address [_value=172.20.1.177], getDevLocation=, getDevManu=Inspur, getDevName=server177, getDevType=CN61108TC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500,2500, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=96, getPointY=45, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} data tree changed write. com.inspur.ice.impl.devices.config.vxlan.VxlanManager config nve1L3Multicast result=true com.inspur.ice.impl.devices.config.vxlan.VrfContext config vrf start com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V #### underlay 介入 ，下发第二条配置 #### com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;vrf context de55a13c-23:11500 ;vni 11500 ;rd auto ;address-family ipv4 unicast ;route-target both auto ;route-target both auto evpn ;address-family ipv6 unicast ;route-target both auto ;route-target both auto evpn ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} #### netvirt 蒋婷到router创建 #### org.opendaylight.netvirt.ipv6service.NeutronRouterChangeListener Add Router notification handler is invoked Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890]. org.opendaylight.netvirt.ipv6service.IfMgr No unprocessed interfaces for the router Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890] com.inspur.ice.impl.devices.config.vxlan.VrfContext Succeed to mergeVrfRouteTargetList, routeTargetList is :RouteTargetList [_key=RouteTargetListKey [_routeTarget=Both, _routeTargetSourceNN=, _routeTargetDestNN=], _routeTarget=Both, _routeTargetDestNN=, _routeTargetSourceNN=, augmentation=[]] com.inspur.ice.impl.devices.config.vxlan.VxlanManager config VrfContextL3Multicast result=true com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V #### genius 介入 ，分配id #### org.opendaylight.genius.idmanager.IdManager Got pool IdLocalPool [poolName=vpnservices.2130706689, availableIds=AvailableIdHolder [low=100000, high=102999, cur=99999], releasedIds=ReleasedIdHolder [availableIdCount=0, timeDelaySec=30, delayedEntries=[]]] org.opendaylight.genius.idmanager.IdManager The newIdValues [100000] for the idKey de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnInstanceListener VPN-ADD: addVpnInstance: VPN Id 100000 generated for VpnInstanceName de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.genius.idmanager.jobs.UpdateIdEntryJob Updated id entry with idValues [100000], idKey de55a13c-2352-4dd6-b418-3cec4b149890, pool vpnservices.2130706689 org.opendaylight.netvirt.vpnmanager.VpnInstanceListener VPN-ADD: addVpnInstance: VpnInstanceOpData populated successfully for vpn de55a13c-2352-4dd6-b418-3cec4b149890 rd de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager processSavedInterfaces: No interfaces in queue for VPN de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnInstanceListener$PostAddVpnInstanceWorker VPN-ADD: onSuccess: Vpn Instance Op Data addition for de55a13c-2352-4dd6-b418-3cec4b149890 successful. #### underlay 介入 ，下发第三条配置 #### com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature interface-vlan ;vlan 2500 ;interface vlan 2500 ;no shutdown ;vrf member de55a13c-23:11500 ;ip forward com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"body\": \"Warning: Deleted all L3 config on interface Vlan2500\\n\", \"code\": \"200\", \"msg\": \"Success\" }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vlan.Vlan interfaceVlanVrf sendCommands return :true com.inspur.ice.impl.devices.config.vlan.Vlan vlan 2500 is created in device 172.20.1.177, do nothing! com.inspur.ice.impl.devices.config.vxlan.VxlanManager config InterfaceVlanMulticast result=true com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;interface nve1 ;member vni 11500 associate-vrf ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vxlan.Nve1Config save nve1 id of 172.20.1.177 succeeded com.inspur.ice.impl.devices.config.vxlan.VxlanManager config Nve1L3Multicast result=true com.inspur.ice.impl.devices.config.vxlan.L3Config successed to mergeRouterIdWithVrfContext, deviceIp is :Ipv4Address [_value=172.20.1.177] com.inspur.ice.overlayconfig.impl.DistributeConfig distribute vrfRoutingInstanceConfig input VrfRoutingInstanceConfigInput [_configType=Config, _deviceIp=Ipv4Address [_value=172.20.1.177], _vrfContext=de55a13c-23:11500, _notCheckDeviceRoute=false, augmentation=[]] com.inspur.ice.impl.devices.config.bgp.BorderLeafConfig device do not config router bgp, deviceIp is :172.20.1.177 com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteTable com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener IceVxlanL3ChangeListener Write - before : null after : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], getIceExtGateways=[], getIceRouterSubnet=[], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} com.inspur.ice.overlayconfig.impl.DistributeConfig distribute l3 vxlan config input VxlanConfigl3Input [_configType=Config, _deviceIp=Ipv4Address [_value=172.20.1.174], _routerId=de55a13c-2352-4dd6-b418-3cec4b149890, _vlanId=2500, _vni=VniRange [_value=11500], _vrfContext=de55a13c-23:11500, augmentation=[]] com.inspur.ice.overlayconfig.impl.ConfigCache init vrf : 11500 for device : iceDeviceId:172.20.1.174 com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;vlan 2500 ;no vn-segment ;vn-segment 11500 ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"body\": \"Warning: Enable double-wide arp-ether tcam carving if igmp snooping/Hsrp over vxlan is enabled. Ignore if tcam carving is already configured.\\n\", \"code\": \"200\", \"msg\": \"Success\" }] } }} com.inspur.ice.impl.devices.config.vlan.Vlan configVlanVnSegment sendCommands return :true com.inspur.ice.impl.devices.config.vlan.Vlan vlan is not created, so create vlan! IceLeafSwitchChangeListener LeafSwitch before:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.174], getDevIpAdd=Ipv4Address [_value=172.20.1.174], getDevLocation=, getDevManu=Inspur, getDevName=server174, getDevType=CN61108PC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500-501, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=255, getPointY=246, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} after:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.174], getDevIpAdd=Ipv4Address [_value=172.20.1.174], getDevLocation=, getDevManu=Inspur, getDevName=server174, getDevType=CN61108PC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500-501,2500, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=255, getPointY=246, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} data tree changed write. com.inspur.ice.impl.devices.config.vxlan.VxlanManager config nve1L3Multicast result=true com.inspur.ice.impl.devices.config.vxlan.VrfContext config vrf start com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;vrf context de55a13c-23:11500 ;vni 11500 ;rd auto ;address-family ipv4 unicast ;route-target both auto ;route-target both auto evpn ;address-family ipv6 unicast ;route-target both auto ;route-target both auto evpn ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vxlan.VrfContext Succeed to mergeVrfRouteTargetList, routeTargetList is :RouteTargetList [_key=RouteTargetListKey [_routeTarget=Both, _routeTargetSourceNN=, _routeTargetDestNN=], _routeTarget=Both, _routeTargetDestNN=, _routeTargetSourceNN=, augmentation=[]] com.inspur.ice.impl.devices.config.vxlan.VxlanManager config VrfContextL3Multicast result=true com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;feature interface-vlan ;vlan 2500 ;interface vlan 2500 ;no shutdown ;vrf member de55a13c-23:11500 ;ip forward com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"body\": \"Warning: Deleted all L3 config on interface Vlan2500\\n\", \"code\": \"200\", \"msg\": \"Success\" }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vlan.Vlan interfaceVlanVrf sendCommands return :true com.inspur.ice.impl.devices.config.vlan.Vlan vlan 2500 is created in device 172.20.1.174, do nothing! com.inspur.ice.impl.devices.config.vxlan.VxlanManager config InterfaceVlanMulticast result=true com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.174, deviceType=CN61108PC-V com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.174 and nxapi commands: config terminal ;interface nve1 ;member vni 11500 associate-vrf ;end com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vxlan.Nve1Config save nve1 id of 172.20.1.174 succeeded com.inspur.ice.impl.devices.config.vxlan.VxlanManager config Nve1L3Multicast result=true com.inspur.ice.impl.devices.config.vxlan.L3Config successed to mergeRouterIdWithVrfContext, deviceIp is :Ipv4Address [_value=172.20.1.174] com.inspur.ice.overlayconfig.impl.DistributeConfig distribute vrfRoutingInstanceConfig input VrfRoutingInstanceConfigInput [_configType=Config, _deviceIp=Ipv4Address [_value=172.20.1.174], _vrfContext=de55a13c-23:11500, _notCheckDeviceRoute=false, augmentation=[]] com.inspur.ice.impl.devices.config.bgp.BorderLeafConfig device do not config router bgp, deviceIp is :172.20.1.174 com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteTable 交换机配置 vlan 2002 vn-segment 11002 vrf context 1b58fa19-02:11002 vni 11002 rd auto address-family ipv4 unicast route-target both auto route-target both auto evpn address-family ipv6 unicast route-target both auto route-target both auto evpn interface Vlan2002 no shutdown vrf member 1b58fa19-02:11002 ip forward interface nve1 member vni 11002 associate-vrf router bgp 65101 vrf context 1b58fa19-02:11002 rd auto address-family ipv4 unicast route-target both auto route-target both auto evpn address-family ipv6 unicast route-target both auto route-target both auto evpn IP Route Table for VRF \"1b58fa19-02:11002\" '*' denotes best ucast next-hop '**' denotes best mcast next-hop '[x/y]' denotes [preference/metric] '%' in via output denotes VRF 命令 devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;vlan 2002 ;no vn-segment ;vn-segment 11002 ;end devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;vrf context 1b58fa19-02:11002 ;vni 11002 ;rd auto ;address-family ipv4 unicast ;route-target both auto ;route-target both auto evpn ;address-family ipv6 unicast ;route-target both auto ;route-target both auto evpn ;end devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature interface-vlan ;vlan 2002 ;interface vlan 2002 ;no shutdown ;vrf member 1b58fa19-02:11002 ;ip forward devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;interface nve1 ;member vni 11002 associate-vrf ;end devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;router bgp 65101 ;vrf 1b58fa19-02:11002 ;address-family ipv6 unicast ;advertise l2vpn evpn ;redistribute direct route-map all ;address-family ipv4 unicast ;advertise l2vpn evpn ;redistribute direct route-map all 路由器添加接口 日志分析 #### port 创建 #### com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener IcePort data tree changed. com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener WRITE com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener Adding Port : value=IcePort{getDeviceId=de55a13c-2352-4dd6-b418-3cec4b149890, getDeviceOwner=network:router_interface, getIceAllowedAddressPairs=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:70:03:a0], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getProjectId=baec961077ea4e67933a8c140ca444d1, getStatus=ACTIVE, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439], isAdminStateUp=true, augmentations={interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=, getIceVifDetails=[], getVifType=unbound, getVnicType=normal}, interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener IcePort{getDeviceId=de55a13c-2352-4dd6-b418-3cec4b149890, getDeviceOwner=network:router_interface, getIceAllowedAddressPairs=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:70:03:a0], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getProjectId=baec961077ea4e67933a8c140ca444d1, getStatus=ACTIVE, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439], isAdminStateUp=true, augmentations={interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=, getIceVifDetails=[], getVifType=unbound, getVnicType=normal}, interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} router interface created. #### 写vxlan L3 subnet 下的interface #### com.inspur.ice.overlaymapper.impl.IceVxlanCacheManager writeIceRouterSubnet IceRouterSubnet [_cidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], _iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _key=IceRouterSubnetKey [_iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]]], augmentation=[]] com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener IceVxlanL3ChangeListener Write - before : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], getIceExtGateways=[], getIceRouterSubnet=[], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} after : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], getIceExtGateways=[], getIceRouterSubnet=[IceRouterSubnet{getCidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], getGatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} com.inspur.ice.overlayconfig.impl.ConfigCache ADD IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], getIceExtGateways=[], getIceRouterSubnet=[IceRouterSubnet{getCidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], getGatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} failed with {} com.inspur.ice.overlaymapper.impl.IceNeutronvpnUtils Get subnetmap : Subnetmap{getId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], getNetworkId=Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5], getNetworkType=VLAN, getPortList=[Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]], getSegmentationId=500, getSubnetIp=100.100.1.0/24, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], augmentations={}} com.inspur.ice.overlayconfig.impl.ConfigCache init l3 vxlan : IceDeviceVxlanL3Key [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.177], _iceVxlanId=IceVxlanId [_value=11500]] : IceRouterSubnetKey [_iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]]] com.inspur.ice.overlayconfig.impl.DistributeConfig distribute interface vlan config input InterfaceVlanVrfMemberInput [_configVlanType=Config, _deviceIp=Ipv4Address [_value=172.20.1.177], _fabricForwarding=AnycastGateway, _vlanId=500, _vlanIp=100.100.1.1/24, _vrfMember=de55a13c-23:11500, _secondaryIp=true, augmentation=[]] com.inspur.ice.overlaymapper.impl.IceVlanCacheManager writeL3VpnInterface VpnInterface [_key=VpnInterfaceKey [_name=85617daf-4a17-49c0-a3a2-9841625aa5c9], _name=85617daf-4a17-49c0-a3a2-9841625aa5c9, _vpnInstanceName=de55a13c-2352-4dd6-b418-3cec4b149890, _routerInterface=false, augmentation=[Adjacencies [_adjacency=[Adjacency [_adjacencyType=PrimaryAdjacency, _ipAddress=100.100.1.2, _key=AdjacencyKey [_ipAddress=100.100.1.2], _macAddress=fa:16:3e:d8:2c:da, _subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], augmentation=[]]]]]] com.inspur.ice.overlaymapper.impl.IceVxlanCacheManager writeIceRouterSubnet IceRouterSubnet [_cidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], _iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _key=IceRouterSubnetKey [_iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]]], augmentation=[]] com.inspur.ice.overlaymapper.impl.IceNeutronvpnUtils Get subnetmap : Subnetmap{getId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], getNetworkId=Uuid [_value=476baec5-fd39-349d-a84e-57c1d2221ec5], getNetworkType=VLAN, getPortList=[Uuid [_value=85617daf-4a17-49c0-a3a2-9841625aa5c9]], getSegmentationId=500, getSubnetIp=100.100.1.0/24, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], augmentations={}} com.inspur.ice.overlaymapper.impl.IceVlanCacheManager writeL3VpnInterface VpnInterface [_key=VpnInterfaceKey [_name=85617daf-4a17-49c0-a3a2-9841625aa5c9], _name=85617daf-4a17-49c0-a3a2-9841625aa5c9, _vpnInstanceName=de55a13c-2352-4dd6-b418-3cec4b149890, _routerInterface=false, augmentation=[Adjacencies [_adjacency=[Adjacency [_adjacencyType=PrimaryAdjacency, _ipAddress=100.100.1.2, _key=AdjacencyKey [_ipAddress=100.100.1.2], _macAddress=fa:16:3e:d8:2c:da, _subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], augmentation=[]]]]]] com.inspur.ice.overlaymapper.impl.NeutronL3VpnInterfaceListener NeutronL3VpnInterfaceListener add recieve !!! com.inspur.ice.overlaymapper.impl.IceVlanCacheManager vpnPortMap ：ADD VpnInterface [_key=VpnInterfaceKey [_name=85617daf-4a17-49c0-a3a2-9841625aa5c9], _name=85617daf-4a17-49c0-a3a2-9841625aa5c9, _vpnInstanceName=de55a13c-2352-4dd6-b418-3cec4b149890, _routerInterface=false, augmentation=[Adjacencies [_adjacency=[Adjacency [_adjacencyType=PrimaryAdjacency, _ipAddress=100.100.1.2, _key=AdjacencyKey [_ipAddress=100.100.1.2], _macAddress=fa:16:3e:d8:2c:da, _subnetId=Uuid [_value=49c27f08-05d1-3f62-a810-91a6f700856a], augmentation=[]]]]]] failed com.inspur.ice.overlaymapper.impl.NeutronCommonUtils IceFwconnectionrouters is null!!! com.inspur.ice.overlaymapper.impl.NeutronL3VpnInterfaceListener NeutronL3VpnInterfaceListener iceFwconnectionrouter is null !!! org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager add: intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 onto vpnName de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager addVpnInterface: VPN Interface add event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 onto vpnName de55a13c-2352-4dd6-b418-3cec4b149890 from addVpnInterface com.inspur.ice.overlaymapper.impl.NeutronL3VpnInterfaceListener NeutronL3VpnInterfaceListener update recieve !!! org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager update: VPN Interface update event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 on dpn null oldVpn de55a13c-2352-4dd6-b418-3cec4b149890 newVpn de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager addVpnInterface: VPN Interface add event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 vpnName de55a13c-2352-4dd6-b418-3cec4b149890 on dpn null org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager processVpnInterfaceUp: Binding vpn service to interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 onto dpn 115377256237675 for vpn de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnFootprintService createOrUpdateVpnToDpnList: Created/Updated vpn footprint for vpn de55a13c-2352-4dd6-b418-3cec4b149890 vpnId 100000 interfacName85617daf-4a17-49c0-a3a2-9841625aa5c9 on dpn 115377256237675 org.opendaylight.netvirt.vpnmanager.VpnOpStatusListener update: Processing update for vpn de55a13c-2352-4dd6-b418-3cec4b149890 with rd de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.fibmanager.VrfEntryListener populateFibOnNewDpn: dpn: 115377256237675: VRF Table not yet available for RD de55a13c-2352-4dd6-b418-3cec4b149890 org.opendaylight.netvirt.vpnmanager.VpnFootprintService publishAddNotification: Successful in notifying listeners for add dpn 115377256237675 in vpn de55a13c-2352-4dd6-b418-3cec4b149890 rd de55a13c-2352-4dd6-b418-3cec4b149890 event org.opendaylight.netvirt.vpnmanager.VpnFootprintService$DpnEnterExitVpnWorker onSuccess: FootPrint established for vpn de55a13c-2352-4dd6-b418-3cec4b149890 rd de55a13c-2352-4dd6-b418-3cec4b149890 on dpn 115377256237675 org.opendaylight.netvirt.vpnmanager.VpnFootprintService createOrUpdateVpnToDpnList: Sent populateFib event for new dpn 115377256237675 in VPN de55a13c-2352-4dd6-b418-3cec4b149890 for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 org.opendaylight.genius.datastoreutils.DataStoreJobCoordinator Exception when executing jobEntry: JobEntry{key='VPNINTERFACE-85617daf-4a17-49c0-a3a2-9841625aa5c9', mainWorker=org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager$$Lambda$741/1527639192@6e597ade, rollbackWorker=null, retryCount=0, futures=null} java.lang.NullPointerException: null at org.opendaylight.netvirt.vpnmanager.VpnUtil.getVpnSubnetGatewayIp(VpnUtil.java:1464) ~[395:org.opendaylight.netvirt.vpnmanager-impl:0.5.2] at org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager.processVpnInterfaceAdjacencies(VpnInterfaceManager.java:679) ~[395:org.opendaylight.netvirt.vpnmanager-impl:0.5.2] at org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager.processVpnInterfaceUp(VpnInterfaceManager.java:379) ~[395:org.opendaylight.netvirt.vpnmanager-impl:0.5.2] at org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager.lambda$addVpnInterface$0(VpnInterfaceManager.java:247) ~[395:org.opendaylight.netvirt.vpnmanager-impl:0.5.2] at org.opendaylight.genius.datastoreutils.DataStoreJobCoordinator$MainTask.run(DataStoreJobCoordinator.java:285) [304:org.opendaylight.genius.mdsalutil-api:0.3.2] at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402) [?:?] at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:?] at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:?] at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:?] at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) [?:?] org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager VPN Interface update event - intfName 85617daf-4a17-49c0-a3a2-9841625aa5c9 onto vpnName de55a13c-2352-4dd6-b418-3cec4b149890 running config-driven org.opendaylight.netvirt.vpnmanager.VpnInterfaceManager update: vpn interface updated for interface 85617daf-4a17-49c0-a3a2-9841625aa5c9 oldVpn de55a13c-2352-4dd6-b418-3cec4b149890 newVpn de55a13c-2352-4dd6-b418-3cec4b149890 processed successfully com.inspur.ice.impl.common.DevInfo deviceIp=172.20.1.177, deviceType=CN61108TC-V com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener IceNeutronPortChangeListener data tree changed. com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener IcePort data tree changed. before IcePort{getDeviceId=de55a13c-2352-4dd6-b418-3cec4b149890, getDeviceOwner=network:router_interface, getIceAllowedAddressPairs=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:70:03:a0], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getProjectId=baec961077ea4e67933a8c140ca444d1, getStatus=ACTIVE, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439], isAdminStateUp=true, augmentations={interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=, getIceVifDetails=[], getVifType=unbound, getVnicType=normal}, interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} after IcePort{getDeviceId=de55a13c-2352-4dd6-b418-3cec4b149890, getDeviceOwner=network:router_interface, getIceAllowedAddressPairs=[], getIceExtraDhcpOpts=[], getIceFixedIps=[IceFixedIps{getIpAddress=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceSecurityGroups=[], getMacAddress=MacAddress [_value=fa:16:3e:70:03:a0], getName=, getNetworkId=Uuid [_value=21e67093-9cf3-4172-9943-64fca7be2da5], getRevisionNumber=5, getTenantId=Uuid [_value=baec9610-77ea-4e67-933a-8c140ca444d1], getUuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439], isAdminStateUp=true, augmentations={interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.binding.rev180302.IcePortBindingExtension=IcePortBindingExtension{getHostId=, getIceVifDetails=[], getVifType=unbound, getVnicType=normal}, interface org.opendaylight.yang.gen.v1.urn.opendaylight.params.xml.ns.yang.ice.neutron.portsecurity.rev150712.IcePortSecurityExtension=IcePortSecurityExtension{isPortSecurityEnabled=false}}} com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener WRITE com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener Updating Port : original = IcePortKey [_uuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439]] update = IcePortKey [_uuid=Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439]] com.inspur.ice.overlaymapper.impl.IceNeutronPortChangeListener Search port failed from neutron data store Uuid [_value=562cd511-c923-44b1-993a-ed3647e02439] com.inspur.ice.impl.HttpPostCmd devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature interface-vlan ;vlan 500 ;interface vlan 500 ;no shutdown ;vrf member de55a13c-23:11500 ;no ip forward ;ip address 100.100.1.1/24 ;fabric forwarding mode anycast-gateway com.inspur.ice.impl.HttpPostCmd nxapi return is: { \"ins_api\": { \"sid\": \"eoc\", \"type\": \"cli_conf\", \"version\": \"1.0\", \"outputs\": { \"output\": [{ \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"body\": \"Warning: Deleted all L3 config on interface Vlan500\\n\", \"code\": \"200\", \"msg\": \"Success\" }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }, { \"code\": \"200\", \"msg\": \"Success\", \"body\": { } }] } }} com.inspur.ice.impl.devices.config.vlan.Vlan interfaceVlanVrf sendCommands return :true com.inspur.ice.impl.devices.config.vlan.Vlan vlan is not created, so create vlan! org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ArpNotification Gratuitous Request Received from interface 115377256237675:ens192:500 and IP 100.100.1.1 having MAC 00:01:00:01:00:01 target destination 100.100.1.1, learning MAC org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ARP NO_RESOLVE: VPN not configured. Ignoring responding to ARP requests from this Interface 115377256237675:ens192:500. org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ArpNotification Gratuitous Request Received from interface 115377256237675:ens192:500 and IP 100.100.1.1 having MAC 00:01:00:01:00:01 target destination 100.100.1.1, learning MAC org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ARP NO_RESOLVE: VPN not configured. Ignoring responding to ARP requests from this Interface 115377256237675:ens192:500. com.inspur.ice.overlaymapper.impl.IceLeafSwitchChangeListener LeafSwitch before:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.177], getDevIpAdd=Ipv4Address [_value=172.20.1.177], getDevLocation=, getDevManu=Inspur, getDevName=server177, getDevType=CN61108TC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500,2500, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=96, getPointY=45, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} after:Switch{getDevDesc=ICNT NOS-CN(tm) inos, Software (inos-cn), Version 9.2(2), RELEASE SOFTWARE Copyright (c) 2016-2018, Used subject to license from copyright owner. Compiled 11/8/2018 7:00:00, getDevId=Uri [_value=iceDeviceId:172.20.1.177], getDevIpAdd=Ipv4Address [_value=172.20.1.177], getDevLocation=, getDevManu=Inspur, getDevName=server177, getDevType=CN61108TC-V, getDeviceRole=ServerLeaf, getDeviceVlanId=1,500,2500, getEnablePassword=inspur@123, getEvpnTemp=1, getMacAddress=00:00:00:00, getOnlineStatus=onLine, getPointX=96, getPointY=45, getSnmpRead=public, getSnmpVersion=v2c, getSnmpWrite=public, getSshPassword=inspur@123, getSshUserName=admin, getSyncStatus=sync, isHasFex=false, augmentations={}} data tree changed write. com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener REMOVE VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener UPDATE RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteTable com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener REMOVE RouteTable com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener IceVxlanL3ChangeListener Write - before : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], getIceExtGateways=[], getIceRouterSubnet=[IceRouterSubnet{getCidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], getGatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} after : IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], getIceExtGateways=[], getIceRouterSubnet=[IceRouterSubnet{getCidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], getGatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} com.inspur.ice.overlayconfig.impl.ConfigCache ADD IceDeviceVxlanL3{getIceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], getIceExtGateways=[], getIceRouterSubnet=[IceRouterSubnet{getCidr=IpPrefix [_ipv4Prefix=Ipv4Prefix [_value=100.100.1.0/24]], getGatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]], getIceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], augmentations={}}], getIceVlanId=IceVlanId [_value=2500], getIceVpnId=Uuid [_value=de55a13c-2352-4dd6-b418-3cec4b149890], getIceVpnType=Router, getIceVrfRoute=[], getIceVxlanId=IceVxlanId [_value=11500], isDistributed=true, augmentations={}} failed with {} com.inspur.ice.overlayconfig.impl.ConfigCache ADD IceRouterSubnetKey [_iceSubnetId=Uuid [_value=08617069-adb2-476c-90e9-b6bb4f425091], _gatewayIp=IpAddress [_ipv4Address=Ipv4Address [_value=100.100.1.1]]] failed with IceDeviceVxlanL3Key [_iceDeviceId=Uri [_value=iceDeviceId:172.20.1.174], _iceVxlanId=IceVxlanId [_value=11500]] com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener REMOVE VrfRoute com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener UPDATE RouteGateway com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener ADD RouteTable com.inspur.ice.overlayconfig.impl.IceVxlanL3ChangeListener REMOVE RouteTable org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ArpNotification Gratuitous Request Received from interface 115377256237675:ens192:500 and IP 100.100.1.1 having MAC 00:01:00:01:00:01 target destination 100.100.1.1, learning MAC org.opendaylight.netvirt.vpnmanager.ArpNotificationHandler ARP NO_RESOLVE: VPN not configured. Ignoring responding to ARP requests from this Interface 115377256237675:ens192:500. 交换机配置 添加到路由 interface Vlan2 no shutdown vrf member 1b58fa19-02:11002 ip address 10.10.10.1/24 fabric forwarding mode anycast-gateway 命令 devIpAdd is :172.20.1.177 and nxapi commands: config terminal ;feature interface-vlan ;vlan 2 ;interface vlan 2 ;no shutdown ;vrf member 1b58fa19-02:11002 ;no ip forward ;ip address 10.10.10.1/24 ;fabric forwarding mode anycast-gateway 添加静态路由 交换机配置 vrf context vni-11001 vni 11001 rd auto ip route 10.0.0.0/24 99.99.99.2 目的网段/下一跳 address-family ipv4 unicast route-target both auto route-target both auto evpn address-family ipv6 unicast route-target both auto route-target both auto evpn 在BGP中发布（underlay bgpvrf） router bgp 65101 vrf vni-11001 address-family ipv4 unicast network 10.0.0.0/24 /下一跳 动态路由 交换机配置 ------underlay bgpVrfNeighborConfig router bgp 65101 vrf vni-11001 address-family ipv4 unicast advertise l2vpn evpn neighbor 99.99.99.2 remote-as 200 address-family ipv4 unicast 常用接口说明 overlay distributeL2VxlanConfig --> vxlanConfigl2Dynamic \" config terminal ;feature nv overlay ;feature vn-segment-vlan-based ;nv overlay evpn ;evpn ;vni 10071 l2 ;rd auto ;route-target import auto ;route-target export auto ;exit\" Copyright © jiangwei 2018 all right reserved，powered by Gitbook........ 2019-04-18 02:55:41 "}}